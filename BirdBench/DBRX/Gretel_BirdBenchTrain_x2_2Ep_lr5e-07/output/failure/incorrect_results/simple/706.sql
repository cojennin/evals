-- PROBLEM # 706
-- Generation # 0
-- Difficulty: simple
----------------------------

-- PROMPT
CREATE TABLE badges
(
    Id     INTEGER          not null
        primary key,
    UserId INTEGER          null,
    Name   TEXT null,
    Date   DATETIME     null,
        foreign key (UserId) references users (Id)
            on update cascade on delete cascade
)

CREATE TABLE comments
(
    Id              INTEGER          not null
        primary key,
    PostId          INTEGER          null,
    Score           INTEGER          null,
    Text            TEXT     null,
    CreationDate    DATETIME     null,
    UserId          INTEGER          null,
    UserDisplayName TEXT null,
        foreign key (PostId) references posts (Id)
            on update cascade on delete cascade,
        foreign key (UserId) references users (Id)
            on update cascade on delete cascade
)

CREATE TABLE postHistory
(
    Id                INTEGER          not null UNIQUE
        primary key,
    PostHistoryTypeId INTEGER          null,
    PostId            INTEGER          null,
    RevisionGUID      TEXT null,
    CreationDate      DATETIME     null,
    UserId            INTEGER          null,
    Text              TEXT     null,
    Comment           TEXT         null,
    UserDisplayName   TEXT null,
        foreign key (PostId) references posts (Id)
            on update cascade on delete cascade,
        foreign key (UserId) references users (Id)
            on update cascade on delete cascade
)

CREATE TABLE postLinks
(
    Id            INTEGER      not null
        primary key,
    CreationDate  DATETIME null,
    PostId        INTEGER      null,
    RelatedPostId INTEGER      null,
    LinkTypeId    INTEGER      null,
        foreign key (PostId) references posts (Id)
            on update cascade on delete cascade,
        foreign key (RelatedPostId) references posts (Id)
            on update cascade on delete cascade
)

CREATE TABLE posts
(
    Id                    INTEGER          not null UNIQUE
        primary key,
    PostTypeId            INTEGER          null,
    AcceptedAnswerId      INTEGER          null,
    CreaionDate           DATETIME     null,
    Score                 INTEGER          null,
    ViewCount             INTEGER          null,
    Body                  TEXT     null,
    OwnerUserId           INTEGER          null,
    LasActivityDate       DATETIME     null,
    Title                 TEXT null,
    Tags                  TEXT null,
    AnswerCount           INTEGER          null,
    CommentCount          INTEGER          null,
    FavoriteCount         INTEGER          null,
    LastEditorUserId      INTEGER          null,
    LastEditDate          DATETIME     null,
    CommunityOwnedDate    DATETIME    null,
    ParentId              INTEGER          null,
    ClosedDate            DATETIME     null,
    OwnerDisplayName      TEXT null,
    LastEditorDisplayName TEXT null,
        foreign key (LastEditorUserId) references users (Id)
            on update cascade on delete cascade,
        foreign key (OwnerUserId) references users (Id)
            on update cascade on delete cascade,
        foreign key (ParentId) references posts (Id)
            on update cascade on delete cascade
)

CREATE TABLE tags
(
    Id            INTEGER          not null
        primary key,
    TagName       TEXT null,
    Count         INTEGER          null,
    ExcerptPostId INTEGER          null,
    WikiPostId    INTEGER          null,
    foreign key (ExcerptPostId) references posts (Id)
        on update cascade on delete cascade
)

CREATE TABLE users
(
    Id              INTEGER          not null UNIQUE
        primary key,
    Reputation      INTEGER          null,
    CreationDate    DATETIME     null,
    DisplayName     TEXT null,
    LastAccessDate  DATETIME     null,
    WebsiteUrl      TEXT null,
    Location        TEXT null,
    AboutMe         TEXT     null,
    Views           INTEGER          null,
    UpVotes         INTEGER          null,
    DownVotes       INTEGER          null,
    AccountId       INTEGER          null,
    Age             INTEGER          null,
    ProfileImageUrl TEXT null
)

CREATE TABLE votes
(
    Id           INTEGER  not null
        primary key,
    PostId       INTEGER  null,
    VoteTypeId   INTEGER  null,
    CreationDate DATE null,
    UserId       INTEGER  null,
    BountyAmount INTEGER  null,
        foreign key (PostId) references posts (Id)
            on update cascade on delete cascade,
        foreign key (UserId) references users (Id)
            on update cascade on delete cascade
)

-- External Knowledge: Title = 'How can I adapt ANOVA for binary data?';
-- Using valid SQLite and understading External Knowledge, answer the following questions for the tables provided above.
-- Give the texts commented on the post about linear regression.
Generate the SQL after thinking step by step: 
SELECT 
----------------------------

-- RESPONSE
SELECT c.Text FROM comments c WHERE c.PostId = (SELECT p.Id FROM posts p WHERE p.Title = 'How can I adapt ANOVA for binary data?')
----------------------------




-- RESULTS FROM PREDICTION
[]
----------------------------



-- ACTUAL RESULTS
[('Are you asking about how to reduce the effect of outliers or when to use the log of some variable?',), ('I think that the OP is saying "I\'ve heard of people using the log on input variables: why do they do that?"',), ('If I am not mistaken, \n\nlinear regression is the estimation of coefficients that define a good linear map from X to Y.\n\n ANOVA is a test to know if there is significant differences in X when Y take two different values.  \n\nCan you explain us why you think they are the same?',), ('Really great question.  The best way to understand something is from multiple direction of explanation.',), ('Are you refering to the following paper?\nKuk, A.Y.C. (1984) All subsets regression in a proportional hazards model. Biometrika, 71, 587-592',), ("yes indeed. I guess I'll have to dig up that paper somehow. It seems old, however.",), ('I have no idea what this question is about.',), ('It sounds like it is about the difference between correlation and simple linear regression.',), ('Find this article in the meantime, The lasso method for variable selection in the cox model, from Tibshirani (Stat. Med. 1997 16: 385-395), http://j.mp/bw0mB9. HTH',), ('and this more recent one (closely linked to the `penalized` R package), http://j.mp/cooIT3. Maybe this one too, http://j.mp/bkDQUj. Cheers',), ('What R package did you use for HC3 estimation? `sandwich`, `contrast`?',), ('Another question while we are in: What is the design you are considering, I mean is there any clustering or multiple predictors, or is it a simple linear regression? This may help the reader to better understand the context of your study.',), ('Have you tried re-expressing the dependent variable to stabilize variance?',), ('I´m using the sandwich package for the HC3 estimator. I´m using just simple linear regression. Intuitively I feel more comfortable with the bootstrap version and I guess I´ll stick with that. //thx for the input',), ('Sharpie, I wonder if it\'s time to accept a "best answer"or at least explain why you are not satisfied with the answers on offer so far.',), ('What is $\\\\epsilon$?',), ('$\\\\epsilon$ is a zero-mean error term.',), ('ANOVA can be seen as "syntactic sugar" for a special subgroup of linear regression models. ANOVA is regularly used by researchers who are not statisticians by training. They are now "institutionalized" and its hard to convert them back to using the more general representation ;-)',), ('Quick clarification: Do you want to assume that exactly one functional form should fit all of the participants, or do some participants have one form and some another?  If one of the functional form fits them all, do the parameter values get to vary across participants, or not?',), ('@Conjugate prior: No. It is quite clear from the data that the best functional form varies across participants. I am running separate nonlinear regressions.',), ('What is the aim of your analyses?',), ("Yes, I did leave that out, didn't I. The goal is to find a predictive equation for share2 from the variables provided.",), ("Is this what you're looking for? http://www.weibull.com/DOEWeb/confidence_intervals_in_simple_linear_regression.htm",), ('Could you explain where the 82/15/3 values come from?  You appear to be using numbers that you haven\'t yet shared with us, such as "the monthly CDDs" (whatever those may be) and the "total predictions for the year" (whatever those may be).',), ("Hopefully it's more clear now, thanks!",), ('Comparison of Region 1 and 2 on what metric? e.g. log odds ratio, log risk ratio, risk difference ...?',), ('"fails to converge".. "do converge eventually". "a fit that looks good"..."the visual fit is terrible". Sorry, but I\'m confused!',), ('By tweaking convergence criteria I can get the model to converge, but when this happens the plotted function appears to be a much worse fit than the unconverged results. Presumably due to very large convergence criteria.',), ("If your software is well written it automatically standardizes internally to avoid numerical precision problems.  You shouldn't have to do anything special.",), ('@onestop: I am sorry i missed your question. The comparison is on log odds ratio. Thank you for your interest.',), ('@Weijlie You can use $\\\\LaTeX$ code in the usual way, i.e. by enclosing your expression with `$` (inline) or `$$` (display). Also, please make your title shorter (e.g., "Intuitive illustration of Newton\'s method in nonlinear regression").',), ('The *Mathematica* help page has a nice illustration with explanations: http://reference.wolfram.com/mathematica/tutorial/UnconstrainedOptimizationNewtonsMethodRoot.html .',), ("@Can'tTell You can find some help about formatting here: http://stats.stackexchange.com/editing-help",), ('You passed the junk regression (F) test but not t tests -- it is the first sign for multicollinearity problem look here for instance http://stats.stackexchange.com/questions/4099/vif-condition-index-and-eigen-values/8223#8223',), ('@chl has nicely answered your question in his reply at http://stats.stackexchange.com/questions/8237/logic-behind-the-anova-f-test-in-simple-linear-regression/8247#8247 : see the second paragraph.',), ("Could you please confirm that you're also the asker for [Logic behind the ANOVA F-test in simple linear regression](http://stats.stackexchange.com/questions/8237/logic-behind-the-anova-f-test-in-simple-linear-regression) so that we can merge your two accounts. (You will still need to register, see the [FAQ](http://stats.stackexchange.com/faq).)",), ("I recommend you change the title. The answer to the question 'is this result good or bad' is subjective, and would depend on what the predictor variables are - as well as the values of the respondent. Car mechanics / salesmen / manufacturers might prefer more crashes, a car driver might prefer less. The size of the estimate of a standard error is just that - there is no reason to think that smaller is better without context. Perhaps you want to know if your results support your hypothesis or not, or if your results are valid, in which case you would be able to find useful answers here.",), ("@David: +1 Thank you for the suggestion. I changed it to a comparison question. Hope this is ok. I should have qualified my question with a simulation tag. I am evaluating my scenarios from a driver's perspective so less crashes is better. My original hypothesis was that the output (time at which 80% of the cars crash) does not depend on the three parameters. But from my results, I am tempted to say that I can reject my hypothesis but how strongly I can reject is still something I am looking into but thank you for the insight.",), ("I tried to reword a little bit your question. I'm afraid it looks like you're asking two very different questions. For the second one, a lot of resources is available on this site, but also on [CRAN](http://cran.r-project.org/).",), ('@chl, yeap, thanks, I wasnt clear. My questions is really this:  If I write LM in R does R understands it as linear always or triesd to fit any model, not necessarily linear regression but any regression ?',), ("No, `lm()` stands for a linear regression. Your model includes three parameters (minus the intercept) for `b`, `c`, and their interaction `b:c`, which stands for `b + c + b:c` or `b*c` for short (R follows Wilkinson's notation for statistical models). Fitting a Generalized Linear Model (i.e., where the link function is not identity, as is the case for the linear model expressed above) is requested through `glm()`.",), ('I\'m surprised that they give you different answers. It might help to clarify what you mean by "answer".',), ('@Thomas The different answers are $p=ar+b$ and $p=(r-b\')/a\'$ from the two regressions.  The slopes *always* differ whenever the correlation is not 1 or -1: that\'s just another manifestation of "regression to the mean."',), ('I say exponentially because I want to control the weight of old samples just like using weighted moving average. But I want the old samples to be less weighted, exponentially less, than newer samples',), ('If X\'s (independent variables) are measured variables, then it is a "random variable" instead of a "fixed value". According to Wikipedia (http://en.wikipedia.org/wiki/Random_variable):\n"In probability and statistics, a random variable or stochastic variable is, roughly speaking, a variable whose value results from a measurement on some type of random process." Am I correct?',), ("Please don't use replies to ask questions.  You could continue this conversation in comments.  Another option is to re-post this as a question and link back here.",), ('I am tempted to say that this strange peak is the more related to poor binning algorithm of histogram than to the features of data. Besides that since no context is given there is a lot of possible answers, but writing them up would not be interesting.',), ('@mpiktas: actually, the binning algorithm is not guilty here. Wolfgang was right, there was a strong bias in one category of measurements. Problem solved',), ('cheers, you got lucky :)',), ("If it's possible to quote the passage in question that might help: sometimes nuances get lost in the retelling.",), ('Did you not answer your own question?',), ("@Andy: How come? I don't think so, I mentioned that all of the linear models I know are a very bad fit, and that I am wondering if there is another transformation that can be performed on variables to find a better linear fit.",), ('A cubic looking curve would suggest a cubic type transformation! Try fitting an OLS model with higher order polynomial X terms (specifically X^3). There are other potential solutions as well (splines, breaking the X variable into different categories and using dummy variables). A recent post also details exploratory data analysis in examining such relationships, http://stats.stackexchange.com/questions/10363/data-mining-how-should-i-go-about-finding-the-functional-form/10520#10520',), ('It might be worth remarking that "rises to some point and then falls to a steady level without a consequent rise" is distinctly *non-cubic* behavior.  Cubics don\'t have horizontal asymptotes.  More (quantitative) details would be helpful.',), ('Hi,\nYou might try to fit a "nonlinear" model. Unfortunately, I can\'t see the graphic of your variable but I do believe that nonlinear regression will be the best choice for your question. Sincerely,',), ('What do you mean by "direction"? Have you read the answers to similar questions http://stats.stackexchange.com/search?q=causal ? The short answer is no!',), ('Neither of your suggestions imply causation (or direction).',), ('I think the OP meant "direction" in the sense of positive vs negative correlation, not the direction of any causal relationship between X and Y.',), ("I think it's reasonable - I do it fairly frequently. Don't know of a name for it",), ('What exactly are you trying to accomplish? This strikes me as a little odd and, in particular, (highly) **prone to misinterpretation**. How would you appropriately label your axes of the last plot to make it easier to interpret?',), ('A side note -- if you plot a log-log graph, it is better to use [log axes](http://had.co.nz/ggplot2/coord_trans.html). Otherwise you suggest a linear relation between price and weight, which is in fact exponential.',), ('Just the other day someone somewhere on one of the related StackExchange sites was commenting on this scheme as "poor man\'s Kalman filter".  If I manage to unearth the link I will add it here.',), ('@cardinal, what sort of misinterpretation do you mean? As for y-axis labels, the 3rd graph to me is "log(price) detrended" while the 2nd graph is simply "residuals".',), ('@mbq, yes, fair point.',), ('Thanks. I would like to see how this can be reframed as a Kalman filter.',), ("I doubt there is a formal derivation, hence the quotes around poor man's version of adaptive parameters.",), ("@TMOD, let's say, on your last plot, there is a point corresponding to $(-0.4, 3.3)$. Using just the plot, how would one interpret the meaning of that point? (The question is a bit rhetorical, with the point being to think about the information that the plot is, and is not, conveying.)",), ('In the search for outliers, you should first regress your dependent variable against both $x_{1}$ and $x_{2}$ and look for outliers.',), ('Is finding outliers the pupose of your investigation? If so, then you should first regress your dependent variable against both $x_{1}$ and $x_{2}$ and then perform outlier tests. If finding possible causation, then you should consider performing a designed experiment. If the purpose of your experiment is to find a relationship between your two independent variables, looking at a happenstance of collected data will not do the trick.',), ("It isn't clear to me what you mean by outliers. If there are outliers in your data then they will affect the calculation of the regression line.  Why are you looking for outliers in both $x_1$ and $x_2$ simultaneously?",), ('@schenectady Use $$ for LaTeX in comments, please.',), ('It sounds like you want a "prediction interval".  The answer (with formulas) is available on this site [in several places](http://stats.stackexchange.com/search?q=%22prediction+interval%22).',), ("Please post a link to the math.SE question. Usually it's not good to cross-post unless significant time has elapsed.",), ('Can the same number be selected twice? Is n smaller than or greater than U?',), ('Here is the previous question on math.SE: http://math.stackexchange.com/questions/32569/a-question-about-linear-regression',), ('@Nick Sabbe The selection is without replacement. n is always smaller than U.',), ("The problem can be stated more simply: let $\\\\{(x_i,y_i)\\\\} = (1,y_1), \\\\ldots, (n,y_n)$ be the data with the $y_i$ integers, $1 \\\\le y_1 \\\\lt y_2 \\\\cdots \\\\lt y_n \\\\le U$.  Finding $\\\\mathbb{E}[r_{xy}]$ is hopeless.  There's some chance of attacking $\\\\mathbb{E}[r^2_{xy}]$ but it's not by any means easy (it's a ratio of two quadratic forms in the $y_i$).",), ('@whuber Thank you for your comments. If $E[r_{xy}]$ cannot be calculated, an estimation or lower bound is ok.',), ("@Fan That's a good comment: by allowing approximation or bounding, you are opening up the question to many more solution methods.  Consider making this point explicit in the question itself.  If you can, please indicate the ranges of values of $U$ and $n$ you're interested in.  For instance, would you be interested in asymptotics as $U \\\\to \\\\infty$?",), ('@whuber I think if $U\\\\rightarrow\\\\inf$, the problem can be seen as selecting with replacement? This would be also my interest.',), ('@Fan for $U \\\\sim \\\\infty$, then after rescaling by $1/U$ you can view the $y_i$ as the order statistics for a sample of $n$ iid variables from a uniform distribution on $[0,1]$, thereby replacing the discrete sums in the original problem with integrals.',), ('@whuber I see. Do you have some relevant materials about your approach?',), ('@Fan Applicable techniques would include quadratic forms in random variables (http://stats.stackexchange.com/questions/9220), the "delta method" for estimating moments of functions of random variables; distributions of order statistics for uniform variables; the relationship between gaps between uniform variables and the exponential distribution, and possibly even saddlepoint methods, normal approximations, Central Limit Theorem, etc.',), ('@whuber Thank you very much, the information helps me much!!',), ('I know it is not a real answer, but I would strongly recommend to use `R` for this purpose. To my knowledge SPSS is just not made for "reusing" results. However, you could just get the regression coefficients and simply use the `COMPUTE` command to compute the predicted values using the general forumal `y = a + bx`',), ("If I could use R for this I would, however the client runs an SPSS house and is not keen to use R. The COMPUTE option would work if we were just after the mean estimate, but I'd like to get the prediction intervals out as well.",), ('Another option that might keep the client happy would be to use the R plugin in SPSS.',), ('how you determine best lags? From my personal practice omitting some *bad periods of time* by lags my produce better fits, but the model is not passing the _laugh test_. So what is the origin of your data? And yes you can do subset searches, a lot of different data mining techniques are available.',), ("That's exactly my question to more experienced users: Are there better / other determinants than just an improved fit? Obviously I should use a serious model, but still I wonder whether there is some good strategy to follow in order to get a good first guess up to which order lags should be relevant. Data is e.g. quarterly trend deviation from some stock index.",), ('@CantTell Please register your account and accept some answers.',), ('have you tried any information criteria like AC/BIC/HQ to optimize the lag choices? Another option could be cross-validation like out-of-sample and jack-knife performance.',), ('thx Dmitrij, just looked briefly into BIC but none of the others. Any good pointers?',), ('Are "top floor" and "corner unit" correlated with each other?',), ("Sorry, I should've said those coefficients have a negative relationship with the dependent variable (price), but a top floor unit is not necessarily a corner unit and vice versa.",), ('there are also _beta_ coefficients in financial econometrics regarding portfolio theory, so indeed a lot of confusion here indeed. It is easier to use exact definitions as @mark (+1) suggests instead of Greeks.',), ('I think @Andy understood your problem. What he was getting at was that if two covariates are highly collinear, then one will tend to "compensate" for the other. So you can get a negative value for one and positive for another. This makes interpretation of the sign of a particular coefficient estimate a very slippery matter.',), ('It sounds like you have a sample size of 40 and at least 10 total predictors. Your current model can be interpreted as taking the square footage and multiplying it by a constant to get a "base" price. Then adding/subtracting constant dollar amounts based on additional "features" of the unit. Increasing your sample size could help matters.',), ("Dropping exposure sounds like a questionable thing to do, though it may need to be recoded according to the building the unit is in, perhaps on a subjective basis. For example, in North America, southern exposure is probably generally desirable due to increased sunlight. However, if you're in a large urban area and there's a big park just to the west of your condo, then western exposure may be highly desirable.",), ("@santiagozky I've made some edits to your question to make it clear that you're trying to predict values from a linear regression model. However, you'll need to clarify some issues: (a) are you trying to predict new (unobserved) values or get the fitted values for your actual observations? (b) can you give more information on model fit (R `summary()` output) or a scatterplot of your data (as a rough check for outliers, etc.)?",), ('You should mention what kind of data it is and how it was obtained. That would help us answer your question. Statistical methods like lm depend on certain assumptions, and they simply crunch your input through a formula and spit out "answers". If the data does not satisfy lm\'s assumptions, the answer will be random garbage. So you need to think about the data and the system you are trying to model with a straight line, to see if 55.17 and 19.31 make sense. What would they mean in the real world, and is that plausible? And how did your two columns of data arise?',), ('The answer is yes and the details are provided at http://stats.stackexchange.com/questions/9131/obtaining-a-formula-for-prediction-limits-in-a-linear-model',), ('thanks for suggestions, I added more info and corrected the regression, which was from another case I have. @whuber thanks. I thought this was asked before but I couldnt find it',), ('@santiagozky: The description and graph are very helpful because they show this is a time-to-failure problem, not a regression problem.',), ("Excellent update! (Unfortunately, your clarification puts the problem into an area that I'm not familiar with.) Two suggestions, though: 1) Look at the time between errors and read up on the Poisson distribution (http://en.wikipedia.org/wiki/Poisson_distribution), and 2) is anything being done to decrease failures or is the software and process for using the software in a steady state? (Most of the software reliability methods I could find assume that bugs are found and fixed, so the curve you've drawn would become less and less steep over time.)",), ('`var(fitted(lm_model))` do you need this one?',), ('@Dmitrij Post an answer!',), ('What does this expression $\\\\widehat{var(y)}$ mean?  Suppose the data are $(x_i,y_i)$ and the $y_i$ are random variables with expectations $\\\\beta x_i$ for some unknown coefficient(s) $\\\\beta$.  Do you want the variance of the set of $\\\\beta x_i$ (either assuming the $x_i$ are fixed or under some distributional assumptions for them) or do you want to estimate the variance of the $y_i$?  The latter is larger than the former.',), ('@cardinal Good answer!  (Maybe you could post it as a reply?)',), ("@cardinal.  Right on the money.  Please post this as an answer -- let's minimize the number of unanswered questions.  :O)  Ps. Up-votes headed your way.",), ('Thank you cardinal. By the way, is there a way to force the regression line to have a negative slope?',), ('If the fitted line does not have a negative slope, the best you can do is a zero slope, which will pass through the point $(x,y)$, thereby uniquely determining it.',), ('I have deleted my comment and expanded it slightly into a full answer.',), ('Well, any linear-programming package would work. That leaves you with a lot of options. :)',), ("@Cardinal How would you recast this as a linear program?  It's not evident how to do it even in trivial cases (such as two data points and one parameter): there are no constraints and the objective function is nonlinear.",), ('**Key phrase**: Chebyshev approximation. (More to follow. The idea is to *introduce* an extra variable then turn objective into the constraints.)',), ('@cardinal You mean this one :http://mathworld.wolfram.com/ChebyshevApproximationFormula.html It seems quite complicated.',), ("Well, it's a bit related, but not germane to this problem. Your problem can be solved with a simple LP. As soon as I can get to a computer, I'll post an answer.",), ('@cardinal Thank you very much.',), ("@whuber : On second look, this isn't really a duplicate actually. The solution for this problem is not evident for the other. And the solution of the other question is far too complicated for this one, as shown in my edited answer.",), ('@Joris The functional form `S ~ b0 + (A > T) * b1 * (A - T)` is equivalent to `S ~ b0 + b1*max(0,A-T)`.  Changing notation to $x$ = A, $\\\\delta$ = T, $\\\\beta_0$ = b0, $\\\\beta_2$ = b2, and setting $\\\\beta_1=0$ gives a special case of the problem solved in the duplicate post.  If you have improvements to suggest for the solution there, then it would be best also to post your reply there so we can keep this common thread together.',), ("@whuber : off course, my mistake. I'll add an answer there.",), ('A derivation is in the Wikipedia article: http://en.wikipedia.org/wiki/Least_squares#Weighted_least_squares',), ('The [Durbin-Watson statistic](http://en.wikipedia.org/wiki/Durbin%E2%80%93Watson_statistic) is a test for serial correlation: that is, to see whether *adjacent error terms* are mutually correlated.  It says nothing about the correlation between your X and your Y!  Failing the test is an indication that the slope and p-value should be interpreted with caution.',), ("Ah, ok. That makes a little more sense than whether the two variables themselves are correlated...after all, I thought that's what I was trying to find using the regression. And that failing the test indicates I should be cautious interpreting the slope and p-value makes even more sense in this case! Thanks @whuber!",), ("I would just like to add a slope can be very significant (p-value<.001) even though the relationship is weak, especially with a large sample size.  This was hinted at in most of the answers as that the slope (even if it's significant) says nothing about the strength of the relationship.",), ('dollar signs make you enter an equation environment and is why things are randomly getting italicized.',), ('formatting: to get `code`, indent by 4 spaces.',), ("if you use the 4 spaces indenting, you can put stars and dollar signs in there and they will show up as such. if you use them outside the code formatting, they will be treated like markup. If you don't want a complete `code` line, use backticks: `this is code with a $ and *`",), ('It is good that you have used a reproducible example. You could make the example even better by including `set.seed(1)` (or whatever number you like) before running random number generation, so that everybody gets exactly the same results as you (not that it matters much in this case, though).',), ('Thanks for your suggestions, wolf.rauch.',), ('Just two small comments. You wrote "In order to find the estimate of \'lifespan\' when the value of \'weight\' is 1, I add (Intercept)+height=63.64319". Note that this is the estimated average lifespan when weight is = 1 **and height = 0**. That is probably not very meaningful. Also, predictions outside of the range of the observed values of the independent variables need to be treated with caution (weight is between 50 and 120, so weight = 1 is also not very meaningful). Just some side-notes and probably things you already knew. But just in case ...',), ("Wolfgang: I am aware of these points, but it never hurts to point them out anyway, as they are important! :) [also in case others who read this thread weren't aware]",), ("@cardinal, say I was looking for the most affordable diamonds regardless of size (number of carats). i.e. trying to find the best deal for cut and clarity. You're right that the last plot doesn't help much, but it's just confirming that the data is detrended for carats. I'd then do `d$lprice_detrended_carat <- mean(d$lprice) + resid(detrend)` and start looking at that. Maybe this doesn't make the most sense with logs, but the data I'm _actually_ applying this to is not log transformed.",), ('How do you measure accuracy of your current model? In general this question is unanswerable without knowing details about what you are trying to model. If the true model is linear, then linear regression will be the most accurate, for appropriate definition of what is accurate.',), ('@mpiktas Those vectors (x, y, z, v) are stocks. I mean... historical prices of 4 differents stocks. Example 4 stocks on nasdaq, 4 stocks on nyse etc etc.',), ('I\'m a bit skeptical when I hear "stock prices" and "accuracy" in the same sentence. Stock price prediction is the deep, dark morass of statistics, and I\'m not sure that anything in the market can be considered linear.',), ("@Dail Your accounts were merged once again (Thanks @Gavin!). Please don't create new accounts each time you ask a question, but use this registered one instead.",), ('Note that if you have `n` distinct points, you can fit a polynomial of degree at most `n-1` to fit the data exactly.  However, in practice this is a *terrible* idea that will lead to overfitting.  You are probably concerned with accuracy on out of sample (unseen) data.',), ('@Dail Please focus more at reading answers than asking the same question again and again. Total least squares is not a magic wand that will make linear regression a universal, robust model -- you should first try to identify *why* lm is not satisfying (outliers? nonlinear dependence? leverage?) and then try to select appropriate solution.',), ('because of your variable type (0,1), you should use sth like ridge regression. plenty of text are around ;-)',), ('Binary variables are neither necessary nor sufficient to suggest ridge regression.',), ("If the variables aren't all on the same scale, then the $\\\\beta$'s will be difficult to represent all on one plot. If the variables are all standardized, for example, you could make a plot with side-by-side dots for the point estimate and error bars around the bars - the numerical values on the $y$-axis and the variable names on the $x$-axis",), ('Does model 2 also have the main effects from model 1 in it?',), ('Yes, it has the two main effects + the two interactions.',), ("It's possible, also are you sure the coefficient means a decrease of 230% or 2.3%?",), ('im quite sure that the coefficient means a decrease of 230%, as I am using a log transformed dependent variable and there is no continuous independent variable - but of course, I am not certain.',), ('@Andy W I thought if I switch the reference with the included binary it will probably turn around the result, to a positive number, showing the increase between the "before reference" to the "before independent". however I am not quite sure if its not too easy just to interpret the coefficient as an percental change, I should probably look into the formulas again - any ideas, whether this interpreation makes sense?',), ('This is not an answer, but do note that the $t$-distribution approaches the normal distribution as the degrees-of-freedom parameter $\\\\nu$ grows larger. Past $\\\\nu \\\\geq 30$, there is no appreciable difference, particularly in most hypothesis-testing frameworks. The limiting behavior is "from above" in the sense that the if $T \\\\sim t_{\\\\nu}$ and $Z \\\\sim \\\\mathcal{N}(0,1)$, then $|T|$ is [stochastically larger](http://en.wikipedia.org/wiki/Stochastic_ordering) than $|Z|$.',), ('(1) You might find [R-squared: useful or dangerous?](http://stats.stackexchange.com/q/13314) informative. (2) Concerning the phrase "significantly influence," you should also read some of the threads found by searching this site for "causality," including [Statistics and causal inference?](http://stats.stackexchange.com/q/2245).',), ('@whuber: (+1) this question originated on math.SE, and I pointed the OP to the same question you have linked to. I think that prompted the deletion of the question over there and the move here.',), ('What is the motivation for this question?',), ('Off the cuff, it seems the solution must be a line corresponding to one of the segments of the *least concave majorant* of the data.',), ('[Here](http://stats.stackexchange.com/questions/12546/software-package-to-solve-l-infinity-norm-linear-regression) is a similar question and answer. (Forgive the self-citation, please.)',), ("I'm confused by your edit. Did you mean to put the $\\\\max$ function inside the sum? Also, in the original equation, are you wanting to allow for an intercept or not? Otherwise, by forcing the line through the origin, the answer is trivial.",), ('No, the max function is supposed to be outside the sum. $P$ is the dependent variable, $h_i$ are the independent variables. My combined (hedging + original) portfolio starts with value $ \\\\sum_{i=1}^n \\\\alpha_i h_i(0) + P(0)$ and I want to choose $\\\\alpha_i$ so that at the end of the period $[0,t]$ the negative part of the change in value of the portfolio is minimised.\n\nI have just realised that this is trivial, however, if there are no constraints on $\\\\alpha_i$ which, realistically, there would be.',), ('In fact, I would also be interested in minimising the "end-of-day" value of that quantity over a period of time i.e. minimising.\n\n$ \\\\sum_{t=0}^T max\\\\{\\\\sum_{i=1}^n \\\\alpha_i [h_i(0)-h_i(t)] + [P(0)-P(t)], 0\\\\}$',), ('Thanks for the clarification. That helps.',), ('Normally one uses $t$ for time, which would be the "x-variable," so I\'m wondering about the connection between your question and the equation you have provided.  Where in that equation is the one month return?',), ('@whuber Oops!  Corrected.',), ("Thanks.  I'm still puzzled, because now the notation suggests you have a sequence $(r_t)$ = $r_1, r_2, \\\\ldots, r_n$.  With such a situation it's simply impossible that every pair $(r_t, r_{t+1})$ consists of a (one day, one month) return (except in the trivial case $n=2$), because the case $t=1$, where $r_2$ is the y-variable, implies $r_2$ is a one-day return, but then $(r_2, r_3)$ has a *one-day* return for its x-variable, not a one-month return as stated.",), ("It is a standard result of M-estimation theory. The basic idea comes from taylor approximation of the minimizing function. You can look at the derivation in [Wooldridge's book](http://books.google.com/books?id=cdBPOJUP4VsC&lpg=PP1&dq=cross%20sectional%20and%20panel%20data%20wooldridge&hl=fr&pg=PA350#v=onepage&q&f=false), or check out van der Vaart's [Asymptotic Statistics](http://books.google.com/books?id=UEuQEM5RjWgC&lpg=PR1&dq=van%20der%20vaart%20asymptotic%20statistics&hl=fr&pg=PA51#v=onepage&q&f=false).",), ('Some ideas can be gleaned from this [question](http://stats.stackexchange.com/questions/7308/can-the-empirical-hessian-of-an-m-estimator-be-indefinite) too.',), ('Do you mean how to check if the residuals of a linear regression have a constant variance? The residuals from a regression will of course per definition be random...',), ("The residuals can't be constant.  The math of a linear regression is such that some must be positive and some negative.  Perhaps you mean something else.",), ('To learn about tests for constant variance, you can look up the terms homo- and hetero-skedasticity on this site.  Maybe you had in mind a constant mean.  As in, residuals should not be generally high for some values of predicted Y but generally low for others.  You could test this by dividing your predicted Y into discrete groups and running an anova using the residuals as the dependent variable.',), ('Closing, because this question is either empty (interpreted as written) or it covers previous ground.',), ('In particular, I read that the degrees of freedom are N-p-1 where p is the number of parameters, how do we get this?',), ('Regarding degrees of freedom, look at [this CV Q&A](http://stats.stackexchange.com/questions/884/what-are-degrees-of-freedom) or [this nice explanation](http://www.jerrydallal.com/LHSP/dof.htm) referred to in one of the answers.',), ('Re-write your variance as $\\\\hat{\\\\text{var}}[\\\\hat{\\\\beta}]=[(X^′X)^{−1}/(N-p-1)]\\\\sum_i(e_i^2)=\\\\hat{\\\\sigma}^2(X^′X)^{−1}$.  So it seems like your question is more related to why do we estimate $\\\\sigma^2$ as $\\\\frac{1}{N-p-1}\\\\sum_ie_i^2$?',), ('You can find a rather complete list in William Berry\'s little book on "Understanding Regression Assumptions": http://books.google.com/books/about/Understanding_regression_assumptions.html?id=4_Aeo9JdzCMC',), ('While respondents have listed some good resources, it is a difficult question to answer in this format, and (many) books have been devoted solely to this topic. There is no cook book, nor should there be given the potential variety of situations that linear regression could encompass.',), ('Technically, (ordinary) linear regression is a model of the form $\\\\mathbb{E}[Y_i] = \\\\mathbf{X}_i \\\\beta$, $Y_i$ iid.  That simple mathematical statement encompasses *all* the assumptions.  This leads me to think, @Andy W, that you may be interpreting the question more broadly, perhaps in the sense of the art and practice of regression.  Your further thoughts about this might be useful here.',), ('I assumed (perhaps wrongly) that the assumptions the OP talks about are in regards to making *valid inferences* based on the $\\\\beta$ estimates, which require greater constraints than those that simply allow the $\\\\beta$ to be identifiable (as mentioned by @whuber). It would require clarification from tony though as to whether my assumption is correct (and if it is my first comment still stands, in that it is so broad it would be difficult (but not impossible) to write an answer with a scope that wide).',), ("@Andy W I wasn't trying to suggest your interpretation was incorrect.  Your comment suggested a way of thinking about the question that goes beyond technical assumptions, perhaps pointing towards what may be needed for valid interpretation of regression results.  It wouldn't be necessary to write a treatise in response, but even a list of some of those broader issues could be illuminating and might expand the scope and interest of this thread.",), ('@whuber, if $EY_i=X_i\\\\beta$ this means that the means are different for different $i$, hence $Y_i$ cannot be iid :)',), ("@mp You're right; I should have more carefully stated that $Y_i - \\\\mathbf{X}_i\\\\beta$ are iid.",), ("Is your interest in the data or in writing the program?  If it's the first, then forget about Python: use a statistics package and learn how to do linear regression, because it's far more than just a formula: you need capabilities of data manipulation, re-expression, and visualization; ways to obtain regression statistics and residuals; computations of regression diagnostics; and more ways to visualize and check the results.",), ("Ultimately it's in the data but my understanding was that for things like linear regression one could either use R or use python with NumPy or SciPy to do similar things.  As a programmer I'd prefer python although if it's considerably harder in python then I'm open to using R (I've already used R some).",), ("R is immediate and interactive: for this kind of work it's really commanding, not programming.  Sure you can do the job in Python (and even Excel for that matter), but the important thing here is for you to learn about regression: the program/command problem is trivial in comparison, but no amount of programming will overcome lack of understanding of how to use the software properly.",), ('Fair enough.  I probably will learn to do this in R first and then figure out how to translate that into python.  I did find this R tutorial which seems to cover what I want: http://www.gardenersown.co.uk/Education/Lectures/R/regression.htm',), ("With regard to visualization I'm assuming you mean plotting.  In the case of multiple variables, is the idea that you plot each independent variable against the dependent variable one at a time?",), ("What group to set reference doesn't matter for significance test for the whole model. But it does matter for the tests of individul predictors (the other groups).",), ('@ttphns: Then why am I getting different p-values  for the F test depending on the reference group?',), ('Do you include any interactions?',), ('@whuber: No I am just testing a dependent variable on 1 independent variable with different levels.',), ('It might help to give details of the "linear regression."  Important aspects include whether you include a constant and how you encode the groups.  For your second question, investigate [power](http://stats.stackexchange.com/questions/tagged/power) related threads.',), ('@whuber: I am using 1 and 0 as the coding variables. The intercept will be the reference group.',), ('Are you saying your model is $y =\\\\beta x +\\\\varepsilon$ where $x$ takes on only the values $0$ or $1$ (and $\\\\varepsilon$ is a random error)?',), ('@whuber: yes. thats right. So the intercept is the reference group. But when I change the reference group the F values changes. So what should I use as the reference group?',), ("Ad 1) In linear regression, I normally understand ANOVA as a measure of goodness of fit of the model, i.e. to decide whether the model (regression line) explains substantial part of total variability. The question, whether it is equivalent to all slopes being zero, is really very interesting. Ad 2) it looks I'm getting almost the same p-values for t-test and regression ANOVA in this case. Really interesting theorem!",), ("It's number two.",), ('@Fojtasek haha, yeah. I just figured it out and answered my own question. I think the actual act of writing down step by step all the issues I was having is what allowed me to realize the answer XD',), ('Some of the responses to this related question, [What is a complete list of the usual assumptions for linear regression?](http://stats.stackexchange.com/q/16381/930), are not limited to simple linear regression. Does that help?',), ("That the residuals from the *good* part of the data are approximately symmetric, other than that you're safe! PS: you might wanna link to a more recent article, there as been gigantic leaps in computational tractability in this filed in the last 10 years (specially in higher dimensions)....",), ('"Are there hard-and-fast rules for how much information to report" - it really depends on what you want to do after the regression. One might be happy with just the correlation coefficient; one might need the Durbin-Watson value on top of that, and still another one might want to see the diagonal of the hat matrix... it really depends.',), ('Some organizations do have rules.  See the [APA guidelines](http://my.ilstu.edu/~jhkahn/apastats.html) for instance.',), ('Thanks. Will look for a more recent paper to update the question.',), ('you can use s+ or R and use predict function after modeling your data with lm',), ('These bands are automatically produced by `LinearModelFit`: see the help page, especially the introductory section titled "Properties of predicted values include:".',), ('Please make the question self-contained. I do not have access to the book.',), ('@varty: The book is freely available (legally) at the link provided by the OP. That said, I still agree the question should be made self-contained if for no other reason than that future versions, or even printings, of the text may have different equation numbering.)',), ('Sorry this is the eqn I am referring to',), ('$$\\\\beta|(\\\\hat{\\\\beta} -\\\\beta)^TX^TX(\\\\hat{\\\\beta} -\\\\beta)\\\\leq \\\\sigma ^2\\\\chi ^2$$',), ('here beta_hat are the estimates for beta, and sigma is the variance of the error term. On the right of sigma_square is the chi_square distribution. Please let me know if I wasnt clear.',), ("@cardinal The book may be freely available but it would be *nice* on the OP's part to take the effort to make the question self-contained to increase the chances of getting an answer. Speaking for myself, I am less inclined to answer a question that requires me to click a link, go to a specific page in that book, understand the context, navigate back to answer the qn etc.",), ('@varty: I agree. My intent was not to be argumentative, but simply to (kindly) point out that you *did* have access to the book, in case you were interested.',), ('This appears to be the same question as http://stats.stackexchange.com/questions/5700/finding-the-change-point-in-data-from-a-piecewise-linear-function.  If it differs in any substantial way, please let us know by editing your question to reflect the differences; otherwise, we will close it as a duplicate.',), ('I have edited the question.',), ('I think you can do this as a non-linear optimisation problem. Just write the equation of the function to be fitted, with the coefficients and the knot locations as parameters.',), ('can you give examples how your "species x year" matrices look like?',), ("... and clarify what 'distance between years' means?",), ('Even the ordinary regression fit may be invalid because the $(n^2-n)/2$ points reflect at most $n$ independent observations, causing extreme interdependence.  So, testing significance appears to be the least of the concerns here.  It looks like you need a different approach to the analysis altogether.',), ('@psj: My matrices consist of species (several dozens) in rows and years (25) in columns. For the MCPP I shuffle the column positions randomly (10,000 times).',), ("@onestop: Dissimilarity between years were measured by Hellinger distance, that is, Euclidean distance of Hellinger transformed data. Hellinger transformation: N'ij = √(Nij/∑Nij) where Nij is the population size of species i in year j, and ∑Nij is the sum of individuals across all species in year j.",), ("@whuber: I'd be eager to get a suggestion...",), ('I cannot offer an alternative because I do not know what\'s in the data (what exactly has been recorded about these "species"?) and knowing the purpose of the analysis.  Your research might be better served by reformulating the original question to provide this information.',), ("@whuber: It's population size (=number of individuals) of species per year (and they are bird species, by the way). The aim is to check species composition change over time. When there is directional change, then years further apart should show larger Hellinger distances than years closer together.",), ('Presumably, in the second formula, $\\\\le$ is a typo for $\\\\sim$ and $p+1$ is the number of parameters (including the constant), right?',), ('Yes, and Yes. Can you make sense of it though, how does the second formula come?',), ('The left hand side in (1) is a sum of squares of $p+1$ normal variables.  By definition, a chi-squared distribution describes a sum of squares of $p+1$ standard normal variables.',), ('How do you get that? How can we take the XTX matrix out from the variance term?',), ('Factor it: because it is symmetric positive-definite, you can write it as $X^TX = UU^T$ for an invertible $p+1$ by $p+1$ matrix $U$.',), ('Wait, I am grossly missing something here, the matrix encodes the correlation between the different betas right, now if we take the matrix out we are left with a variance term multilplying the Identity matrix, and a new scaled variable on the left which is $$(X^TX)^(-1/2)(\\\\betahat-\\\\beta)$$ this variable has no corrleation structure? Since the variance covariance matrix just becomes sigma^2*I',), ('We are assuming that the variable I wrote above is normaly with mean 0 and a variance conariance matrix of sigma^2*I. My question is  that how can this be done, where do the cross correlation term go?',), ("You would benefit greatly from working an actual problem.  Why don't you fit a line through the points $(0,2),(3,4),(4,8),(7,10)$.  (I chose this for the easy arithmetic.)  You should compute $U = \\\\{\\\\{2,0\\\\},\\\\{7,5\\\\}\\\\}$.  Then you might see where the correlation appears.",), ("The coefficients would be easier to interpret if you used the natural log. Also, you should worry about the linearity assumption seeming to make sense. You don't need normality of anything (variables or error terms) so long as you have enough data---the central limit theorem typically comes to your rescue.",), ('Actually, @Charlie, you do need normality of some statistics if you want the t-tests and confidence intervals to be correct.  The log transformation is strong enough to raise doubts unless the dataset is quite large: the approximate normality of an estimated coefficient of a log response will imply the *non* -normality of the coefficients for the response itself and *vice versa.*',), ("@whuber, True, if you undo the transformation and try to interpret that result, it may not have a normal distribution; I forgot about that part of the question. But I'd recommend bootstrapping that distribution, rather than forcing yourself to find a model that has errors that are normally distributed.",), ('You may wish to check out this similar question on CV: http://stats.stackexchange.com/questions/10079/rules-of-thumb-for-minimum-sample-size-for-multiple-regression/12058#12058',), ('Thanks: the problem is getting clearer.  I\'m still not sure what exactly you mean by "check" the composition change over time and why that requires making all possible comparisons between species.  Could you perhaps share a high-level description of your scientific objective for this analysis? Also, there is a preliminary issue concerning what a "population" is.  I doubt you truly know the populations: these must either be estimates or summaries of observations, but which are they precisely?  That will influence the interpretation of any analyses.',), ('I added a tag for splines, in case anyone wants to list rules of thumb for them.',), ("I'm no sure that I understand what you mean by refining. I'm also a little confused of how to interpret $\\\\frac{A-B}{A+B}$. Perhaps having standard deviation at the bottom would be more intuitive (although with only two stores it probably will be the same)...",), ("Do you really want just to *predict* Y, or do you want to explain the mechanism by which one or more variables *explain* Y?  If the former, then there's no reason not to use 4).  If the latter, you'll have some work to do to make it clear to your audience just how 4) is helpful.  It's not always the case that the most useful solution is the one with the highest r-squared.",), ('I want to get the best r-squared. All four cases are sound, but I want to understand if the highest correlated variables produce the highest r-squared in a multiple regression, or if, more in general, analyzing each independent variable correlation to Y is a good way to prepare the best fit model.',), ('Also I am aware that there are ways to select/discard the independent based on anova and the t-test, after the regression is run. I would like to understand if the general idea of increasing the correlation against Y like in my example is a sound method to increase r-squared.',), ('Robert, R-squared is meaningless when you are re-expressing variables.  For some discussion of R-squared, see http://stats.stackexchange.com/questions/13314/r-squared-useful-or-dangerous/13317#13317.  You should prefer to obtain a model which has a good fit overall and acceptably small residuals.',), ('This is a broad, diffuse set of questions.  You would be able to focus them better, and learn more from us, by first learning a little about multiple regression.  A Web search will turn up plenty of sites where you can start.',), ('I agree that this is too broad to be answered at the moment. Unfortunately, the CPAN page makes it clear that there will be no support and that users are expected to be familiar with OLS estimation. Take a look at the section related to multiple regression in [The Little Handbook of Statistical Practice](http://www.tufts.edu/~gdallal/LHSP.HTM), and if you are still in trouble with a particular aspect try to narrow down the above questions.',), ('thank you - please see my edit above.',), ('My "normal interval workbook" at http://www.quantdec.com/envstats/software contains macros "Tol2," "NCTDist," and "NCTInv" to evaluate the non-central T and illustrates how to compute normal-theory tolerance intervals with it.  See the [K Tables] sheet.',), ('That doesn\'t seem to me to be a sufficiently well-defined problem; for example, if you add one feature, but it\'s highly informative w.r.t. the residuals of the "current" model, the std. errors on the existing coefficients will go down, perhaps substantially, even with no increase in sample size; if the additional feature is sufficiently close to orthogonal to the residuals, it may increase the std. errors of the existing coefficients.',), ("Have you thought of using Poisson regression instead?  It's naturally indicated with dependent count data and your success with a log transformation is consistent with Poisson distributions.  The coefficients would be interpreted in terms of proportional increases in expected probability of missing a day of school.  One advantage is that no special treatment of zeros is needed (although it's still a very good idea to look at a zero-inflated alternative model).",), ("Hi Whuber, Yes, I was thinking about Poisson regression but wasn't sure about this or opting for negative binomial regression. I guess negative binomial as the data is over dispersed - i.e. the mean is lower than the variance in the dataset (hence positive skew). Also, strictly, there is an upper limit on the number of school session in the year, whereas Poisson assumes an unlimited denominator? Or do you still think Poisson is more appropriate? Unfortunately SPSS doesn't support zero inflated models as far as I've seen...) Thanks Whuber :)",), ("I don't see a problem with the unlimited support of Poisson distributions: it's similar to using Normal distributions to model, say, values that must be nonnegative.  Provided the chances associated with impossible values are tiny, it can be a good model nevertheless.  Negative binomial is the standard alternative to Poisson used to test goodness of fit and overdispersion; it's a good idea.  If SPSS is too limited, use something else!  (`R` has packages for zero-inflated models; [search this site](http://stats.stackexchange.com/search?q=%2Bzero+%2Binflation+%2Bpoisson).)",), ("I agree with @whuber  I think you probably want a ZIP or ZINB model. I'd just add that they are also available in SAS via PROC COUNTREG (in ETS) and, starting with SAS 9.2, in PROC GENMOD (in STAT)",), ("Hi Whuber, Peter, thanks both for your input. Unfortunately I only have SPSS and can't acquire SAS due to budgets, though obviously I could look at R. In the absence of a zero-inflated model, do you think Poisson/NB regression are suitable for dealing with zeroes - if not, is it reasonable to 'just add 1' as a constant to all values? Whuber, would it be possible to elaborate on 'The coefficients would be interpreted in terms of proportional increases in expected probability of missing a day of school' please? Thanks both :)",), ("That's not true in general. Perhaps you're just seeing that in your data. Paste this code: y = rnorm(10);\nx = rnorm(10);\nlm(y~x);\nlm(x~y); into R several times and you'll find it goes both ways.",), ('That\'s a bit different from what I was describing. In your example y wasn\'t a function of x at all, so there\'s not really any "slope" (the \'a\' in my example).',), ('lm(y~x) fits the model $y = \\\\beta_{0} + \\\\beta_{1}x + \\\\varepsilon$ by least squares (equivalent to ML estimation when the errors are iid normal). There is a slope.',), ('Your question is asked and answered (sort of) at http://stats.stackexchange.com/questions/13126 and http://stats.stackexchange.com/questions/18434.  However, I believe nobody has yet contributed a simple, clear explanation of the relationships between (a) regression of $Y$ vs $X$, (b) regression of $X$ vs $Y$, (c) analysis of the correlation of $X$ and $Y$, (d) errors-in-variables regression of $X$ and $Y$, and (e) fitting a bivariate Normal distribution to $(X,Y)$.  This would be a good place for such an exposition :-).',), ("Use this modified example: y = rnorm(10);\nx = .5 + .1*y + rnorm(10);\nlm(y~x);\nlm(x~y); so there is a relationship. The phenomena you're describing still doesn't happen every time.",), ('Of course Macro is correct: because x and y play equivalent roles in the question, which slope is more extreme is a matter of chance.  However, geometry suggests (incorrectly) that when we reverse x and y in the regression, we should get the *recipocal* of the original slope.  That never happens except when x and y are linearly dependent.  This question can be interpreted as asking why.',), ("In the modified example, it's not really clear to me why y is sampled from rnorm; for simplicity let's just say it's 1:10 (or better yet, 1:100 so that there are more data points). In that scenario, when I've used a sufficiently large number of observations, I've gotten a steeper slope each time.",), ('Is there a question in here somewhere that I missed?',), ("A well-formatted and thoughtful question is more likely to draw the attention of other users. Please, work on the quality of your post (there's some hint on our [FAQ](http://stats.stackexchange.com/faq) and the [editing help guide](http://stats.stackexchange.com/editing-help)), and I'll second @Dason's comment: What's the question? In the absence of action on your part, we'll have to close this post as *non-constructive*.",), ("You're going to have a problem with that particular formulation; by setting `beta1[i] <- y[i] / x1[i]` and `beta0 <- 0`, the errors will all be zero.  You've got $N+1$ parameters and only $N$ observations...",), ("Of course, you're right. The slopes should be at the group level, not at the individual level; fixed.",), ('What is the distinction between "beta1[group[i]] is drawn from a distribution with zero mean" and "all the beta1[] are drawn from a distribution the mean of which is fixed at zero"?  If all the beta1[] are drawn from a distribution the mean of which is fixed at zero, then necessarily each beta1[group[i]] is drawn from a distribution the mean of which is fixed at zero...  but the posterior mean for each beta1[group[i]] won\'t be zero unless a miracle occurs, if that\'s your concern.',), ("In the former case, I'm expressing my prior belief that the beta1[] are drawn from a distribution with zero mean. In the latter case, I want to constrain the posterior means of each beta1[j] such that if I take the mean of these posterior means, I'll get zero.",), ('That clarifies things, thank you very much.',), ("Isn't lwr and upr the confidence interval? For each of your new ind values, 3,5,0.25 respectively?",), ('@Spacedman - well not exactly...I need the CI for ind not for the height ...because I am calculating the ind (the X) given a new value of the height. And I need the CI for the new X=(Y-beta0)/beta1',), ('@Spacedman- is it that value X +/- the standard error at 95 %CI ?',), ('If you have errors in your design variables, ordinary least squares is not very good. The usual LS estimators are actually biased in this setting. A good way out is to use a [errors-in-variables](http://en.wikipedia.org/wiki/Errors-in-variables_models) model, in which case you also should get the answer to your question for free.',), ("The answer to this depends highly upon your goals and requirements: are you looking for simple association, or are you aiming for prediction; how high are you on interpretability; do you have any information on the variables from other publications that could influence the process; how about interactions or tranformed versions of the variables: can you include those; etc. You need to specify more details on what you're trying to do to get a good answer.",), ('Based on what you asked, this will be for prediction. Influence on other variables just offers possible association. There are no interactions between them. Only one value needs to be transformed, and it has been done.',), ('that looks great, I will try it. Thanks, I am not sure how to upvote your answer here?',), ('It\'s not quite clear what your problem is here.  What are these "other weights z"?  Also, it sounds like your issue with the constraints on beta is actually the problem rather than anything to do with the weighting.  So perhaps you should be a bit more specific about what those constraints are.',), ("I've edited my question...I basically want to know where I can read about/ learn about regression with these types of constraints.",), ("I'm not the swiftest with mathematical notation...what is p? What is the significance of p being multiplied by 1? And is this a mixture design?",), ("@rolando2: Not a mixture model..and p isnt multiplied by 1..it's just px1 in dimensions...no particular significance im just saying some of the coefficients to be estimated must satisfy linear constraints..in the above notation $p$ just happens to be the number of such linear constriants.",), ('Pretty much any linear regression theory textbook will provide the derivation and (closed-form) solution for finding the least-squares estimates of the coefficients under linear equality constraints. See, for example, G. A. F. Seber and A. J. Lee, *Linear Regression Analysis*, 2nd. ed., **Sec. 3.8**, pp 59-62.',), ("Not a stupid question.  I'm not sure though whether you have either: a system of two equations (because y is somehow caused by x1 and x2, and x2 is also caused by x1 and y); or just one equation,about which you don't mind the causality, that you have switched around and want to estimate the second for some reason (what?).  If the former, you have a problem, as you won't be able to fit both equations simultaneously (as they both have the same sets of variables - not possible to separate out the two way causality).  If the latter, it's not clear why you don't just fit your first equation.",), ('I understand the task as follows: we are given a linear regression model ($y$ on $x_1$, $x_2$) under G-M assumptions. We can estimate it\'s parameters through ols. After that we consider the regression of $x_2$ on $y$, $x_1$. It is asked if the reciprocal of $\\\\hat{\\\\beta_2}$ (which is an estimator of $\\\\beta_2$) is an unbiased estimate of the parameter $\\\\gamma_2$ in the new regression model. By "why you don\'t just fit your first equation" you mean to plug $x_2$ from second equation into the first one?',), ("Is this homework?  It's hard to see why this would be a necessary thing to do in an applied situation.",), ("yes, it's my HA",), ('By "fit your first equation" I just meant that if you have values of y, x1 and x2 why do you need to muck around with reciprocals of anything, why not just use OLS to fit whichever of the two equations you want estimates for.  But the exercise looks like one to see the relationship between different estimators, not just a practical one of trying to fit a model.',), ('So can I run a regression of $x_2$ on $y, x_1$ and compute ols estimate of $\\\\gamma_2$ if I am not sure about G-M assumptions?',), ('Is there a theory that says what predictors you should include? If you have a lot of variables that you have measured, and no theory, I would recommend holding out a set of observations so you can test your model on data that was not used to create it. It is not correct to test and validate a model on the same data.',), ('How big were the training and validation sample sizes? Maybe the model without an intercept was better just by chance.',), ("I'm confused with your terminology. Could you give a specific example?",), ('The training sample size was 289 whereas the validation sample size was 406. By the way, how to determine the best training and validation sample sizes?',), ('For example, BMI=BW/BH^2 and obesity=BW^2/BH are two mathematically-coupled variables. Is obesity a spurious predictor for BMI and should never be used?',), ("Obesity is defined as above a specific BMI cut-point, and it's categorical rather than continuous, so I am not familiar with your second formula. Given the definitional association, I would be concerned to see either being used in a regression to predict the other.",), ('"Obesity" as defined above is a continuous (instead of a categorical) variable and is only a (in)convenient term used to illustrate my point. You can call it "new-BMI" or any other terms.',), ("Can you give a fuller example? This seems like a pretty silly thing to do, so I would like to know more information about the particular situation you're thinking of. Can you add a link to your question to illustrate with a specific example?",), ('For example, the dialysis dose (Kt/V) and protein catabolic rate (PCR) are both calculated from pre-dialysis and post-dialysis blood urea nitrogen levels in hemodialysis and are thus mathematically coupled (The formulae are given in the article "Mathematical Coupling and the Association Between Kt/V and PCRn. Seminars in Dialysis 1999;12:S20-S28" http://onlinelibrary.wiley.com/doi/10.1046/j.1525-139X.1999.90204.x/abstract My question is: can PCR be used as a covariate to adjust for the effect of the key predictor variable (such as systolic blood pressure level) on Kt/V?',), ('Jinn-Yuh, this may sound philosophical but I think it gets to part of the issue: what\'s the difference between "mathematically coupled" and not independent?  In many cases we use sets of explanatory variables that have clear lack of independence.  We can remove their correlations with linear transformations (to orthogonalize them).  This effectively expresses the original variables as "mathematically coupled" versions of the orthogonal variables.  What\'s any different about this situation and (dialysis dose, PCR) or (BMI, obesity)?',), ('By "weights" are you referring to coefficients of your independent (explanatory) variables? Not sure what you mean by "attributes". Does your binary regression feature only binary independent variables? An example of your input data and your regression results would help to specify what you want to do.',), ('What software are you using?',), ('Can you share your purpose in comparing the fit of the different models? To develop a predictive model? To describe a particular data set? To provide evidence for or against a particular theory? You might choose different comparison approaches depending on your purpose.',), ('I\'m a bit confused by your notation. Is "x1, x2...x(n＋1)" referring to random variables or observations? And what is the significance of "(n+1)"?',), ('data, observations, thank you!',), ('Anne, at this point it is simply for description.  I would like to quantitatively be able to say that a non-linear fit is preferred.',), ('Presumably you mean "H0: B1 and B2 are the same" and you are looking for statistically significant evidence to reject H0.',), ('You are right Peter. Thank you!',), ('Why do you want to take derivative???',), ('Don\'t you have access to x1,...,x5? I do not get this "x1+...+x5" representation.',), ('Yes, but I just put them there as placeholders as an example. They are just variables. I have thousands of variables in my database, so writing this code by hand is not possible.',), ("@Xi'an Alexander has a character string representing the RHS of a model formula in R. The problem then is how to create a valid R formula from this character string representation.",), ('+1 From me - clear question with simple example and evidence of effort in trying to solve the problem. Not sure why this was down-voted?',), ('Michelle, I use R.',), ("Is this a homework question?  If so, please add the homework tag... otherwise let us know it's not.",), ('As a hint - what is the distribution of $\\\\hat{\\\\beta}_1$?  What is the relationship between that distribution and the $\\\\chi^2$ distribution (if any)?',), ("Not homework, it's from the textbook I'm using to try to teach myself linear regression.",), ('Well, $\\\\hat\\\\beta_1$ ~ $N(0, \\\\sigma^2/S_{xx})$ since $\\\\beta_1 = 0$, and I know that the $\\\\chi^2$ distribution with $k$ degrees of freedom is the sum of the squares of $k$ standard normal random variables... I was thinking that $\\\\hat\\\\beta_1^2$ by itself is a chi-squared random variable with one degree of freedom, but it has a non-standard variance. And how does $S_{xx}$ play into everything?',), ('Note that $n\\\\hat\\\\sigma^2/\\\\sigma^2$ is a sum of standard normal random variables, while $n\\\\hat\\\\sigma^2$ is a sum of $N(0,\\\\sigma^2)$-random variables. And $\\\\hat\\\\beta_1\\\\sqrt{S_{xx}}/\\\\sigma$ is distributed as $N(0,1)$.',), ("Thanks, I did figure it out eventually. I hadn't realized that $e_i$ ~ $N(0,\\\\sigma^2)$, but once I noticed that then I could divide it out as you said and then the numerator worked as well.",), ('Hi Max, how is the score from the test use?. Assuming the possible total score ranges from 5 to 15, are the incremental changes in score important?',), ('Hi Michelle, I\'ve added what I know of how the score is calculated and tried to clarify the question some. I think the main discussion is if it\'s OK to have a non-normal variable in a regression as the outcome or if this breaks some fundamental assumption. I\'m not that familiar with multinomial regression so I\'m also a little curious of how this impacts the interpretation, it\'s much easier to have a score being ".2 better in females" than "20% more improved in females and 30% more mixed results in females"',), ('Personally I find it rather difficult to assess normality from a histogram (or a kernel density plot). I would never rely on them as an "ultimate" evidence. QQ plots are much more powerful for this purpose.',), ('I\'m not familiar with this usage of the word "tariff"; I\'ve tried googling it but all I can find are references to taxation :) I think you just mean the rules/weights by which the score is produced, but if I\'m wrong, let me know.',), ('Sorry if that\'s unclear - I guess it\'s more commonly used in Swedish where it\'s like a taxtable - depending on your answers you\'re taxed differently. What I mean is simply the way the answers are translated into a "continuous variable". I\'ve changed the question to clarify.',), ("The generic term for this is [Errors-in-variables models](http://en.wikipedia.org/wiki/Errors-in-variables_models). The particular case you're considering looks like [Deming regression](http://en.wikipedia.org/wiki/Deming_regression).",), ('Deming regression is very nice, but as far as I understand it assumes that:Uncertainity rates are the same for each point, and I know there will be different in my setting. \n\nAlso there is no mention of how to calculate errors of A i B.',), ('Every correlation matrix will be symmetric because $\\\\mathrm{cov}\\\\left(x,y\\\\right)=\\\\mathrm{cov}\\\\left(y,x\\\\right)$. I encourage you to work out the math to see that this is indeed true. If you know the relationship between $x$ and $y$ (or whatever the variables of interest are) is not symmetric _a priori_, it might benefit you to look in to other methods of analysis.',), ('Interesting points were made on a related question, [Effect of switching response and explanatory variable in simple linear regression](http://stats.stackexchange.com/q/20553/930).',), ('How large a data set do you have?  Is your metric calculated on the same data used to fit the model, on a test sample, or using cross-validation?  If the relationship really is close to linear, linear regression is (quite likely) to outperform a random forest...  As for techniques that are better than linear regression, depending on the model and objective function, certainly there are better techniques, e.g., logistic regression for binary outcomes.',), ('It would be nice to know how did you arrive at this question. I cannot imagine real life situation where such type of information about regression model would be available. (For that matter no imaginary life situation is possible too)',), ("I am preparing for exam, so this is one from the last year's final.",), ("The full data set is roughly 1MM rows X 50 columns, w/ 40 numeric columns and 10 factor columns(each factor has 3 levels).  I have split that into a training set of 200k and a test set of 100k for easier/faster use.\n\nI'm trying to use regression as opposed to classification.",), ('You are getting the conclusions that different individuals often describe their speed differently, but that overall they often notice when their speed changes.',), ('Hi, could you clarify a few things.  Which is your response variable (on the vertical axis) - perceived velocity or actual velocity?  What is the scale of actual velocity (is it the same seven point scale as perceived, or is it a four point scale - one interpretation of what you mean by "I have 4 velocities"). What *do* you mean by "I have 4 velocities" - is this 4 possible velocities?  In the first regression, do you have 60 points or 15? And in the second are there 120 or 30?',), ('Hi, can you show us the scatterplot of your two velocity variables?',), ("Hello everybody thanks for your reply. On the vertical axis there is the perceived velocity. On the horizontal axis I have the velocities, on a 4 point scale. Just velocity 1, 2 , 3 and 4. It is a general case, I don't care about the differences between the velocities (Please note: this velocity example is just a generalization of the real problem that I am trying to solve...). In the first regression I just have 4 points (the average perceived velocity for the 4 velocities). In the second regression I have 120 points (15 participants * 2 trials * 4 estimate of the velocities).",), ('It would help if you edited your question to show the first couple of rows of your data, and put up a scatterplot.',), ('Where you might recall having ill conditioned matrices is if you put time {1,2,3,.....,} in as an explanatory variable. {1,0} dummies are OK though.',), ('Could you clarify "highly skewed along some of the dummy variables"?',), ("I have several discrete indicators  - some of these are for a covariates that I'm adjusting for. eg I have 30,000 observations of X=1, and 1,000 observations for X=0. This is true of predictor I'm making inferences on as well (available observational data is unbalanced).",), ("So, in R parlance you're doing something like `lm (S ~ T, data=X)` where X is the data for one day? And the relationship between S and T changes at irregular intervals because of repair work and also changes throughout each day? The changes that take place during the day are due to what? (That is, the temperature changes during the day, but what else affects it? Precipitation, who-knows-what, rail traffic?) If you know what causes the changes within each day, you may be able to add that to your model. Knowing some more details would help shape an answer.",), ('Intuitively, if $L$ truly is "luck" it shouldn\'t be able to predict anything!  As such, your hope is that $L(n-1)$ captures deviations (of team record relative to run production) that were *not* due to luck but perhaps to other factors, and that such factors may persist to time $n$.  Therefore your best hope is to see whether $L(n-1)$ is useful for predicting $L(n)$ (rather than $P(n)$).',), ('@mike - I don\'t understand what your obstacle/difficulty is. If you think you "could use L(n−1) in conjunction with P(n−1) to better predict P(n)" then I\'d have expected you to try that in a straightforward, 2-predictor regression (checking some diagnostics, probably needless to say). If you do, please share the results!',), ("Well, logistic regression is used when the dependent variable is binary so, to use that, you'd need the game-level data, not the winning percentage (which is binary data averaged over the season). Unless the diagnostics really don't check out, I don't see any problem using linear regression where winning percentage is the dependent variable. If they don't, use logistic regression on the game-level data. Either way, the results should be pretty similar.",), ('You can use logistic regression even if you have binomial data.  You just also need the sample sizes.',), ('@Dason, do you see any benefit to using logistic rather than linear regression, though?',), ("I guess I should mention, the number of games per season is readily available, I'm just not sure if there's any point in bringing it into the analysis.",), ("You're right, @Dason. You don't need the game level data, you could just use the number of wins as the dependent variable - the estimates and inference would be exactly the same.",), ('Several questions:  Please tell us N and the measurement level of your dependent variable(s)... Are your variables with >100,000 classes Xs or Ys?...For how many variables, or for how many levels of a given variable, are you trying to impute values?...Why do you need to impute values in order to make predictions?',), ("Are the response variables the same in both cases (eg, both weights)? Are the covariates the same (eg, both weights)? Is this two different groups (eg, college basketball & football players) or two different studies? The answers to these questions would help people answer. Also, I notice that you've asked 9 questions & not accepted a single answer yet. You may want to click the 'check' mark next to answers that you've found helpful in your previous questions; it's a nice thank you to people who've helped you, and encourages people to keep helping you.",), ('If you have 4 categorical variables with 4 levels each, you should have 3*4=12 coefficients for your independent variables (plus the intercept)...',), ("@andrea: I've decided to treat them as numerical variables.",), ("0.02 is barely significant (especially if you consider the fact that you have five tests in total) and 0.11 is not very high. A generous interpretation would be that with a little more power the overall F-test would also be significant (and perhaps the first coefficient as well). A more conservative interpretation is that you shouldn't have much confidence in any of these results (including the coefficient with a .02 p value). Either way, you shouldn't read too much in the difference between .02 and .11.",), ('Is this homework? If so, please use the Homework tag.',), ("No, it's not. I think it is true bcoz after all, sum of squares is a square of linear combination of Y's given constant X's. But is it? Simple proof like this would be appreciated!http://math.stackexchange.com/questions/47009/proof-of-fracn-1s2-sigma2-backsim-chi2-n-1",), ('The descriptions you give in both the question and your comment are a little bit muddled. Have you written out what your matrix $A$ must be for the sample variance? Does that help you see how to generalize?',), ('Corrected for D. I think the critical point is that diagonal element of D should be something like (1,1,1,...,1,0,0). Is there any way to prove it? or Is there anyway to show that $\\\\chi^2(n)=\\\\chi^2(n-2)+\\\\chi^2(1)+\\\\chi^2(1)$ where sse/$\\\\sigma^2 \\\\sim \\\\chi^2(n-2)$, $\\\\sum{e_i^2}/\\\\sigma^2 \\\\sim \\\\chi^2(n)$',), ('Would using AIC/BIC or the LogLik be sound statistically to make a choice between non-linear and linear models on the same dataset?',), ('... have you tried it?',), ('Yes I have tried it before posting here....just wanted to have a double check on what i am thinking is correct or not...',), ("Okay!  Sorry to ask, then, but you'd be surprised how often people ask questions that could be answered by just doing it.  Double-checking your thinking is always welcome!",), ("Sorry, but I'm having real trouble understanding this. Maybe some example numbers would help?",), ("@Charlie, your question doesn't contain a question. Please edit it.",), ('It might be a case for non-linearity. Can you show a plot of fitted values vs. residuals?',), ('Its best to avoid using the word skew like that because skewness has a precise and different definition in statistics: http://en.wikipedia.org/wiki/Skewness',), ('What are your goals in creating this model?  To minimize the error associated with your predictions?',), ('@MichaelBishop Thanks for your reply. I added a line in my original post: In my particular application, I don\'t have to generate a prediction every single time (it can simply returns "no clue"), but when I do, I would rather it be correct. Which is why I want to find a way to return "the regression is not confident here"',), ('"Confident" is an interesting word in this context, because linear regression provides information for constructing intervals of "confidence" around prediction: [prediction intervals](http://en.wikipedia.org/wiki/Prediction_interval#Regression_analysis). A wide interval reflects lack of "confidence" or trustworthiness in a prediction. You could use these widths to screen predictions for likely correctness. This could work well in conjunction with [goodness of fit tests](http://en.wikipedia.org/wiki/Lack-of-fit_sum_of_squares) (which would indicate whether to trust the prediction intervals!).',), ('Function interpolation (in the form of Chebyshev or Bernstein polynomials) may be of interest to you. From Weierstrass’ Theorem, these approaches provide bounds on the L-infinity error of any function over a closed interval.',), ('Could this be done not with polynomials, but with functions of the form a*sin(b*x), for instance?',), ("That's a rather vague question. Are you interested in specific applications, theoretical textbooks, or something else?",), ("I only mention the work on polynomial interpolation because I have some experience with it. I THINK the specific type of function you choose is not important as long as the basis functions are orthogonal. Sorry I can't be more help.",), ("Hi Chl, I put this question as I've very little idea on non-linear regression technique. I want to learn it from scratch. The reference I'm looking for is more theoretical, with some application example from any field.",), ('Can you explain further what you mean? It is very common to test whether the slope parameter for a variable is different from zero. I would call that "hypothesis testing". Are you unaware of that, or do you mean something different? What constitutes a scenario for your purposes?',), ('I am unaware of that. I was also unsure if regression-based analysis is used for any other sort of hypothesis testing (perhaps about the significance of one variable over another, etc).',), ('Depends on the sampling distribution (likelihood). A table showing the conjugate prior of various likelihood models is shown on wikipedia: http://en.wikipedia.org/wiki/Conjugate_prior',), ('This is really two different questions, because the one about piecewise linear regression does not involve sums of t distributions.  If you still are interested in the one about t distributions, please post it as a separate question.',), ("The terms you're using makes this somewhat unclear. We typically use the *generalized* linear model (eg, logistic regression) to classify, not linear regression. If you're referring to *discriminant function analysis*, that can certainly be used (it's more efficient than logistic reg if the response variables are multivariate normal), but DFA isn't usually called multiple linear regression in my experience. If you want to classify cases into more than 2 categories, you might check out [multinomial logistic regression](http://en.wikipedia.org/wiki/Multinomial_logit).",), ('Hello gung. I mean, if you want to apply generalized linear regression to an indicator matrix (0-1), you may get some issue with masking as explained in "The Elements of Statistical Learning" (Hastie et al), chapter 4. That is, many examples will be classified to the wrong class. And I cannot understand the theory behind this issue.',), ('@gung\'s point is still a good one that your term "multiple linear regression" is a confusing one in this context.  Could you re-write your question with terminology closer to that used normally?',), ("Let's say I have k classes, each coded with its index ({1,2,..,k}). I would like to test how linear regression performs on my dataset. I read different approaches to this issue. One uses an indicator matrix, for instance. I wonder whether one need to use a matrix at all. Can't one just apply the regression to a scalar and then classify (somehow). And, of course, the issues coming with this approach that some classes may not be classified (almost) at all.",), ('I would like to clarify that I want to use least-squares regression to perform the classification',), ('Why do you want to do this? (For almost all statistical purposes, the choice of reference level makes no difference, nor does the number of coefficients that are positive)',), ("Why isn't $\\\\Theta_{opt}=(0,0,\\\\ldots,0)$ always a solution?",), ('@whuber: $\\\\Theta_{opt}$ is a set of vectors, not a single vector.  If a vector belongs to this set, it must provide a least-squares optimal solution (i.e. minimizes the squared residual sum).  An all-zero vector would fail to minimize this sum.',), ("@guest: statistics isn't the only purpose of regression -- it is often used for modelling systems from data.  In these cases, one might have prior knowledge about the factors being used in the model, and desire to shape the model's form in such a way to improve human-interpretability of the model.  For example, if I have a factor with 5 levels, and I know that only one of those levels causes an increase in the regressand variable, I would prefer a model form where that 1 level has a positive coefficient, instead of the 4 other levels having negative coefficients.",), ('Mark, the reason for my comment was that $\\\\Theta_{opt}$ does not have a clear definition.  What do you mean by "optimal" vectors?  Obviously $(0,0,\\\\ldots,0)$ will be of minimal length.  What criterion excludes it from the set $\\\\Theta_{opt}$, then?  You haven\'t given us any information in that regard.  Your response sounds self-contradictory: you seek some parameter vector with certain characteristics (all elements are positive), yet you seem to focus on the LS solution, which apparently does not have those characteristics.  What, exactly, are your objective function and constraints then?',), ("@whuber, I'll try to clarify: assume that we have a given dataset and model formula (i.e. what factors / variables / interactions should be in the model).  If we perform unconstrained OLS linear regression using this data and formula, we will get a non-unique vector of coefficients $\\\\theta_0$ which minimize the RSS.  $\\\\theta_0$ is one of possibly several vectors in $\\\\Theta_{opt}$.  All vectors in $\\\\Theta_{opt}$ share the property that they minimize the RSS (all with the same minimal value). Does that make sense?  There may be a $\\\\theta_1 \\\\in \\\\Theta_{opt}$ with fewer negatives than $\\\\theta_0$.",), ('Also, note that specifying the model formula is not the same as specifying its specific terms.  I guess this is a bit confusing with my notation, since $\\\\Theta_{opt}$ may lead one to believe that all of the vectors it contains have elements corresponding to the same terms.  The terms may be freely changed (under the constraint of maintaining OLS optimality), and this is what affects the length of vectors in $\\\\Theta_{opt}$.  So, technically, one could think of $\\\\Theta_{opt}$ not as a set of vectors, but as a set of (terms,coeffs) pairs.',), ("On second thought, if you have terms for all levels of the factors specified in the model formula, there really isn't a difference in the terms of one solution to the next -- so, I guess thinking of it as a fixed-length vector (which may contain zeroes) is fine after all.",), ('You pick values for $\\\\theta_1$ and $\\\\theta_2$. Then minimise with respect to $\\\\theta_3$. You get the value $\\\\theta_3^{(1)}$. Having it you now use OLS to estimate $\\\\theta_1^{(1)}$ and $\\\\theta_2^{(1)}$. You got your first iteration. Then iterate until convergence.',), ("Are you referring to identifiability problems, Mark?  Usually, one sets up the model so that there is a unique parameter vector minimizing the RSS. It sounds like you are contemplating a more general framework in which the normal equations are of less than full rank and so you seek a positive vector among the linear subspace of solutions.  (If that is so, we're talking about a linear programming problem, which gives you access to efficient solution methods.)",), ('Ok, thanks, but what do you gain from iterating like that compared to performing a full-blown NLS on the complete model? You essentially perform lots of OLS regressions iteratively if you run NLS using a Gauss-Newton method anyway, right?',), ('You do not perform lots of OLS regressions if you run NLS. The point of the procedure is that it practice it might work better than straight NLS. The NLS is guaranteed to work if your starting values are close to the optimal ones. So the outlined procedure can be thought of a way to get better starting values. Theory is nice, but in practice sometimes is very hard to get convergence. Any trick then helps.',), ('Improved convergence is of course desirable. Thanks for that answer! I may be wrong, but I think you may, in principle, use OLS estimates for the so called Gauss-Newton regressions which are used for computing the step size at each iteration in a Gauss-Newton method.',), ("You may get a good answer here, but I'm going to flag this for migration to stats.SE, as the answer to this question essentially boils down to understanding how linear regression works.",), ("Yeah that's fair enough. Would it be better if I deleted it and moved it myself? Or is that unnecessary?",), ("You shouldn't need to do anything. I flagged it, but it may take an hour or two before a mod gets to it, it being a Sunday and all.",), ('I won\'t provide an answer here, because the question will be moved. But you can try a few things to understand what\'s going on: 1. run lm( Y ~x1 + x2 - 1). the "-1" will remove the intercept. 2. use relevel to change the reference category of x2.',), ('In linear models, the explanatory variables are usually assumed to be non-random, so talking about their distribution (or, indeed, correlations) is meaningless. Are you sure that multiple linear regression is a good choice here? That being said, the residuals can be heavily correlated with $Y$ (or $X$) if, for instance, you have non-independent errors or non-linear relationships.',), ('@whuber: based on what you wrote, it seams likely that it is a linear programming problem -- do you have suggestions on resources (e.g. links, articles) which address this particular kind of problem with linear programming?  (also doing this kind of thing in R?)',), ('Related, [What is the expected correlation between residual and the criterion variable?](http://stats.stackexchange.com/q/5235/1036)',), ('Dear Andy, I have thought that the residual should be randomly distributed along the Y. If it correlated with Y, there must be some problems.',), ('Can you provide with several other methods for my data?In my data, Y and most Xs are numeric variables and the other Xs are binary variables. Because there are 25 Xs, it is very difficult to formulate the suitable relationship between Y and Xs. Do you have any suggestions? Thanks in advance!',), ('If I properly understand, a full column of X missing means that you did not observe an explanatory variable at all. I do not believe it is possible to estimate it unless there is a known relationship with the observed covariates. This actually happens all the time because there are covariates that you do not observe (for a number of possible reasons).',), ('@Procrastinator I think we can do regression X on Y for the missing data estimation.',), ('Are you interested on a [calibration model](http://en.wikipedia.org/wiki/Calibration_%28statistics%29)? Otherwise it makes no sense to estimate $X$ and then use it on a regression model for $Y$.',), ('@Procrastinator As I said on my update above, I hope to have training data that has more "weights" for application-specific input space.',), ("@whuber Maybe I'm rusty on this, but isn't there a single standard error of estimate that applies to all data points, and thus a single width for all points' prediction intervals at a given confidence level?  CodeNoob seems to want or need intervals of different widths for different points, which I don't think is possible.",), ("@rolando Yes, there is a single SE of estimation, but it doesn't tell the whole story.  When predicting the value associated with $(x_1,x_2,x_3)$, you also have to account for the (correlated) uncertainties in the parameter estimates $(\\\\hat{a}_1, \\\\hat{a}_2, \\\\hat{a}_3)$.  This causes the prediction intervals to spread hyperbolically as the point $(x_1,x_2,x_3)$ moves further from the mean value used in estimation; the widths can even become infinite.",), ("you're absolutely right - the value of $\\\\beta$ alone does not indicate the strength of the relationship unless, as @gung mentions below, your variables are standardized.",), ('You have some outlying residuals.  What do your regression diagnostics say about their leverage and how many of them there are?  What do the diagnostic plots suggest about goodness of fit?',), ("1) Since you're not estimating the absolute values in your model, why are you using absolute values in the check?  2) If the mean of $Y$ is close to 0 (relative to the std. dev. of $Y$), all this comparison will tell you is that the predictions have less dispersion than $Y$, which is a consequence of the estimation procedure and not worth worrying about.",), ("@jbowman I'm using absolute values because while I was reading through the predictions one at the time I noticed that the relative outcomes were all lower by substantial margin. I added the average of the absolute difference between each prediction and relative outcome to the question.",), ("@whuber I added the diagnostics. If I'm not mistaken their impact shouldn't be very large given the number of data points.",), ("The most revealing part is that the fitted values have a mean near 0, showing that @jbowman's surmise is correct.",), ('It depends, what did you read specifically about $R^2$ that makes it a non-ideal indication of fit in your case?',), ("your first (auto.arima) model doesn't have an intercept... try using a-mean(a) and b-mean(b) to eliminate the need for one. You'll get the same coefficient estimate that lm gives if you do.",), ("Yes, if I remove the mean, the `auto.arima` model generates the same coefficients that `lm`. But why isn't it estimating an intercept when I use the raw data? Shouldn't the procedures be the same inside `auto.arima`?",), ("If you just use `arima`, you can specify `include.mean=TRUE` in which case you'll get the `lm` coefficient estimate; I too find it a little odd that `auto.arima` doesn't allow this option (I just tested it) but I expect there's a good reason for it.",), ('I believe it\'s because the purpose of `auto.arima` is automatically estimates all coefficients, including the choice of include or not the intercept.\n\nSo, it seems the results are different because the intercept is not being calculated in the "regression" part of the model, but in the ARIMA. And in this specific case, they disagree: (a) `lm` chooses to include an intercept and (b) ARIMA chooses not to include. Is it correct? EDIT: Actually it doesn\'t seem to be correct, because if the intercept is on the $N_t$ part, it\'s the same thing if it\'s on the regression when we substitute.',), ('What makes this a non linear model?',), ('The line you see is not fitted. The data gives you a similar graph like michaellis menten, but in that model there is no time involved, here I have response ~ time not velocity ~ [substrate concentration]',), ('With a non linear regression michaellis menten model, you could predict variables like Km and Vmax, so I like to find a model to predict similar variables.',), ('You could validate by simulation when the predictors truly have no effect or by permuting the responses to give an idea what prediction accuracy to expect purely by chance.',), ('@Macro thanks macro i think your pointing towardss cross validation. \nI ran cv.lm with 10 fold cross validation on the datset with 63% corrolation and i found the Sum of squares = 2.91e+08 is Mean square = 1063519 and the data set with 40% corolation has Sum of squares = 23.1    Mean square = 0.08. A low mean square means that it has predictive ability so my main questions is how to i find the the probability of it concurring randomly?',), ('If you simulate data where the predictors truly have no effect, this will give you a reference. If you permute the response variables, you will also have a situation where the predictors have no effect, so that would be another way of getting a reference.',), ('A large part of your problem is that you have a highly nonlinear function (in the parameter in question) and not much data.  You might want to try the experiment of using $n=20, 40, 80, 160, 320$ and plotting the posteriors for a given weakly informative prior, to see how fast they become "reasonable."  Plots of the series of generated estimates and the `acf` function will help with determining burn in times and thinning rates.',), ("@jbowman Thanks for the sim suggestion, I'll give it a try on the weekend.",), ('Of possible interest: [Link](http://www.ats.ucla.edu/stat/r/dae/rreg.htm).',), ('You may want to look up the different kinds of sums of squares. Specifically, I believe linear regression returns type III sum of squares, while anova returns a different kind.',), ('If you save the results of `lm` and `aov` you can check they produce identical fits; e.g., compare their residuals with the `residuals` function or examine their coefficients (the `$coefficients` slot in both cases).',), ('[I answered](http://stats.stackexchange.com/questions/28688/how-to-interpret-model-diagnostics-when-doing-linear-regression-in-r/28697#28697) many of these questions last week. You will learn more if you search for similar questions from other sources.',), ('An even better discussion here: http://stats.stackexchange.com/questions/5135/interpretation-of-rs-lm-output',), ("@Procrastinator Your link does show someone who proves examples of robust estimation in R and specifically M-estimation.  I think that qualifies as an answer to the question.  Why don't you want to post it as an answer?",), ('@MichaelChernick Thanks for your comment. I did not post it as an answer because I do not really understand what $M$-estimation means. I did a quick search by mere curiosity and found that link.',), ('These are good questions, but as the comments and attempted replies indicate, they cover too much ground.  Please focus on one at a time, perform some research, and be specific about what you\'re looking for. "I didn\'t get it" doesn\'t give us enough clues to provide the information you need.',), ('It looks like that link gives examples of _univariate_ regression models using robust ($M$) estimation, but I think the question is about $M$-estimation for a multivariate dependent variable.',), ('In the linear regression model the dependent variable $y$ is considered continuous, whereas in logistic regression it is categorical, i.e., discrete. In application, the former is used in regression settings while the latter is used for binary classification or multi-class classification (where it is called multinomial logistic regression).',), ("That sounds like an answer to me, @Pardis :) I'd +1.",), ('And moreover, if my intent was to study the correlation between x1 and x2 and its impact on y... how shall I construct my experiment?',), ('Well, why exactly are you using a mixed model? If you can explain that to your audience, this could go a long way in terms of providing intuition for the audience.',), ("re: your second question - the second model drops the fixed intercept while the first does not; it effectively constrains the mean of the observations to be 0. I don't recommend the second model unless you have substantive theory which supports it.",), ('Thanks Macro. But I don\'t understand why it " it effectively constrains the mean of the observations to be 0. "? I was thinking that the (1 | county) has intercepts any way and the fixed-effect intercept can be absorbed those intercepts anyway',), ('the random effects have mean 0 so they cannot absorb the missing intercept the same way fixed effects can in a usual regression model.',), ('What the random effect to in a mixed model is increase uncertainty in the estimates because these effects are to be viewed as a sample from a large population as a center is in a muLticenter clinical trial.  You may have chosen 15 centers out of thousands that could have been selected. This unseen variability that would show up of you repeated the trial with different centers need to be taken into account.',), ('In the Bates\', yes, 0-mean was emphasized in the model definition; but in Gelman and Hill 2007, 0-mean is not in the model definition. Are you saying that "lmer" and "nlme" force 0-mean, since they were designed by Bates? Thank you!',), ("The random effects are required to have mean $0$, for identifiability. Yes, `lmer` and `nlme` both certainly impose that restriction in the model definition. You're welcome.",), ('interesting question.  Perhaps some sort of EM algorithm would work...',), ("I don't understand, can't you use cross-validation for estimating the optimal ridge parameter?",), ("@Pardis: No loss function is given in the question so we don't know what *optimal* means. Can you see the trouble we run into if the loss function is the MSE?",), ("Actually, cannot we still use cross-validation? Let's say we are looking for $\\\\lambda$ that minimizes the error like this: $||(X_{cv}^t X_{cv})\\\\beta(\\\\lambda) - X_{cv}^t Y_{cv}||$, where $\\\\beta(\\\\lambda) = (X_{train}^t X_{train} + \\\\lambda I)^{-1} X_{train}^t Y_{train}$. That is not exactly correct, but can work.",), ("@Pardis: If you don't know $Y$, how do you find $\\\\|Y - \\\\hat Y\\\\|_2^2$? :)",), ('@JohnSmith: You are alluding to the point I was driving at. There is no indication of how to measure "optimality". What you are effectively doing is introducing a different metric (distance function) to measure "quality" of prediction or fit. We need more details from the OP in order to get very far, I suspect.',), ("@cardinal I see, but isn't the loss function for OLS the square loss? We see that the solution can be expressed in terms of $X^tY$, just like the solution of ridge regression.",), ('@Pardis: Finding the estimates is not the problem, as you note. :) However, if you decide to do crossvalidation, how are you going to estimate the out-of-sample MSE, i.e., on the left-out fold for each iteration? :)',), ('@cardinal Very good point, thanks :)',), ('@cardinal: The loss function is the MSE, as in the OLS.',), ("@JohnSmith: cv isn't possible, we don't know individual $Y_i$",), ('I am not sure that it is possible to come up with such an interpretation. Simply because what you provided are images in the original space of features and responses. And penalized regression involves the space of coefficients, which is very different.',), ('I will update my question, @JohnSmith. Thanks!',), ('How can nominal variables be correlated? Are you just using "collinear" as a catch-all for any kind of association here?',), ("Is the purpose to draw information from the parameters or to predict the outcome of future games? If you are predicting the you don't need to worry about multicollinearity(you should of course still not include the reference dummy variable.) On another note you should use multinomial logistic regression and not linear regression since you are looking at a categorical output variable. To help you further please provide an example of your data.",), ('@Macro: I added simplified format of how I am formatting the data. My understanding of "collinear" is that there are independent variables that correlate to each other.',), ('@pgericson: The goal is to identify who our "best" players are. Unlike other sports where there is a center of the action, it\'s sometimes hard to identify who is benefiting the team so we are hoping that regression will help us identify undervalued players.',), ("This seems like a homework problem. If it is, please add the `homework` tag so that we can treat it accordingly. Also add some comments on how you've tried to solve the problem and where you got stuck.",), ("Var(Xi)=μ=E(Xi)>0. If this were fixed then this would be an error in variables model with variance ratio σ$^2$ $_ro$/μ. It isn't because μ changes with the Xi. I have seen models with non constant variance in Y and models with errors in variables but not this type of model which has errors in variables with non-constant variance.  When the variance in Y is not constant sometimes it is possible to model the variance as a function of the value of the covariate x.  Maybe something like that could be done for the error in X.",), ('There\'s also no need to email users of Stack Exchange sites asking them for an answer. After I got your email, the first thing I did was search Google for "bayesian test of linear regression hypothesis" and this question was in the top 5.',), ('If this is homework, please tag it as such.',), ('Your answer is on the wikipedia page about simple linear regression:  http://en.wikipedia.org/wiki/Simple_linear_regression#Confidence_intervals',), ("I referred to the wiki article. But I didn't get some of the stuff. I have reposted in the question",), ('possible duplicate of [Should confidence intervals for linear regression coefficients be based on the normal or $t$ distribution?](http://stats.stackexchange.com/questions/29981/should-confidence-intervals-for-linear-regression-coefficients-be-based-on-the-n)',), ('It would be preferable to incorporate this into your previous question. Have you searched the site for duplicate and related questions?',), ("Please update your previous question. I'm closing this one as a duplicate.",), ('I find significant autocorrelation in the residuals from calculating the rolling return in such a method',), ('@Luna, why is that wrong? It appears you used `x3` to generate the `y`s, so it should be included in the model and the $p$-value agrees with that conclusion.',), ("@Seth - you are right. I was just giving a toy example of using anova generally in model comparison. So it's not linked to my original question.",), ('@Macro - you are right. Now I see the point. Thank you!',), ("Can you say more about what you'd like to know? (This question is fairly simple / sparse.) You may also be interested in [this recent question](http://stats.stackexchange.com/questions/31690/how-to-test-the-statistical-significance-for-categorical-variable-in-linear-regr).",), ('good catch, @gung. I thought this sounded familiar. I voted to close as a duplicate.',), ('multicollinearity is what you get when two or more predictors are highly correlated with each other, but it appears you are generating variables which are not correlated at all.',), ('But if we put a design matrix for categorical variable and an intercept together, the combined design matrix will be of reduced rank, i.e. rank-difficient. Am I right?',), ("R has outsmarted you :), it treats the two regressions differently.  In the first case, it takes the first factor and re-frames it as a smaller number of factors, each representing the difference between a level and the first level (so there are 2 coefficients to estimate in your case instead of 3.)  It then does the same for the second.  Hence you get an intercept and 3 total coefficients.  In the second case, it knows there isn't an intercept, so it doesn't re-frame the first factor, instead leaving it as is.  It still re-frames the second factor, though, to avoid the multicollinearity.",), ("@jbowman, that's a great catch, & I think, exactly the answer to this question. Why don't you turn the comment into an answer?",), ("@gung - thanks for the suggestion, I couldn't think of a good reason why not, so I did :)",), ("What do you mean under `nonlinear regression ... with the help of genetic programming`? Could you provide more details? Why don't you simply use polynimial fitting?",), ("genetic programming is able to find a mathematical model of data by the evolutionary computing method of iterative test-generate-select. given a set of operators, and training data (input-output), the program constructs randomly a set of expression trees, refined through many iterations of recombination , mutation, etc, (which in effect is exchange of subtrees), fit evaluation, and fitness based selection to arrive at a model of data. i hope that's somewhat descriptive. i tried regression trees, polynomials, and the result on training and test data is not as good as genetic programming.",), ('Have you tried some kind of regularization scheme (e.g. ridge regression)?',), ('In general, you should have a good reason _not_ to include the intercept, not the other way around. See the thread above, which has lots of useful info.',), ('Luna, I appreciate the good questions you\'ve been asking here & the productive discussions that have been brought about because of your efforts. As you are becoming more active here, it would be worthwhile to become more familiar with CV\'s style of asking questions. We tend to discourage repeating the title, & having conversational elements such as "Hi all" & "Thank you!". The idea is that these questions ultimately become a searchable public record & that stuff ends up getting in the way.',), ('Some very rough rules of thumb: you should investigate collinearity *before* you do any fitting. If you find it is present, you should either (a) use a method that handles collinearity, (b) remove collinear features, or (c) transform your features (eg using PCA). Once you have fitted a model, you can look for heteroscedasticity in the residuals. In general, if you are making a predictive model you should not remove outliers. Instead, use a method which is robust to the presence of outliers.',), ("How does one best investigate collinearity? Looking at the off-diagonal elements of the predictors' correlation matrix?",), ('Regression involves fitting a linear model to real data which includes some random variation. Are you really interested in fitting regression lines or just ploting lines like your graphs seem to indicate? There is no point to fitting regression to data without a noise component.',), ('i am looking to find parameters for a theoretical model that best fit real world data. If you\'d like i can edit the images in the question; adding just enough noise to make it look *"more random"*, without changing my question at all.',), ('thanks for the clarification.',), ('A good way to check things like this is to simulate the data using known coefficients and very small random error terms, and then check that the output is consistent with the simulation.',), ('The best way to investigate collinearity is condition indices and proportion of variance explained by them. High correlation is neither a necessary nor a sufficient condition for collinearity.',), ('It would be the mean when `wool=="A"` AND `tension=="L"` if you also included the interaction between `wool` and `tension` in the model. Without that, I think you\'re just left with an estimate of the mean, assuming the two factors have independent effects.',), ('Assuming the factors have independent effects, do you know how that estimate is produced?',), ("The model you've written is $Y_i = \\\\beta_0 + \\\\beta_1 B + \\\\beta_2 M + \\\\beta_3 H + \\\\varepsilon_i$ where $B,M,H$ denote dummy variables for the levels of the variables in question. The $\\\\beta$ estimates are chosen to minimize the residual sums of squares - is that what you're asking?",), ('Question 1 is related to [Look and you shall find (a correlation)](http://stats.stackexchange.com/q/5750/930) and Question 3 to [Is adjusting p-values in a multiple regression for multiple comparisons a good idea?](http://stats.stackexchange.com/q/3200/930). More generally results from [this query](http://stats.stackexchange.com/search?q=%2Bcorrelation+%2B%5Bmultiple-comparisons%5D) may be of interest.',), ("Ah, sorry, didn't fully answer your question.  $cov(\\\\hat{\\\\beta}_1,\\\\hat{\\\\beta}_2)$ would be the entry in the matrix with column name, in your case, `as.factor(X1)1` row name `as.factor(X2)1`, assuming what you wished to find the covariance for were the coefficients with values -0.12795 and 0.05666, and similarly for the other covariances.",), ('Thanks @jbowman, I think now I understand! I must use 0.00340945299? Ok?I just added the covariance matrix to the question.',), ('Yes, you have got it!',), ('**Thanks a lot!!!!!!!**',), ("@jbowman, why don't you convert your comments into an answer (since that's what they are)? Then the OP can accept it, & I can upvote it.",), ("It also happens to be wrong... I didn't realize that `summary(reg)$cov` gave an unscaled covariance matrix.  I'll post the correct answer in a minute or two.",), ('Deleting my erroneous comment to avoid causing confusion... and posting this note to clarify why the comment thread may be incomprehensible afterwards.',), ('I also edited the question to avoid confusion. Thanks @jbowman.',), ("This is the same situation addressed at http://stats.stackexchange.com/questions/33193. Both questions derive from the same misconception, so let's be very clear: **there is no normality assumption about the distribution of _any_ of the variables in regression modeling.** The present question also covers the same ground as another (which I haven't looked up) asking for a transformation to convert a categorical variable to approximate normality: the answer is that no such transformation can possibly exist (except perhaps when there is a large number of categories).",), ('@whuber is right, +1. Also note that in R, `lowess()` & `loess()` are essentially the same thing. [?lowess](http://stat.ethz.ch/R-manual/R-patched/library/stats/html/lowess.html) is the older & [?loess](http://stat.ethz.ch/R-manual/R-patched/library/stats/html/loess.html) is the augmented version (although, for simple things, I find `lowess()` easier to use).',), ('I think you are asking about prediction intervals. Note, however, that you use "$x_i$", instead of "$y_i$". Is this a typo? We *don\'t* predict $x$s.',), ('@gung: I use $x$ to denote for example time, and $y$ the value of some variable at that time, so $y=f(x)$ means that I made an observation $y$ at time $x$. I want to know how far the fitting function predictions can be from the real values of y. Does that make sense? Function $real(x_i)$ returns the "correct" value of $y$ at $x_i$, and my data points consist of ${(x_i, real(x_i))}$.',), ("If you have a single categorical variable you are effectively doing a one-way anova and the concept of a linear relationship between your regressors and your response isn't well defined since you don't have any way to measure the distance between your categories or for that matter even ordination.",), ('That seems perfectly reasonable. The parts I\'m focusing on are, eg, "$e_i=real(x_i)-(mx_i+b)$", usually we think of the errors / residuals in a reg model as"$e_i=y_i-(mx_i+b)$". The SD of the residuals *does* play a role in calculating prediction intervals. It\'s that "$x_i$" that\'s weird to me; I\'m wondering if it\'s a typo, or you\'re asking about something I don\'t recognize.',), ("I think I see; I missed your edit. This suggests that the system is perfectly deterministic & if you had access to the *real* underlying function, you could always predict $y_i$ perfectly w/o error. That's not the way we typically think about reg models.",), ("@gung: Does this question make sense if I'm observing a nondeterministic algorithm, instead of a function? I'm not shure how to reformulate my question, so that it makes sense for statisticians :(. I want to estimate how good the linear model approximates the observed thing, assuming that this thing is linear, but deviates from a line with a random error.",), ('I think you may have a fundamental misunderstanding about what prediction intervals and confidence intervals are.  I feel that way based on the initial phrasing of the problem.  Gung is doing a good job trying to explain things to you.  If you could be more precise about your question we may be able to give you a clearer answer and correct any confusion if it exists.',), ('The [Gauss-Markov Theorem](http://en.wikipedia.org/wiki/Gauss%E2%80%93Markov_theorem) is one major motivation to prefer least-squares regression over some other arbitrary linear estimator :)',), ('Are you referring to a question of *experimental design,* where (for example) you could choose $2n$ $x$ values and you might elect to take $n$ at one value of $x$ and another $n$ at a second value of $x$, as opposed to spreading all $2n$ $x$ values out? Or are you talking about regression with $(x,y)$ data?  And what do you mean that "$n$ is proportional to $\\\\sigma^2$? How is that possible when either you control $n$ or the data are already given to you?',), ('Hi @whuber you got the first one right. 2n x values, with n at an $x_1$ and n at an $x_2$. And when I say "n is proportional to $\\\\sigma^2$ I mean that in general, if you have an experiment with high standard deviation in the results, you are going to have to set $n$ to be very high to get a good estimate. In my tests I could change $\\\\sigma$ because I was running a simulation.',), ('And thank you @Macro I will definitely check that out.',), ("bmx, It looks to me like you have a clear idea of your question and a good awareness of some of the issues. You might be interested to review three closely related threads. http://stats.stackexchange.com/questions/17773 describes prediction intervals in nontechnical terms; http://stats.stackexchange.com/questions/26702 gives a more mathematical description; and in http://stats.stackexchange.com/questions/9131, Rob Hyndman provides the formula you seek. If these don't fully answer your question, at least they may give you a standard notation and vocabulary for clarifying it.",), ('Mike OK, but exactly what do you mean by the "maximum error"?  Error of what?  Residuals?  Coefficients?  Predictions for $y$ at given values of $x$?  Where you refer to a "good answer," what exactly constitutes an *answer*?',), ('The maximum "likely" error of the difference in means of the samples at the points $x_1$ and $x_2$. If one assumes that the means are not going to appear outside the confidence intervals I talked about (this is what I meant by restricting the "realm of possibilities"), than the biggest possible difference between the difference between the true means and the difference between the sample means is from the bottom of one interval to the top of the other, so 2 times the ME stated. A good answer is one that will precisely estimate the value of the true difference.',), ("I admit to some confusion. You seem to be interpreting confidence intervals in an unconventional way; and I do not see anymore what your question has to do with regression. Aren't you now just asking how to estimate a difference of means in two independent samples?",), ("@whuber, It is definitely not a conventional interpretation, should I call it something different? I'm just assuming I wont have to worry about improbable results happening. To your other question, I am asking why regression would be better.",), ('I think you are trying to work on a problem that is too difficult. If you have a single covariate and a single response finding a breakpoint in a line may not be too difficult but with 196 different variables allowed to change how could you even picture the shape of the response in such a high-dimensional space much less identify breakpoints!',), ("Can you provide the link to the discussion? Also we have an [anova tag](http://stats.stackexchange.com/questions/tagged/anova), but I don't have sufficient rep to suggest/create a synonym with your created analysis-of-variance tag, could you create the synonym?",), ('Also there is a related discussion in a prior question on the site (although not a duplicate), [Why is ANOVA taught / used as if it is a different research methodology compared to linear regression?](http://stats.stackexchange.com/q/555/1036)',), ("It seems like you're talking about two different but related things. Since the other commenter was referencing `R` functions, I think you're talking about `aov()` (http://stat.ethz.ch/R-manual/R-patched/library/stats/html/aov.html) and he's talking about `anova()` (http://stat.ethz.ch/R-manual/R-patched/library/stats/html/anova.lm.html)...",), ('@smillig The discussion was not about computer routines.  We just discussed SAS in my case and R in his as support to our positions.  It is common to refer to the variance decomposition table in the general linear model as the ANOVA table. But ANOVA is usually differentiated from regression in the way I stated.   Thanks for linking the other post.',), ("I can't understand why nobody uses the purpose argument, i.e. that linear regression is a *modelling* method and ANOVA is a *hypothesis testing* framework. I know that they can be extended in a way they overlap, but the canonical formulations are perfectly separated.",), ("@MichaelChernick I know the discussion wasn't about computer routines, but since the other commentator was referring to `R` functions, I thought the documentation for what each of those things do might shed light on where each of you is coming from (which, as I think mbq is alluding to in his comment, is probably behind the differences between the answers on the original post). I think your answer is correct and definitely didn't deserve to be downvoted.",), ('@smillig Thank you.  My references were to the SAS routines which I think show the kind of separation between ANOVA and linear regression that I suggested.',), ("If it's any consolation, Michael, others have had experiences like yours on the math site. There seems to be a tendency for downvotes to replace constructive conversation about differences of opinion or interpretation (perhaps because pure mathematics does not seem to allow for any such differences, since from a certain immature perspective (which is present even in some professionals), all answers are either *right* or *wrong*: whence, I would contend, questions like the one you reference don't belong on the math site at all).",), ("@whuber: I agree it belongs here. If you and Michael haven't already done so, please consider casting votes to migrate the math.SE question. Your two votes would finish it off.",), ('@whuber One point to keep in mind is that one of the downvotes came from Michael Hardy who I understand is a professional statistician.',), ('@cardinal : How does one vote to migrate?  There a button for voting to close, but how does one express a desire to migrate?',), ("@whuber : I am one of those who have expressed who have complained of downvotes without any comments saying what's wrong.  I've rarely downvoted things, but I think I've always commented when I did so.",), ('@MichaelHardy: Vote to close as off-topic and select stats.SE as the destination. If the five close-voters do this, it should be automatically migrated here. :)',), ('@cardinal : Done.',), ('In ANOVA, we are trying to find how much of the variance is accounted for our manipulation of the independent variables. In multiple regression, we do not directly manipulate the independent variables, but instead just measure the naturally levels of variables and see if this helps us to predict the dependent variable. However, ANOVA and multiple regression are fundamentally the same since both of them try to explain the variance in the level of one variable on the basis of the level of one or more other variables. Ref: "SPSS for Psychologists: A Guide to Data Analysis Using Spss for Windows"',), ('My friend\'s wife constantly nags him to do chores: "Honey, it will only take ten minutes!" From experience he has discovered that a "ten minute" chore will take an hour and an "hour" chore will take up most of the day. So now when she asks him to do something that will only take $x$ minutes, he figures it\'ll really be $x/10$ hours: that\'s the bias correction. They\'re both happier now that they\'re in agreement :-).  (In many fields, this is called *calibration*.)',), ('@whuber thanks for the analogy. I wanted to know how bias is calculated. I mean by linear regression as you said I can find a model that can give me the calibrated value as you mentioned. But I wanted to know how can I know about the bias from the scatter plot. Actually I wanted to know how it actually looks like given some data',), ('My comment included actual data :-). Seriously, the procedure is no different than the intuitively obvious approach taken by my friend: he found a way to predict real experience ($y$) from estimated values ($x$) and used that relationship to adjust future estimates. Are you asking how to apply OLS to such situations? If so, then what have you learned about OLS so far and where are you stuck?',), ('Because this question makes no sense without the background provided by the link--and not everyone will have the time or interest to sit through it--please explain the context and the meaning of the notation.',), ("Thanks @whuber updated to add context.  I'm a newbie to stats, so I'm not sure if this you enough information.",), ('Also of interest: [Significance-of-coefficients-in-linear-regression:-significant-t-test-vs-non-sigificant-F-statistic](http://stats.stackexchange.com/questions/24720/).',), ('This is a question about selection. It is a common topic here & elsewhere. You should *not* only include significant variables, to understand why, you may want to read this: [algorithms-for-automatic-model-selection](http://stats.stackexchange.com/questions/20836/20856#20856). You should also search / read around CV under the tags: [tag:feature-selection], [tag:model-selection], & [tag:stepwise-regression], for starters. If there is still something that you want to know after having read through that, edit your Q here to clarify, otherwise this Q should be closed as a FAQ.',), ('"Best" equation for what purpose? Prediction, estimation, understanding, theory-building?',), ('@whuber prediction :-)',), ('OK, then in your researches pay special attention to mentions of holding out data to use for verifying the model as well as more sophisticated versions known as "cross validation." Using these approaches will make you somewhat immune from the dangers of many model selection procedures such as stepwise regression.',), ('What $\\\\chi^2$ test you have in mind? $(\\\\theta-\\\\theta_{ML})(Var(\\\\theta))^{-1}(\\\\theta-\\\\theta_{ML})$?',), ("Yes. One problem is that I don't know $Var(\\\\theta)$ so I would have to estimate it from the data.",), ('"Mixed Effects Models and Extensions in Ecology with R" by Zuur et al. is a nice introduction to linear mixed effects models, which focusses less on theory and more on application of the methodology.',), ('Dear Roland, I believe that that book is useful, but I am rather searching for something on line...do you have any webpage to suggest?',), ('As I said in your earlier post, lm() has a plot associated with it. So, if your model is M1 you can use plot(M1).',), ('dear @PeterFlom yes, but you also told me to avoid using lm for repeated measures design. So, my question is if I have to use lm for analyzing my data or another function. Any suggestion?',), ('Like I said, look into multi-level models. In R, you can look at the `nlme`package. Also, search this site for the topic, there is a *lot* written about it here.',), ("The problem with your current setup is that your repeated measures are *nested within* your subjects, if your model doesn't account for that fact, your p-values will be off. You do need to use some approach that addresses this issue.",), ('That depends on the problem under consideration. A polynomial regression always yields a better fit than an affine regression but it could make no sense from the interpretation perspective.',), ('By presupposing a polynomial solution, this question implicitly rules out procedures that might be simpler or more effective.  I would like to suggest that you begin your research by reviewing any appealing threads related to [model selection](http://stats.stackexchange.com/questions/tagged/model-selection+-time-series?sort=votes). You may find many of these informative and stimulating. Then consider returning with a more specific question motivated by that information.',), ('Did you checked for linearity!? check for other models for non linear data. or you can always plot the data and try to build a polynomial model that as the same look...then just shape it (fit it)...',), ("If I understand correctly, the difference in the dependent variable $y$ may depend: 1) on another variable $x_6$ you didn't detect (it may be the time); 2) on random errors. Which may be your case?",), ('What is it that you want to test?',), ('If you do see spatial autocorrelation in the residuals you can include the observations at nearby locations ("spatial lags") as predictors in the model as Sameer suggests. Another option for handling spatial autocorrelation is to model the spatial trend by including a semi-parametrically estimated function of the spatial coordinates using, for example, a generalized additive model. See [this related question](http://stats.stackexchange.com/questions/35510/why-does-including-latitude-and-longitude-in-a-gam-account-for-spatial-autocorre) for more info.',), ('What do you mean by "in general"?',), ('I changed the title.  Is that suitable to everyone?',), ("@Zen What is so strange?  The OP meant the sample means.  The are expected values for the empiricla distribution.  I would not have chosen that notation but I didn't want to go about changing that too.",), ("The notation in the original title made it look like the question was why $E(Y) = \\\\beta_0 + \\\\beta_1 E(X)$ (which is true by definition if the model is correct) or possibly why $E(Y) = \\\\hat \\\\beta_0 + \\\\hat \\\\beta_1 E(X)$ (which certainly isn't true) but it seems that the consensus is that this question is asking why $\\\\overline{y} = \\\\hat \\\\beta_{0} + \\\\hat \\\\beta_{1} \\\\overline{x}$ where $\\\\overline{y},\\\\overline{x}$ are the sample means. This seems correct given the text in the question so I've changed the notation to reflect that interpretation. OP, please revert if this is wrong!",), ('The notation seems more appropriate now. I meant sample means. @whuber: By "in general" I meant whether this always occurs (and if not, under what conditions it does).',), ('For a discussion of the opposite case, you can also see here: [how can a regression be significant yet all predictors be non-significant](http://stats.stackexchange.com/questions/14500/), in addition to the question linked above.',), ('But "always" comprises exactly what scenarios?  *You* need to stipulate the circumstances and assumptions to be made, not your respondents! Otherwise some people will assume you are thinking of $X$ as a random variable and others will assume it is not random; some may assume it could be a vector of values and others that it is just a single (scalar) value; etc.',), ('@whuber I am sorry, my limited mathematical capability permits me only so much formal stringency. Please let me know if the formulation is more appropriate now.',), ('What are the degrees of freedom for the t?  If df is > 5 the tails are not very large and the difference from a standard normal are not that great.  Why not try a Cauchy?  Why mix the t with a normal?  Try all errors Cauchy.  Did you pick a model where the slope(s) of the regression curve (surface) is (are) large?  You could also pick a few points that would have errors that would make them highly influential for the least square regression parameters or the bivariate correlation between a predictor and the response.',), ('As far as I understand robust regression and robust methods in general, they work best when the sample is contaminated with a few outliers. This means that certain percent of the sample points do not conform to any relationship. What you have basically done was comparing regressions with iid noise vs regression with independent non-identically distributed noise. Asymptotically the loss of non-indenticallity is not a big deal, i.e. central limit theorem still holds. Thus you do not see big difference, since in your case simple OLS is robust.',), ('with three degrees of freedom I see higher variance of estimator for OLS',), ('Please, [register](http://stats.stackexchange.com/faq#login) your account. That will enhance your experience with this site and will allow you to vote on and accept previous answers.',), ('Note that one perspective on the relationship between regression & correlation can be discerned from my answer here: [What is the difference between doing linear regression on y with x versus x with y?](http://stats.stackexchange.com/questions/22718//22721#22721).',), ('The usual rule of thumb is 10 points for each independent variable.',), ("Is y =ax$^2$ with a>0 the onlt way to get a perfect inverted U?  I don't think so.  If the inverted U corresponds to a different function then there will not be an a that gives a perfect fit to y=ax$^2$.  Then it would be possible for some other model to fit better.",), ('I suspect that you squared the mean-centered x instead of mean centering the squared x. If you mean center before you square, you get a u-shaped predictor.',), ("I'm not sure there's a problem here, but if you can provide your data, people may be able to say more.",), ('@Fojtasek I also tried to mean centering the squared X, then I have a positive beta for the normal independent variable and a negative beta for the squared one. That is exactly the case when there is an inverted U-shaped relation, but in that case there is a multicolliniearity problem because the VIF value is higher than 10.',), ('How are your indices measured? If they include estimates of variability, then two could be enough (using a t-test or its analog). The basic statistical principle that applies here  is that when random variation is an unlikely explanation of what you are observing, then you have the right to attribute any apparent trend to non-random causes. When the trend is strong, very few data values may be needed to come to such a conclusion, all generic "rules of thumb" notwithstanding.',), ("There's very good info at http://stats.stackexchange.com/questions/18480/interpretation-of-log-transformed-predictor .",), ('Thanks for the link Rolando2',), ('The site shows your profile flair so there is no need to sign your posts.',), ('This question is very broad. can you be more specific?',), ('The trouble with OLS is that it assumes the errors are independent, and, if data are clustered, that assumption is violated.',), ("Doesn't this question answer itself (in the negative)? If you make the magnitudes of the $\\\\epsilon_i$ small, then you can induce any correlation you please between them without appreciably changing any of the relationships among the $X_i$ and $Z_i$.",), ("I can't follow. The $\\\\epsilon_i$ are the residuals from the linear regressions. They are determined by the model fit. What choice do I have regarding their magnitude?",), ('You are asking a theoretical question. You can *construct* data by specifying the $X_i$, $Z_i$, and $\\\\epsilon_i$ subject to the constraints implied by your assumptions. You can easily meet all those constraints (about correlations) by choosing $\\\\epsilon_i$ of small magnitude.',), ('I am thankful for the attention my question is receiving from you. However, my question arises from a practical problem where my data have already been sampled and are not being specified by me. I thought it beneficial to try and pose the question as general as possible, but now it appears that posing it in terms of random variables instead of data has made it a wholly different question.',), ('It is correct that questions about random variables are usually different than questions about data. But since nobody has attempted to answer yet, why not just edit this question so that it asks what you really want to know?',), ('Good suggestion. I edited the question.',), ('Great--good job. My previous comments still apply, though. They show you how to construct datasets in which the residuals can have arbitrarily strong correlations regardless of the correlations among the original variables. Such a construction shows your question about making *general* statements must necessarily have a negative answer. Having seen this, you might want to refine your question further to include any constraints on the generality you seek.',), ("Bayesians may make the same assumptions about the error terms in linear regression but the inference depends on priors on the regression parameters.  Frequentists don't use priors.",), ('There are many conventions for doing this. Different journals like different things; different programs do different things by default.',), ('Thanks Peter. I have put in more details.',), ('Many social scientists will be used to APA format, using the style guide of the American Psychological Association.',), ('See [What is a good resource on table design](http://stats.stackexchange.com/q/3542/1036) and [Some notes on making effective tables](http://stats.blogoverflow.com/2012/02/some-notes-on-making-effective-tables/) for references of interest. IMO for presentations you want to focus information much more than typical. Tables with more than 4~5 rows of information are too difficult to read in that venue.',), ('After giving it some thought, I think the answer is plain "no". If you post that as answer, I\'ll accept. If you already have certain constraints on generality in mind that lead to interesting insights, I\'d be very much interested. I don\'t find any, neither in my data nor my imagination.',), ('For an interesting discussion of thorny issues surrounding this topic, see http://stats.stackexchange.com/questions/9334/determining-best-fitting-curve-fitting-function-out-of-linear-exponential-and',), ("And don't forget to make sure the results are CrossValidated ... seriously :)",), ('Broken link: please update the link to "this function".',), ('Note the following thread is related, & will be of interest: [When should you center your data & when should you standardize?](http://stats.stackexchange.com/questions/29781/).',), ('Why do you think that the researchers should have used logistic regression? The response variable is clearly continous (although maybe with some censoring). Please elaborate, if we are to help you in providing a good answer to this question.',), ("Hi.. that's all what is given :S..so u think the answer is a. Yes?",), ("I would say **b** because simple linear regression assumes independence of the residuals, which probably isn't the case with repeated measurements on individuals.",), ('Hi Max..can you please explain why you choose b and nt a?.. thanks',), ('@Francois I think Max is right. I will also go with (b). As these are repeated measurements of your variables, so you should adopt models for longitudinal data. You should take into account the possible correlation in these repeated measurements. The responses closer in time are likely to be highly correlated. There are many other practical issues involved with it, I would suggest that you follow this useful book for theories-http://www.amazon.com/Applied-Longitudinal-Analysis-Probability-Statistics/dp/0470380276',), ('I would also have picked (b) for potentially several reasons, the most obvious issue being the dependence.',), ('Are you able to post a copy of the lecture notes, or link to them?',), ("SSY = SSE if $\\\\hat{\\\\beta} = 0$, which is not the same as $\\\\beta=0$.  Randomness of the residual guarantees, in the case of continuous residuals, that $\\\\hat{\\\\beta}$ won't actually equal 0 even when $\\\\beta=0$..",), ('@jbowman Thanks. But if at the same time SSE/DFE also estimates sigma^2, then my question is how is SSE/DFE = (SSY - SSE)/(DFY - DEF).',), ('Max There is no further explanation. The text is basically when beta=0, (SSY - SSE)/(DFY - DFE) estimates sigma^2.',), ('What do you mean by "better estimation"? Do you mean higher $R^2$ or something else?',), ('I mean that the values I get from the regression (a,b) are closer to the real values.',), ('Then what do you mean by "real values"? The values you get from the regression *are* the real values.',), ('@PeterFlom What do you mean by "The values you get from the regression are the real values"? Aren\'t the coefficient values you get from the regression just estimates of the real values?',), ("OK, @mark999 is right, I wrote too soon. But that doesn't solve the problem. The real values are unknown. So, what does Erel want? Lowest standard error on the parameter estimates?",), ("@PeterFlom Yes I assume that's what Erel wants, together with any other advantages/disadvantages of those methods.",), ("I don't think this kind of assessment is generally used with simple regression models. What would it tell you that you wouldn't find out from using the entire dataset to generate your regression parameters? Normally the reason to use an evaluation dataset is to prevent overfitting, but that's not an issue when you already know that your model is going to contain just one independent variable.",), ('Having admitted you do not yet understand the math, this might be helpful for you http://stats.stackexchange.com/questions/41794/bayesian-updating-for-a-discrete-rating-value/43048#43048 and for practicle use you might wish to look at this http://stats.stackexchange.com/questions/43471/examples-of-bayesian-and-frequentist-approach-giving-different-answers/43498#43498',), ('So that we can understand this question, please tell us what it means for predictors to be "strong" or "weak."',), ('I edited my main post, I hope that makes it clear. Thank you!',), ('Of possible interest: [Explanatory power of a variable](http://stats.stackexchange.com/a/11951/930).',), ("Cross validation (as Nick Sabbe discusses), penalized methods (Dikran Marsupial), or choosing variables based on prior theory (Michelle) are all options. But note that variable selection is intrinsically a very difficult task. To understand why it is so potentially fraught, it may help to read my answer here: [algorithms-for-automatic-model-selection](http://stats.stackexchange.com/questions/20836//20856#20856). Lastly, it's worth recognizing the problem is w/ the logical structure of this activity, not whether the computer does it for you automatically, or you do it manually for yourself.",), ('The concept of _variable importance_ may be helpful. As may be the `caret` package: http://cran.r-project.org/web/packages/caret/vignettes/caretVarImp.pdf\n\nConversely, I would not put too much stock in _statistical significance_, which measures something different than importance (however that concept may be defined).',), ('You can have a look at [Multivariate multiple regression in R](http://stats.stackexchange.com/a/11132/1909) for a walkthrough including multivariate tests.',), ('Cristina, could you please explain exactly how your model would differ from two independent regressions `y1~x` and `y2~x`?',), ('Type "conjugate bayesian gaussian" in Google to find some informations about the derivation.',), ("My dependent variables are highly correlated that's why I want to account for both of them together in the same model.",), ("@Cristina: do you mean a PLS 'regression'?",), ('The models for linear regression and binary logistic regression are both special cases of the generalized linear model (GLM): Linear regression with an identity link and Gaussian conditional distribution (with equal variance), logistic regression with a logit link and binomial conditional distribution.',), ('I discussed the nature of logistic regression in a slightly different context here: [difference-between-logit-and-probit-models](http://stats.stackexchange.com/questions/20523//30909#30909). That answer also briefly mentions the connection b/t linear regression and the generalized linear model. It may be helpful to read it.',), ('Does this help? http://stats.stackexchange.com/questions/12900/when-is-r-squared-negative',), ('well, it does shed some light on the topic (now I realize that my question only concerns linear regression where the y-intercept is not constrained)...',), ('This is discussed in detail in the book by Davison and Hinkley, *Bootstrap Methods and Their Application*, along with an explicit algorithm (Algorithm 6.4). They explain concepts, pitfalls and details at greater length than is possible in a reasonable answer here.',), ('@Glen_b Thanks for the reference. Unfortunately I am not in a university or company so I do not have the resources to acquire the book.',), ('See [here](http://en.wikipedia.org/wiki/Law_of_total_variance).',), ('It can be ordered from amazon; a full explanation of the algorithm and all the associated caveats and issues is not really the sort of thing you can cover in a few hundred words or even a one page answer.',), ('Check also out answers to this post: http://stats.stackexchange.com/questions/34769/regression-detecting-significant-predictors-out-of-300-independent-variables',), ('Note the following threads are related, & will be of interest: [When should you center your data & when should you standardize?](http://stats.stackexchange.com/questions/29781/), & [Variables are often adjusted (e.g., standardised) before making a model - when is this a good idea, and when is it a bad one?](http://stats.stackexchange.com/questions/19216/).',), ("This seems like a math question, not a statistics one. In statistics, we would want (indeed, require) many more points than the mathematical minimum. There's nothing really about regression here - it's more the minimum points to define certain curves in various spaces.",), ('Even in statistics one should inspect the design matrix for adequacy, apart from doing the hypothesis testing on coefficients and VIF analysis...',), ("I have merged your two unregistered accounts, Murphy. Please, don't forget to [register](http://stats.stackexchange.com/faq#login).",), ('REGRESSION procedure in SPSS uses Type III SS only. Use GLM (=UNIANOVA) procedure instead if you want to choose among types of SS.',), ("I guess the other question is type I or III preferred?  I understand when performing a normal anova the differences, but I'm wondering given the hierarchy in her, type I is the way to go?",), ('Yes, type I is often referred to as hierarchical (or sequential) decomposition in linear model. Default (in SPSS) type III is in no way hierarchical.',), ('You may notice that the ANOVA table lists the degrees of freedom associated with the analysis as **0**; you have the same number of variables in both models, that is the reason that no F or p-values can be computed.',), ('1) Ability to find (and **introduce**) bugs is an individual trait, having little to do with the department. 2) Code patches are supposed to **fix** bugs (unless the QA process is not working in your organization). 3) Why are your salesmen coding instead of selling products?',), ("Given that bugs is a count variable you probably shouldn't be using linear regression at all, but some form of count regression such as Poisson or negative binomial.",), ('@Peter: Thank you, I have never heard of Poisson regression, but it sounds useful (Often bug creation modeled as Poisson distributed). I *think* I still have the problem of interaction between patch size and the other variables though, no?',), ('Yes, that problem is separate.',), ('I am not expert in this field, but "lines of code" seems problematic as a variable.... Different languages and different styles within a language, will produce more or fewer lines to do the same thing.',), ('@Peter: Yes, those are valid criticisms. However, my goal is to predict bugs, not "explain" why those bugs occur, so if it turns out that one group has a higher density than another, it could be stylistic. (And we all use the same language). Also, my data set is fairly large.',), ('@Xodarap, I see much uncertainty about the **dependent** variable you want to analyze/predict. Do you currently work with number of bugs per line of code? Forget about this curious "department*LOCs changed", BTW, because the variable makes no sense at all.\n\nOn independent variables: please consider adding time since the initial operating capability of the system, and some quantitative measure of the requirements behind the patch.',), ("@DeerHunter: The dependent variable is bugs. Thank you for your comments about the experiment design, but I think a full explanation of what I'm doing goes outside the scope, and I would prefer to have an answer about the question I asked.",), ('Thanks for the clarification of scope and the left-hand side var. What is the question then, since the only option that remains from the ones you described is:\nLHS: `bugs` RHS: `department`, `patch size`',), ('If your data set is large, why are you worried about overfitting (in the original question)?',), ('@Peter: In backtesting, these methods do *much* better on the training than the testing samples (as measured by $r^2$). Of course some of that is to be expected, but I hope to minimize it.',), ('OK, that makes sense. Interesting problem.',), ('The structure of $\\\\Sigma(x)$ is unknown, I suppose that in my problem it is close to linear. From some plots it looks like a linear fit is not too bad, even though there are some important outliers which are quite problematic.',), ("How much heteroskedasticity is there (roughly, $\\\\max \\\\sigma_i / \\\\min \\\\sigma_i$ as a guess?  If it's less than about 3 or so, the efficiency loss is pretty small, or so I was taught as a rule of thumb...",), ('The heteroskedasticity is moderate, but my estimates are suffering because of some big outliers (several standard deviations away from the regression line).',), ('Probability of what?',), ("The probability of the measurement being correct. Smaller measurements are more accurate, but I'd like to associate a probability with this.",), ('@Peter: I found [this link](http://www.psycho.uni-duesseldorf.de/abteilungen/aap/gpower3/user-guide-by-distribution/z/poisson_regression) which talks about Poisson regression based on an "exposure time". It seems like if I could have my "exposure time" be the number of lines of code changed, this would fit nicely. Do you think so? Do you know what it\'s called when you do regression with a non-fixed time window? I can\'t find any sources on it.',), ('A method that you did not mention is the use of Student-$t$ errors with unknown degrees of freedom. However, this may not be as fast as you need.',), ("@Procrastinator: (It's easy to imagine a configuration of outliers where) this will not work.",), ('@user603 That is true for any method, there is no Panacea ;). I was simply pointing out another method. +1 to your answer.',), ("@Procrastinator: I agree that all methods will fail *for some rate of contamination*. And 'failure' in this context can be defined quantitatively and empirically. But the idea is to still favour those methods that will fail only at higher rates of contamination.",), ('Since this is being done repeatedly during an optimization routine, perhaps the data in the regression are (eventually) changing slowly. This suggests an algorithm adapted to your situation: start with some form of robust regression, but when taking small steps during the optimization, simply assume in the next step that any previous outlier will remain an outlier. Use OLS on the data, then check whether the presumptive outliers are still outlying. If not, restart with the robust procedure, but if so--which might happen often--you will have saved a lot of computation.',), ('The arrows appear to point to the locations where the second derivative of $f$ is largest: is this what you mean by "the point on the curve at which the horizontal component of the curve meets the oblique component"?',), ("Note that you don't need to burn in the simulations, as you're not doing MCMC.  Also how much bias are you seeing? And how many simulations are you doing?  Your simulation based estimate of bias could have a large simulation variance if the number of simulations is not large enough",), ("I'm including a burn in because I don't want my answer to depend on the initialization of the AR(1) process. I've used fairly big sets of simulations (> 10000), and the bias can be around 0.1 if the sample size is small (N = 20). The bias progressively disappears as N increases.",), ('@prob: The point of the "burn in" is that this process does not start at the stationary distribution and so, just as in MCMC, using some burn in period will get you somewhat closer. In contrast to the general MCMC situation, it\'s easy to characterize the rate of convergence and also almost as easy to just start right at the stationary distribution (which the OP hasn\'t done). This also relates to the concept of transient response in signal processing, which the OP might want to investigate.',), ("How are you assessing the bias in this simulation? I've replicated your code and not found any indication that they give significantly different results. (10000 reps)",), ('@AdamO I have included the whole Monte Carlo experiment I have used. But more importantly I found a mistake in my assumptions (see the answer).',), ('It depends on what you mean by "best" and on the nature of your data. For example, if $y$ is measured with additive error then really $y = \\\\beta x^\\\\gamma + \\\\varepsilon$, which is nonlinear, but if $y$ is measured with *multiplicative* error then $\\\\log(y) = \\\\log(\\\\beta) + \\\\gamma \\\\log(x) + \\\\delta$ is actually a linear model. *The two models are not the same*: typically they will produce (slightly) different estimates of $\\\\gamma$ and different standard errors. If also $x$ is measured with error, both models get substantially more complicated and yield yet two more estimates.',), ("What is the reason to constrain the ranges? If it is just a technical question you should maybe ask on stackoverflow, since this is more of a stats theoretical site. If the reason for constraint is a inheret to your model, I'd advise you to consider a (semi)-logarithmic model depending on what you are doing. Cutting out data point entirely tends to be a really bad idea for inference.",), ("I'm trying to fit some biological data, and need to constrain the coefficients such that they are not nonsensical...",), ('Oh I completely misunderstood you then. Sorry. May I ask: Is the linear model necessarily appropriate? What results do you get when estimating right now?',), ("no problem....linear model's a start...right now I get negative values for one of the coefficients, which I can't have.",), ("I fixed up the formatting, but do you really mean to have $C_0$ five times? This would be most unusual; I think you want $C_1$, $C_2$ etc.  Also, you didn't include an intercept; was that deliberate?",), ("yep, that's what I meant, thanks!",), ('I also tried beta regression, but got an error...\n\n`library(betareg)`\n`model5 = betareg(acc~scale(iq),subjdata)`',), ('`library(car)` is necessary, for the logit function.',), ('It may help you to read two of my answers to related questions: [Difference between logit and probit models](http://stats.stackexchange.com/questions/20523//30909#30909) (which discusses link functions & GLiMs in general--a comment at the end specifically addresses your 1 & 3), & [Difference between generalized linear models & generalized  linear mixed models](http://stats.stackexchange.com/questions/32419//32421#32421) (which discusses how your 4 is different from 1 & 3).',), ('Look into Box-Cox transformations, that should be at least a good start.',), ("@PeterFlom I think the OP is looking for some search algorithm that looks to return a normal distribution from the original data. I've always done this by hand visually inspecting the transformation result, the question is quite interesting.",), ("The problem is that i have a separate model for each region of my country, so i can't inspect one by one. There must be some kind of test or package to automate this!",), ('Java code only regresses $y$ on $geno_A$, i.e. you get the same result with $lm(test_trait \\\\sim geno_A)$. I know next to nothing about Java but you probably need to use multiple regression command as opposed to SimpleRegression.',), ('You should provide us with ALL the estimated coefficients form Java, then we might know what goes on.',), ('What do your outcome y and explanatory variable x look like?',), ('I put them here https://gist.github.com/4680507 Dimitriy',), ('I have updated as *EDIT1*',), ("Welcome to the site, @Adam. Do you have a statistical concepts / understanding question here? This question looks half like 'how do I do ______ in SPSS' (which would be off-topic for CV, see our [FAQ](http://stats.stackexchange.com/faq)), & half 'how do regression & time-series work', which really requires you to take a couple semesters of statistics, not a couple paragraph long CV answer (much as I do like them). Can you edit your Q to clarify the specific statistical question that you have?",), ('As with @gung I am uncomfortable with the way you have conceptualized this.  As others have said you will need to take several statistics courses to be able to do a competent job with this type of analysis and its interpretation.  And the fact that you or your boss automatically assume that LINEAR regression is the way to go is quite telling.',), ("What about the effect of net income on the % of women?  Or the effect of something else on both of them? To make any kind of case for a causal effect in an observational study needs a lot more than a 'significant' regression coefficient.",), ("BTW did you consider saying 'No' when you were told? It's what I'd say to my boss if he asked me to do the year-end accounts.",), ('In the part `samples from (Y,X1,....Xn) and then a bunch of conditions by which the OLS estimations behave quite well.` is that really what you meant or did some of your sentence get cut off. Also you have a misspelling in the title of the question.',), ('thanks, I fieed the title (bias).',), ('Yeah, I meant that. You have the sample/observations, and then the conditions (Gauss-Markov), which guarantee the estimators to be the best unbiassed ones etc',), ("As a side note, stepwise selection methods (such as forward stepwise) are very unlikely to pick out the model that you ought to be using. If this doesn't make sense, you may want to read my answer here: [algorithms-for-automatic-model-selection](http://stats.stackexchange.com/questions/20836//20856#20856).",), ("But whatever the method you use (expertise included), you begin from 0, and you are 100% having the problem I talk about... It's like omitted variable bias is there every time",), ('Also, my answer here: [identifying-the-population-and-samples-in-a-study](http://stats.stackexchange.com/questions/31488//32149#32149), is tangentially related, & may be of interest.',), ("You are correct to be concerned.  A lot of inference is based on the assumption that we have the true model.  I've been running regressions a long time and I've never had the true model.  For my purposes it rarely makes sense to even think that one true model exists.  Instead, ask yourself what the goals of your modelling are (prediction in sample, prediction out of sample, estimating the average causal effect of x3, data summary, etc.) because your goals will indicate which modelling strategies are best.",), ("Only conclusion (3) is consistent with your data.  The rest are not. In (1), the number of increases is an exceptionally poor test statistic and so gives you no grounds for any conclusion for or against a linear relationship. In (2), the SD is actually quite *large* compared to the size of the slope.  (That's what a small $R^2$ is telling you.)  In (4), you are being misled by a single anomalous value (the first in the series).",), ('Short answer: No.',), ('Because the value of a beta can be anything (including of absolute value greater than $1$), it cannot generally be interpreted as a partial correlation.',), ('Why do you call it "linear" if you are using a polynomial? Every point you want it to pass through is a constraint that will reduce your degree of freedom. You can then use a constrained optimization algorithm.',), ("It is linear because you are finding co-efficients to a *linear* combination. For example, if you want to fit your data to a cubic, then you are finding the co-efficients (the $c$'s) of $y = c_0 + c_1x + c_2x^2 + c_3x^3$.",), ('@Mohammad: One other way to approximate what you want would be to use a weighted least squares solution, and give very large weights to the points that you want the regression line to pass through. This should force the solution to pass very closely to the points that you choose.',), ('@JasonR Good to see you on here. Yes WLS is indeed an interesting contender. I have gone with whubers answer because of the clever polynomial factorization, and because it maintains the error structure nicely.',), ('I don\'t quite follow your terminology.  When you say "i and j are observations..." do you mean you have two series $X_{1,t}$ and $X_{2,t}$, in which case you would be better off dropping the i and j and going with $X_{1,t}$ and $X_{2,t}$ or even $X_t$ and $Y_t$.  Or does it mean something else?',), ("Sorry - I mean there are two series.  Your interpretation is correct.  I have edited the question for clarity.   I'm copying exactly the syntax of the paper I'm using and didn't explain it clearly enough.  I suppose the reason they used this particular notation is because it's the same variable across a panel.",), ("It's a state space model and so KF estimation would be an obvious tool. I don't know Stata though. If you write the model in terms of differences and simplify, it looks like you can at least estimate $\\\\beta_{t-1}$ with a regression model, though you have to worry about the induced correlation structure in the error term.",), ('@Glen_b thanks for your input.  The more I investigate the more I think this has to be done in R or MATLAB because of the time variation.',), ("Welcome to the site, @nkhuyu. I don't fully understand your question. Are you worried about multicollinearity, or are you worried about finding an incorrect sign due to sampling error alone?",), ('It is multicollinearity.',), ('Linear regression IS a machine learning algorithm!',), ('Zack, that still doesnt answer my question, are the statistical assumptions the same between GLM and BRTs?',), ('Since when does regression _assume_ multi-collinearity?',), ('Dirk: Ive edited my question.',), ("LMs and GLMs don't make *any* assumptions, statistical or otherwise, about the relationships among predictors. This is fortunate because pretty much the only time they'll be all neat and uncorrelated is for balanced experimental data.",), ('Thanks everyone! I am mostly concerned because I am using a BRT to predict species richness across a large spatial gradient. As the predictors I am using 10 environmental variables including three temperature climate variables (Mean annual temperature, Max temp of warmest month, Min temp of the coldest month). The three temperature variables are correlated, is it still use to put them all as predictors in my model?',), ('This thread may also be of interest: [when-and-why-to-take-the-log-of-a-distribution-of-numbers](http://stats.stackexchange.com/questions/18844/).',), ('You should clarify your question: Do you want a time series model? What is nma?',), ("polyfit gives the coefficients only if you supply a model too, i.e. the order of the polynomial. You can't get an analytically model from data completely blindly, you need to at least decide on a general form of a model before hand. All the regression can do is find the coefficients (or parameters) that make your model most closely resemble your data. If you want regression on data without specifying a model then perhaps you need a black box approach like neural networks?",), ('@Dan: Given the question that was asked I think it is important to emphasize taht a neural network is no less a model than a polynomial, it just has more parameters. It\'s a composition of linear combinations of sigmoid functions (typically). The "backtracking" algorithm is an instance of gradient descent in parameter space.',), ("@DCS: I agree with that. I guess it's just a case of how expressive you want your model to be. My interpretation of the question is that the OP wants to end up with a model that he can read an understand, NNs won't provide that which is why I called it a black box approach. But yes, it is just regressing a more complicated (more parameters at least) model, but a very general model that is likely to fit many shapes of data.",), ("Do you mean the skewness and kurtosis of the dependent variable or of the *residuals*? If it's the former, your problem is complicated by the fact that those moments depend on the regression coefficients themselves and on moments of the IVs.",), ('We do not "totally disregard" that the coefficient is jointly normally distributed with the others. Just look at how the variance of each coefficient is obtained to see that. And, there are plenty of tests that take into account multiple coefficients. The standard $F$-test is one, [Scheffe\'s method](http://en.wikipedia.org/wiki/Scheff%C3%A9%27s_method) is another, Wilks\' generalized variance and ellipsoids of concentration yet another. It comes down to what statistical question you are interested in.',), ('I guess my question is the follows, if my null hypothesis is that a particular beta is zero. How do I do it? Do I have to figure out the marginal distribution of that beta from the joint normal distribution?',), ('Yes, you do: It is $\\\\sigma^2 e_i^T (X^T X)^{-1} e_i$ where $e_i$ is the elementary vector corresponding to the $i$th coefficient.',), ("Thanks for the note Cardinal but can you please elaborate/derive that. In particular I am reading the book by Tibshirani et al, The Elements of Statistical learning. Seems he defines the z score as z = beta/sigma*sqrt(v), where v is the corresponding diagonal element of the XtX matrix. He says that the score should have a t distribution and uses it to test for the significance. I just don't see how we get to that result. And also why that z score should have a t distribution. How do we get to that score, is that the marginal distribution? Many thanks!",), ('@cardinal I think with a little bit more explanation that would be an answer rather than a comment.',), ('ganesh, your most recent question in that comment looks like a new question to post ("How is it that this scaled parameter estimate has a t-distribution under the null hypothesis?") ... but it\'s one you should search for answers to first. It relies in part on [this result](http://en.wikipedia.org/wiki/Student%27s_t-distribution#As_the_distribution_of_a_test_statistic).',), ('While @Glen_b gave you an excellent answer, I would like to note that if your $\\\\sigma_i^2$ (or the proportionality constants $\\\\tau_i$, in his notation) are not very accurate, then you may end up being worse off with your regression than you would have assuming equal variances. Econometricians have given this topic a lot of thought in the 1970s-1980s.',), ("My comment on June 22, 2011 in user whuber's answer should include the subscript $i$ in the $\\\\epsilon$'s, and should make use of the fact that the error terms $\\\\epsilon_i$ are independent.",), ('$\\\\mathrm{Var}(\\\\hat{\\\\beta_1}) = \\\\mathrm{Var}\\\\left(\\\\sum\\\\frac{(x_i - \\\\bar{x})y_i}{S_{xx}}\\\\right) = \\\\mathrm{Var}\\\\left(\\\\sum{\\\\frac{(x_i-\\\\bar{x})\\\\epsilon_i}{S_{xx}}}\\\\right) = \\\\mathrm{Var}\\\\left(\\\\frac{(x_1 - \\\\bar{x})\\\\epsilon_1}{S_{xx}} + \\\\frac{(x_2 - \\\\bar{x})\\\\epsilon_2}{S_{xx}} + \\\\ldots + \\\\frac{(x_n - \\\\bar{x})\\\\epsilon_n}{S_{xx}}\\\\right) = \\\\frac{(x_1 - \\\\bar{x})^2 \\\\sigma^2}{(S_{xx})^2} + \\\\frac{(x_2 - \\\\bar{x})^2 \\\\sigma^2}{(S_{xx})^2} + \\\\ldots + \\\\frac{(x_n - \\\\bar{x})^2 \\\\sigma^2}{(S_{xx})^2} = \\\\sigma^2 \\\\left[\\\\sum{\\\\frac{(x_i-\\\\bar{x})^2}{(S_{xx})^2}}\\\\right] = \\\\frac{\\\\sigma^2}{S_{xx}}$',), ('Could you please provide context, details, and an explanation of what you mean by "diverging" and "significantly different" regression models?',), ('I think it would help to phrase the question without using the term "significantly". What do you really want to know?',), ('Re the edit: the answer depends on whether the two models have any data in common or not.',), ('I think you might be interested in  [prediction bands](http://en.wikipedia.org/wiki/Confidence_and_prediction_bands)',), ("Isn't this just a parallel slopes question? do you want to know if the slope is the same in the two groups? Just put an interaction term in the model, and look at the parameter test for the interaction term",), ("Firstly, apologies I made the original post but made a hash of registering and now can't comment as the person who asked the question! Testing if the the slopes are different is not the same thing. The slopes are different but that doesn't tell me if the solutions are the same between 0< $x$ <10. Continuing from the example, I would like to be able to say at what temperature (x) there is a benefit in using material/case 1.",), ('You may fix up your registration by merging all your accounts at http://stats.stackexchange.com/help/user-merge.',), ("You'd need to define what you mean by 'trend strength'. There are many possible interpretations.",), ('@Glen_b - I added an example, thank you.',), ('(+1) The edited question is exemplary in how well it explains what is needed and provides a context for answers.',), ('@whuber - thanks, I hope somebody will answer my question, I am sure this is not so complicated issue for local statisticians.',), ("What's the matter with Peter Flom's answer? Your comments to it do not seem to acknowledge what he has told you, but instead ask about things that--even after your edits--appear to me to be irrelevant or extraneous to the question.",), ("@whuber - probably I didn't understand Peter Flom's answer (R^2). I'll check it once more time (I deleted these comments)",), ("I am referring to the first part of his answer. I believe the reference to $R^2$ is there only because he was trying to get clarification of your original question: with your edits, it is evident that $R^2$ is not relevant (and may even give results you find to be erroneous.)  You are asking about the *size* and amount of *trend* and that is measured by the slope term: it's that simple.",), ('(1) Could you please explain what you mean by "input"? What is the form of your model and what aspect(s) of it constitute "input"? (2) In ordinary least squares regression there is no penalty on (dependent) values, *per se,* but only on the *residuals.* In what sense can "values" (presumably *data*) be "constrained"?  They are, after all, your data! Are you proposing to change them or are you really talking about constraining the fit? Or maybe constraining any predicted values? All in all, it\'s hard to guess what you\'re asking.',), ('Input in this case is a sparse vector of ~1500 binary variables.  The output is a number between 0-100.  Essentially I am attempting to "learn to rank" given a human generated ranking.  But, I never mentioned the inputs above.  Maybe you misread?',), ('Yes, I misread "output" as "input" (during multiple readings--a strange trick of the mind!). But although your problem is now clearer, many of my questions remain: it\'s hard to guess what you\'re doing when you have supplied such a broad and abstract description of your problem and it would be difficult (in any responsible manner) to suggest a solution without knowing more.',), ("It's just standard least squares linear regression, nothing fancy.",), ('"Standard least squares regression" is a *procedure,* not a problem. If you restrict yourself to a procedure that happens to be inapplicable or inferior for your data, you may get bad answers (or the good answers will try to make the same points I\'m making). That seems to be the case here. By describing your *problem* and allowing respondents to suggest *solutions,* you are more likely to get useful and correct answers.',), ("Quick comment (I hope I haven't misunderstood your question): it is $\\\\text{posterior}\\\\propto\\\\text{likelihood}\\\\times\\\\text{prior}$ and not equal to. There is a proportionality constant that should not be neglected and that ensures that the posterior is a probability density for any likelihood and prior.",), ('This sounds a question asking for re-assurance, and basically, yes you\'ve got the interpretation correct.  Bayes is similar to the "loss function" + "regularising penalty" framework of machine learning.  In bayes you have log-likelihood and log-prior as the equivalents - but you also get accuracy measures as well as predictions.',), ("Yes, it has a statistical basis: and that basis shows your technique is not a valid way to identify outliers! If you use *Mathematica's* `NonlinearModelFit` procedure, it will give you influence measures, prediction confidence intervals, prediction standard errors, and standardized and studentized residuals: all of these are useful for identifying outliers (of various types).",), ('@tibL Yes, I meant proportional to the likelihood $\\\\times$ prior. I already change that part in my question.',), ("@probabilityislogic Yes, I'm looking for re-assurance and in the process, I'd like to get another detailed description to make sure I understand how it works. The part that I thought was a bit weird is that we have a likelihood with $\\\\bf{w}$ as unknowns, so it's not possible to solve it (although we can plot it for every value of $\\\\bf{w}$) but after multiplying likelihood by the prior, we obtain a probability $p({\\\\bf{w}}|{\\\\bf{t}})$ where is natural to ask for a given $\\\\bf{w}$ and get a number.",), ('Agree with @whuber, the trend strength is the slope. Maybe you  want to consider here how different the slope is from zero. You might want to do this by looking at the ratio of the slope estimate and its error. If you are familiar with R, this is the t value given by the summary of a linear fit. But this is different from the "trend strength".',), ("I am confused by your question because there don't seem to be any interactions that include your two covariates of interest.",), ("Hey can you explain me why the solution here: http://stats.stackexchange.com/questions/53324/spotting-trends-in-time-based-data/53350#53350 doesn't bring you what you want ? (just curiosity)",), ('I have plotted the data, and I don\'t see a straight relation; can you please elaborate on - "It might give you some inspiration as to the kinds of formula you\'d want for a4"?',), ("Sorry, I thought it might work, but I just tried and it's not even close. I've deleted my previous comment.",), ('Because you appear to be in an exploratory mode ("trying to come up with a model"), why not first use exploratory methods like loess smoothing?',), ("`nls` does non linear least squares in R. It is used differently from `lm` because the models aren't linear; you can't leave the parameters implicit. They appear in the formula. See `?nls`. There's also the possibility of using glms if your model has a linear predictor (i.e. a transformation of the mean of the response is linear in predictors). But I would agree with @whuber - if you don't have a specific functional form in mind, start with some exploratory tools.",), ('I tried `gam`, and I am getting better prediction accuracy. This might be a dumb question - but I am getting this vibe that a linear model is generally sufficient for most cases, may be this is a topic for another question, but comments are welcome.',), ('suggest you move your "another question" into a question in its own right - it looks serious enough to need its own answer.',), ('Thanks. I will create another question.',), ('We can\'t tell you if the model is "right" or not without more information about the goals for your analysis.  Perhaps you want to describe what you think the output of the regression mean and people can offer confirmation or correction.',), ("You've got 167 variables and 35 observations.  You're problem may well have a solution, but it is likely to be a complicated one for someone who is unfamiliar with basic regression mechanics.  I'd suggest collaborating with someone with better statistical fluency, and taking the opportunity sometime to work with simpler datasets (many observations, few variables, normal distributions) to get you to a place where you'll be able to deal with more complex situations.",), ('Basic question like this are almost always duplicates - [How can adding a 2nd IV make the 1st IV significant?](http://stats.stackexchange.com/questions/28474/how-can-adding-a-2nd-iv-make-the-1st-iv-significant) Also essentially duplicated [here](http://stats.stackexchange.com/questions/30363/factor-significant-within-model-but-non-significant-after-drop) and [here](http://stats.stackexchange.com/questions/26014/univariate-non-significant-results-becoming-significant-on-multivariate-analysis). That took approx 40 seconds.',), ('The answer appears in [your previous thread](http://stats.stackexchange.com/questions/54896/regression-on-means-of-log-transformed-variables): the methods are not equivalent. Consider, too, that a few thousand observations is by no means "large." But your final question looks worthy of consideration: what should be done when you have so many observations that they are more than you need and more than your computer can cope with? The answers depend partly on the purpose of the regression: is it for exploration, testing hypotheses, prediction, or perhaps something else?',), ("The coefficients will all be estimates of the same population parameters, and averages of them would eventually converge to the whole-sample estimates but the p-values won't 'average' in the way you suggest. It's not clear quite what you intend by 'insignificantly different' though.",), ('First question: Your method sounds like some sort of bootstrap; these methods are well developed and there is a huge literature on them. Probably the most common reason for using a bootstrap is that there is not a good existing estimate of the variance of whatever you are trying to estimate. Why are you suggesting this method instead of the usual one? What are you hoping to learn or gain from doing this? Second question: 1000 is by no means a "large" number of observations these days. If you had many millions or billions of observations, you might want to sample to cut down on running time, bu',), ('+1, I think this is a good question. Note however, that some similar question have been addressed before on CV. It may be worth your time to [search the site for regression assmptions](http://stats.stackexchange.com/search?q=regression+assumptions) & see what already exists. I can recommend [what-is-a-complete-list-of-the-usual-assumptions-for-linear-regression](http://stats.stackexchange.com/questions/16381/) & [what-does-having-constant-variance-in-a-linear-regression-model-mean](http://stats.stackexchange.com/questions/52089/).',), ('This sounds like a strange thing to want to do. WHY do you want to see if two series are "significantly correlated"? Also, time series are tricky; if you "struggle with statistics" you may want to find a consultant who knows more.',), ("I've noticed your flag to delete this question. I don't think we need to delete the whole thread, especially given the fact an answer has been upvoted and accepted.",), ('You could interpolate using either nearest neighbor, linear, or spline/pchip.  You could use a Kalman Filter.  You could use a smoothing spline and AIC and interpolate in the spline.  What is the question that the correlation is going to answer?  I prefer to use MatLab but many folks on this site seem to prefer "R".',), ('As with just about any basic introductory statistics question (i.e. an issue that is covered any introductory statistics class), this has been answered multiple times on this site before - e.g. [What is a complete list of the usual assumptions for linear regression?](http://stats.stackexchange.com/questions/16381/what-is-a-complete-list-of-the-usual-assumptions-for-linear-regression). Please consider taking a minute to search the site before asking/answering questions like this (finding this duplicate took approx 30 seconds), as cluttering the site with duplicates does no one any good.',), ("You can't tell visually, but you can get some hints. Right away you can see that the relationship between $Y$ and both $X$ variables is non-linear. This is an indication you might want to log-transform the variables.",), ("thanks. yes, i am contemplating this non-linear behaviour. can i sa y that if its non-linear, then the data won't fit? (before the log transform) thanks. Or should i say that we cannot decide until we log transform it ? thanks",), ('It lloks like you have some outliers in your data: you may want to remove this and have a second look at the scatterplots. Besides that, I would be careful drawing conclusions from 2D-plots for a 3D analysis...',), ('The easiest way to "justify" the failure of a model would be by showing that your residuals are not Gaussian; $N(0,\\\\sigma^2)$. That would violate the assumption of normality and justify the transformation of your data (maybe all of them, maybe some of them). While there are indication that "*the data won\'t fit*" just showing a scatter plot of the data is not sufficient. BTW, check if square of your predictors do a slightly better job and think what is the physical meaning of your logged predictors for example.',), ("You really can't say anything until you model it.",), ("Clearly the marginal relationships are nonlinear. The question is whether the conditional relationships are (I'd bet heavily on 'yes'). If none of the variables are exactly zero, I'd start by looking at same plot, but of the logs of the three variables, which might also improve the apparent heteroskedasticity.",), ('thanks for all your answers. actually I am just required to say whether the model fits the data or not, without going into the remedy like log transform which I am going to do that later on. So can i say that "its non-linear and model looks like cannot fit the data but have to do a transformation to find out if its true" ?',), ('since you say that there is a curved relationship between the predictor and the response, have you considered a quadratic term in your regression model?',), ('I tried square and root transformations; but if you mean adding squares of predictors to the regression equation, then no.',), ('yes, if you see a curve linear relationship between predictor and response, then perhaps a model that contains a quadratic term, $X^2$ in one of the predictor terms would be a better fit.',), ("if transforming either leads to a reasonable description of the mean, I'd focus on which one did a better job of describing the variance. What are your responses and predictors measuring?",), ('My predictors are signals going through a medium with attenuation and response is the distance. Physical background of all that says that the dependence is logarithmic. @Glen_b',), ("Can you write down what the physical background suggests the relationship is? Surely the physical background doesn't have an ambiguity about where the logs go!",), ('@Glen_b\tPhysics would lead to transforming predictors, however, then their distributions differ from normal, and R2 is less, regression is worse. I maybe would prefer to transform the response for better prediction (its distr. becomes closer to normal after transf.). So I want to understand _statistical_ rules for regression.',), ("So ... distance becomes left skew after taking logs? What is the functional form of the expected relationship of the original variables? [Note that regression doesn't assume the response is unconditionally normal, and it doesn't assume the predictors are anything.]",), ("what does the prime (') mean here? is that the derivative?",), ('transpose of the matrix.',), ('and |X| is the determinant of X?',), ('That is correct',), ("I wouldn't bother comparing these models until after checking their goodness of fit.  I think you will find in the second one that neither the response nor its logarithm are linear functions of time.  This calls (seriously) into question any comparison of the slope estimates.",), ('@Glen_b The transformed response is not skewed, almost normal (log with some empiriclly found coeff.). [The original disributions](http://pp.vk.me/c407129/v407129874/6c39/9NLhYpU0DjQ.jpg) (the last is the response). All predictors are in linear relationship between each other (highly correlated) and in nonlinear with the response [(like this)](http://pp.vk.me/c10418/u17020874/153949434/x_9898cf38.jpg)',), ("Thanks. However, there are no assumptions about the distributions of the predictors, so they may look like anything whatever, and it's not possible to tell from this whether the *conditional* distribution of the response is non-normal - you have there a plot of its unconditional distribution, about which no assumptions are made. In any case, if that was the conditional y-distribution, I would hardly be concerned by it at that sample size. I'd worry much more about likely heteroscedasticity.",), ("When you say 'optimal', what is to be optimized? Is $\\\\phi$ to be minimized?",), ("It's questionable whether one should do post-hoc power analysis at all. See e.g [this article](http://effectsizefaq.com/2010/05/31/what%E2%80%99s-wrong-with-post-hoc-power-analyses/). This refers to post-hoc as in after the *analysis* has been done, not just after the sample has been collected.",), ('Interesting. Someone else has already done analysis on this data, where we did achieve significant results. However, someone else was questioning whether we had enough samples to validly perform a multiple linear regression. Does the presence of significance indicate that we do have enough samples, or is there another way to determine this?',), ('Are they worried about power or over-fitting? If your result was significant, you had enough power to detect the effect size that you found. But if your N was less than about 10 times the number of variables, that can be problematic.',), ('Our N is greater than 10 times the number of variables for some models, and less than 10 times the number of variables for other models. What does having an insufficient N indicate, even if you do get significance?',), ('That would indicate possible overfitting of the data. To give an extreme example, if you have N-1 independent variables, you will get a perfect fit even if all the data is random.',), ('I thought I had responded to this, sorry = $\\\\phi$ is to be maximised',), ('Self study? Homework? If so, tag it so.',), ('@curious_cat homework tag is [deprecated](http://meta.stackexchange.com/q/147100/182549) on SO, no need for it here either',), ("When both $y_1$ and $y_2$ are linearly dependent on $x$, *of course* they will be correlated to each other by virtue of those relationships. What do you want to get out of your proposed model that isn't already produced by the two separate regressions?",), ('If you find the duplicate does not have enough detail, check out other similar questions by searching on [regression prediction limit](http://stats.stackexchange.com/search?tab=relevance&q=prediction%20limit%20regression).',), ('Forget dose for a second. How would making the intercept 0 in that equation make the survivorship 100% in any case?? How is dose measured? Is it categorical with different levels?',), ('Typo - as you pointed out I should have said set the intercept to 100, not 0.',), ('Is dose categorical or continuous?',), ('When you refer to `gender * hypertension` as "an interaction term", you appear to be confusing it with `gender : hypertension` (which is the interaction). In R formulas `gender * hypertension` is the same as `gender + hypertension + gender : hypertension`.',), ('Our site contains [over 100 threads](http://stats.stackexchange.com/search?tab=votes&q=interpretation%20interaction%20regression) concerning interpreting interaction terms in multiple regression! You will find a rich resource there for pursuing your inquiry.',), ('Dose is continuous.',), ("I'm not sure the sense in which you mean 'will also constrain the dose'",), ('Survivorship = b1*time + b2*dose + b3*time*dose + 100\ntherefore suvivorship only equals 100 when dose and time (or there coefficients) are equal to zero. I would like to be able to define the model such that the dose at the start of the experiment (time=0, survivorship=100%) is unconstrained.',), ('Since described feature is not linear, MLR cannot do this. In my opinion, survival analysis (e.g. fitting proportional hazard model) can solve the problem',), ('@ O_Devinyak thanks for your suggestion. I will look into this. Also, please can you explain why the described feature is non-linear. I do not understand how you have reached this conclusion.',), ("It doesn't constrain the dose - it constrains the effect of the dose. Your problem is that your concept is something like multiplicative in the hazard, but your model is not. Imagine some baseline survival curve against time, and them imagine the effect of dose... what should happen? That governs the effect. Regression is not directly a suitable tool for this kind of work; issues like censoring make it less suitable again. There are both parametric (e.g. Weibull models) and nonparametric/semiparametric (e.g. KM, proportional hazards) survival models.",), ('I have just realized that to constrain surviorship to 100% would be incorrect as I do not know at what time point outcome moves below 100%. I will look into survival models. Thank you for you help.',), ('This looks like standard bookwork. If this is for some subject could you add the self-study tag please?',), ('The answers you are getting seem to be missing the question you are really asking and are hung up on least squares analysis, which is not what you want. Searching our site for [errors-in-variables regression](http://stats.stackexchange.com/search?q=errors-in-variables+regression) and [PCA](http://stats.stackexchange.com/questions/tagged/pca?sort=votes) will head you in the right direction. In the meantime, think some more about what you really mean by "close as possible": it\'s easy to construct (infinitely many) curves that pass *exactly* through your points, in sequence.',), ('Welcome to the site, @Jrod. Just for the sake of clarity, what do you mean by "binomial"? For example, do you mean that it\'s *binary* (the only values that exist for this variable are $0$ or $1$), or that these are counts of \'successes\' out of some total number of trials with a given probability of success?',), ('Yes, I meant binary.  I have two different areas where the data was collected and have coded them as 1 or 0 and used it as an interaction term in my regression.  However the variable by itself had a significant p value in some of my analyses.',), ("Hmmm. Can you edit your question? Do you have an interaction term in your model? If so, that's crucial information.",), ('I do have an interaction term.  The equation works out to be be y=x*g+a+g  where g is the binary variable.',), ('CLRM: curiously labelled rebarbative model? You shouldn\'t assume your own private abbreviations are universal, so please explain. Whatever model you are talking about, there won\'t be a single command that will "correct" violations of assumptions.',), ('@NickCox Your right. I assumed CLRM was a universal abbreviation since it is stated as in few textbooks I picked up. But fixed it.',), ('What made you choose a parametric model?  A semiparametric model (e.g., proportional odds ordinal logistic model for continuous $Y$, quantile regression, or Cox model) may be more appropriate and have fewer assumptions.',), ("My initial reaction was to start with a `glm(..., family=poisson)`, b/c you have count data.  I would start with `summary(glm(Nodes_removed~(.), data=study_data, family=poisson))`, but I notice the QQ plot doesn't look so great (if you plot the glm() object you get diagnostics).  Also, there are a lot of terms in there, and I don't know what types of interactions etc. seem sensible.  With data like this, I would do careful thinking about what makes sense, b/c blindly including a lot of terms can be dangerous.",), ('(+1) Thanks for your help!',), ('possible duplicate of [In linear regression, when is it appropriate to use the log of an independent variable instead of the actual values?](http://stats.stackexchange.com/questions/298/in-linear-regression-when-is-it-appropriate-to-use-the-log-of-an-independent-va)',), ('I think this is a bit different from that question, due to the random forest/ensemble aspect, but the question could perhaps be reworded after looking at the other post.',), ('@PeterFlom could you help me reword the question? I am not proficient enough in this field :)',), ('I assume that this is homework/self-study related. Please see the following link: http://meta.stackexchange.com/questions/10811/how-to-ask-and-answer-homework-questions/10812#10812 Try to show us your train of though. Someone just spitting out `b`-`b`-`b` would be quite useless for you anyway. ( `b`-`b`-`b` is not valid answer)',), ("Welcome to the site. Further to @user11852's comment, please add the [`self-study` tag](http://stats.stackexchange.com/tags/self-study/info) and take a look at the section on asking questions in the [faq](http://stats.stackexchange.com/faq) especially the later part of the section relating to asking questions",), ('Well, for question three I would think that model B would have a smaller r^2 value because there are more values, so there is a greater chance of correlation.  For question 1 both b and c have terms with an order above 1. And both a and c have terms with exponents. But I am thinking that c is not a linear regression because there is something being multiplied by B0.',), ('I have edited your question for formatting and clarity (but I have not tried to fix the English errors). Please double check that it still says what you want.',), ('Thank you! And I have added the self-study tag and commented with my thought process behind the questions. I have spent a lot of time thinking about them and just want to make sure I fully understand the concepts, so any help is appreciated!',), ('I have edited your title so it matches your question  - which seems to be about observations, not variables.',), ('If this is for some subject, could you add the `self-study` tag please?',), ("The autocorrelation of overlapping weekly periods will be high, too, so that's no cure for what ails you.",), ("I am looking an assessment of house prices.  My independent variable is Assessed House Price divided by Sales Price.  My dependent variables are several race categories (percent black, white, hispanic, and asian) and median household income.      \nI'm finding that census tracts with a larger percent of blacks have a higher assessment to sales price ratio than other areas.",), ('Confidence intervals for what?',), ("I wouldn't aggregate; just consider the use of a smoothed response, mostly simply some kind of weighted moving average. Nothing stops you modelling that. You would be implying that most daily fluctuations are uninteresting or unintelligible, but that seems to fit what you are saying.",), ('Confidence intervals for the mean difference.',), ("I'm having trouble picturing what you're doing. Are you after a confidence interval for a coefficient in your model? If not, perhaps you could explain both the model and the source of the mean difference in the model.",), ('Scale parameter makes it possible to account for nonhomogeneity variances http://stats.stackexchange.com/q/48237/3277',), ("Does this apply to models that don't use probit or logit? My example specifically uses two continuous variables.\n\nDoes scale still just account for heteroscedasticity?",), ('Specifying skewness and kurtosis is not sufficient to determine the distribution. Can you explain more?',), ("Furthermore, can you show what you've tried?",), ('Is your question on how to *technically* plot the lines and points in `R`?',), ('Your title says "SE", while your text says "SD"; which is it?',), ('It is SD. I just want to know how to fit the regression line to the mean and SD of the points. Thx',), ('Welcome to the site, @user26091. I took the liberty of editing your question with $\\\\LaTeX$. Please make sure it still says what you want it to.',), ('Thanks!  It does not quite say what I want it to however but close.',), ("I thought there was something wrong with c, but I couldn't figure out what it was supposed to be.",), ('Since it\'s for the purpose of study, please add the self-study tag. I took a stab at improving "c" - if that\'s wrong, please clarify.',), ("I think that works but I'm just trying to find notation for each random value minus the residual. And will add!",), ('Hi @gung How did you make "LaTeX" look like that? It\'s cool.',), ('@PeterFlom, $\\\\LaTeX$ is "\\\\$\\\\LaTeX\\\\$".',), ("No. If I denote $y=X'\\\\beta+\\\\epsilon$, then for example if   $\\\\beta=0_p$ you have $y=\\\\epsilon$.",), ('@user603 What if we substitute it for Yhat?',), ("You mean it won't be zero anymore?  It will be 1 then, right?  Why is that?  Is it related to the fact that $\\\\sum k_i X_i = 1$ and $\\\\sum k_i$ = 0?",), ('What about in a simple linear regression?',), ("No,  my example doesn't say anything about what happens when you substitute $X$ by $\\\\hat{Y}$. But if you substitute $X$ in your question by $X'\\\\beta$ you will have 0 again...",), ('Alright..how might one go about proving such a statement?  Is your statement just saying multiplying by a constant still yields zero?  I am still not entirely clear.',), ("no my statement is saying that OLS makes the $e$ linearly orthogonal to the $X$ and thus to any linear combination of them, including $X'\\\\beta$. If two vectors are orthogonal, their dot product is 0...",), ("Nobody knows $\\\\beta$, surely? How is anybody computing $X'\\\\beta$?",), ('The first thing is to formulate the question you are trying to answer. Assuming you believe that the salary is dependent on the years of employment and not vice versa you indeed look at the `lm(Y ~ X)` model . Don\'t worry about "the mean", just add an intercept in your model and it will take care of that. What you care after making this regression is how your model\'s residuals are distributed.',), ("What do you mean by that?  The probability distribution they follow?  Aren't they normally distributed?",), ("Yes, that's what I mean. You would like them to normally distributed; they are not guaranteed to be.  If they are not you have to re-state your model in a way they end up being. A linear model does assume that practically $Y \\\\sim N(X\\\\beta,\\\\sigma^2I)$ so you want a nice residual distribution $\\\\epsilon \\\\sim N(0,\\\\sigma^2)$ to go along with it.",), ("No, they are not independent, as they come from the same set of sample data.  This is the basis of the reason for adjustments such as the Bonferroni Joint Confidence Intervals (this you'll probably learn down the road).  But as a simple comment, no they are not independent.",), ('That makes sense, thanks!',), ('please add the self-study tag',), ('Added, thanks Glen.',), ('If `X` is a continuous variable in the regression, the coefficient is the change in `Y` for one unit of `X`. If you multiply the regression coefficient (slope) by 10, then this represents the change in `Y` for 10 units of `X`.',), ("Right, but how does that relate to an increase of 10%?  It doesn't, does it?  It is not as though for example we are moving from 100 where 10% would be 10.  Sorry if this is ambiguous but the question itself seemed rather ambiguous.",), ("A priori: no it doesn't relate to an increase of 10% in `X`. There is a special case: If `X` is log-transformed and `Y` not, then 1% increase in `X` is associated with a $\\\\beta/100$ unit increase in `Y` (see [this post](http://stats.stackexchange.com/questions/18480/interpretation-of-log-transformed-predictor) for more information).",), ("What's the matter with rewriting $\\\\exp{( \\\\ln{X} - \\\\ln{IC_{50}})}$ as $X \\\\exp{\\\\left(- \\\\ln{IC_{50}}\\\\right)}$? All your numerical difficulties vanish.",), ('Can you not algebraically simplify your function? To: $Y=\\\\frac{IC_{50}M + BX}{IC_{50} + X}$',), ("I used that equation because I'm familiar with it, not because it's a specific equation giving me trouble. I've updated my question with a less convenient example.",), ("That example is still readily simplified in a way that doesn't require $\\\\ln0$: $Y = B + \\\\frac{(M-B)}{1+(X/IC_{50})^H}$.",), ("Alright you got me, I was rushing to think of an example between lab work and got sloppy... pretend I had an example on hand that didn't simplify. I'm asking about a theoretical statistical method, not about a specific equation.",), ('If, as seems to be the case, you\'re talking about constructing an independent (predictor-, $x$-) variable, there are no assumptions you need to satisfy for OLS or GLMs. Your question "or in general" is unanswerable. I\'m not sure what you mean by "SAR"; seasonal autoregression? something else? As for what reviewers might say, one could only speculate as to what your reviewers *might* come up with. It tends to be a bit of a mixed bag at the best of times.',), ('Thanks for your answer! By SAR I mean "simultaneous autoregressive model", which is a type of linear regression model that accounts for spatial autocorrelation in the model residuals. Is there not something like an assumption that a continuous predictor variable is actually genuinely continuous?',), ("But there's no assumption in OLS or GLMs that a predictor should be continuous; it is what it is and you condition on the observed values. If it were observed with error, that might be an issue; discreteness shouldn't be.",), ('And should I worry about the scatterplot? Right now there are two vertical bars full of data points at zero and one and just a few data points in between. The regression line seems somewhat lost in that plot. Should I maybe instead report a boxplot based only on the datapoints that are either fully within or without a participating country even though I used intermediate values in the regression models?',), ("Do you mean that you don't think the scatterplot is very informative? (Does it look something like [this](http://i.imgur.com/wPjf2V4.png)?) As for what you should do instead, if you find a boxplot more informative, you could certainly do that, but there are [other possibilities](http://i.imgur.com/raBRe66.png) that may work well.",), ("Yep, that's pretty much how it looks. But I really like your second approach of visualizing it, thank you!",), ("You need to treat `sibsp` and `parch` as factors, not as numeric variables. Split the data into training and test set using stratified randomization to ensure that both sets have the same combinations of all explanatory variables. And don't expect wonders: after all, for each person with a sibling on board, there will be another record in the dataset with *identical* values of all independent variables but with a different age (unless they are twins/triplets etc.)!",), ('Oh. I have one response with more than one predictors.',), ('Please have a look at [this post](http://stats.stackexchange.com/questions/59784/regression-for-a-model-of-form-y-axk) that deals with a similar question. [This paper](http://onlinelibrary.wiley.com/doi/10.1017/S1464793106007007/abstract) might also be of interest.',), ('"exponential" usually implies something based on `exp()`: what you have here is more commonly called power function, power law, or scaling law. Other names no doubt exist. There is no connection with power in the sense of hypothesis testing.',), ('How many times did you re-run the test.  Replication can indicate variation (or consistency) in results.',), ('The fact that kNN can beat SVM is not really surprising, nevertheless I somewhat feel your problem will look way better after taking a log of decision (at least).',), ("@mbq Sorry, I am somewhat confused. Do you mean that I should take the log of the prediction (SVM's output) before calculating the correlation coefficient?",), ('Rather log the decision before both making a model and calculating correlation coefficient. If your values have a huge and skewed spread neither SVR optimization target nor Pearson correlation make any sense.',), ('As this is basically standard bookwork, please add the `self-study` tag. You might like to read its [tag wiki info](http://stats.stackexchange.com/tags/self-study/info).',), ("The errors of your observations are function of a random variables (the y's) and are therefore themselves random. Conditional on X alone, they are not given.",), ('Yes, I fully agree on that. But what you say works in theory. If I draw, say, 100 random samples of identical size from the same population, each observation error will be a random variable with (0, sigma^2). What if, instead, I only draw one sample? In that case, the mean of the error of each observation is the error itself. Is it clear what I am saying? So, what I am trying to understand is, how does a package like Stata calculate the variance-covariance matrix using only one sample drawn from the population?',), ('Check out the BTYD R package, which has some models for calculating CLV based on RFM statistics. http://cran.r-project.org/web/packages/BTYD/index.html',), ("See @gung's answer [here](http://stats.stackexchange.com/questions/33433/linear-regression-prediction-interval). Nothing to do with heteroscedascity.",), ('Or even heteroscedasticity.',), ('You assume equal variances around the true regression line, not your fitted one.',), ("Yeah, I already checked that. Great approach by Fader et al. However, I'm right now interested in regression-type models like the one described above to tackle this task.",), ('Please spell out your abbreviations. Presumably, "SSR" is the sum of squares of residuals and "df" is degrees of freedom. If that\'s so, then how do you determine that df = p-1? Usually it has $n-p-1$ degrees of freedom when a constant is included in the regression and there are $n$ data values. That leaves me wondering whether I have correctly understood your abbreviations.',), ("Oops! I'm so used to using SSR for Regression Sum of Squares and SSE for Error Sum of Squares that I forgot that these are not standard abbreviations.",), ('About the intuition behind question #2: I am wondering why knowing $\\\\sigma$ ought to change the df at all. Suppose you knew *all* the parameters of the problem: $\\\\beta$ and $\\\\sigma$. Obviously that does not change SSE, SSR, or SST (unless you used them to compute $\\\\hat{y}_i$ and $\\\\bar{y}$, but that would change their definitions). So how could *your* knowledge of the true parameters change the *sampling distribution* of the statistics (could this be a new form of ESP? :-)?',), ('How does time relate to your stated model? Is the $x$ variable time or is time yet another variable?',), ('@Nick Cox Time is the independent variable. The model should read $y=at^b + c$',), ('Looking at R-squared and your auto-correlated residuals, it seems to me that you are having here what is called as "spurious regression". These are more discussed in detail under non-stationary or unit root process.',), ('ANOVA and linear regression using dummies are identical, with identical assumptions.',), ('Is the extra structure scientifically or practically interesting or important? From the high $R^2$ you have it is presumably a smaller part of the structure than the overall shape. That might drive a decision to regard it as notable but not worth including in the model. On the other hand, approximate sinusoids often correspond to some overall intelligible periodicity (e.g. seasonality) which in turn may suggest a modification to your model.',), ("The extent to which this could be described as an experiment is an important detail. Even taking gender as given, area is standing proxy for an enormous number of possible influences including climate, nutrition, standard of living, age and race composition. (You don't say which country this is, but the bigger the country, the more the problems. I guess the US.) I think @Russell S. Pierce is hinting in this direction too.",), ("What's a 'predictory' variable? From your text, it actually sounds like you mean 'response variable'",), ('yes ! response variable or independent variable . I am editing it . thanks',), ('Whoah. Response variable = dependent variable = y-variable. Independent variable = explanatory variable = predictor variable = x-variable. Which is it?',), ('please edit my question ! i am not comfortable with statistical terms . Thanks',), ("If I could tell for certain which kind of variable you meant where, I would do so. Consider the regression model $y = \\\\beta_0 + \\\\beta_1 x_1 + \\\\beta_2 x_2 + \\\\varepsilon$. The first variable, $y$, is the dependent variable, the response. The second and third variables, $x_1$ and $x_2$ are predictors, explanatory variables, independent variables. As it is right now, it looks like it's right, but you must check whether it says what you mean.",), ('Thanks Glen_b, I am delighted with the learning of types of variables in regression models andthe answer given below by Maaten buis made me clear the concept.',), ('Another terminology you might come across is "regressand" for the dependend variable (the $y$) and "regressor" for the independend (the $x$)',), ('@bioinformatician Here are lists of terms that may help you. Let\'s start with synonyms for "dependent variable" = "explained variable", "predictand", "regressand", "response", "endogenous", "outcome", "controlled variable". Next are some synonyms for "explanatory variable" = "independent variable", "predictor", "regressor", "stimulus", "exogenous", "covariate", "control variable". Some of these terms are more popular than others across different disciplines.',), ('@Graeme Walsh This is Awesome ! I am  very happy today as i got to learn a lot that i could not do so in past couple of days.',), ("Is there are any particular reason you're suppressing the intercept by including `+ 0` in the formula?",), ('What do you consider "really weird coefficients"? `*` means interaction plus main effects in formula syntax. Indeed factors are not something that can be multiplied. Think about what a factor actually is.',), ('This seems to be less a coding problem and more a conceptual problem. You need to clarify the statistical questions first and then get a handle on how to interpret interaction models... and looking at coefficients is NOT how one goes about accomplishing that task. These are issues that should be handled in a regression course.  SO is not set up to repair major gaps in your statistical education.',), ('Ok, so this is a regression model that I am following from a paper. It is not one that I came up with. The reason that time is being put into factor is there are several observations per day and to minimize the dataset, the series is put into fifteen minute intervals, afterwhich assessing time as a dummy variable - if the observation falls into that interval - tells you about the intraday seasonality for the day. Putting the firm into dummy variables tells you the firm effects and the influence it has on the model but you want to know how the firm interacts with time, hence the product.',), ('I have no idea what you\'re asking. You want to fit a non-monotonic function of $x$... what exactly is your problem with polynomial regression or sine regression again?? Also... "link function"... you keep using that word... I do not think it means what you think it means.',), ('(1) Your `R` code has syntax errors: `group` should not be quoted. (2) The plot is beautiful: the red dots exhibit a linear relationship while the black ones could be fit in several ways, including a piecewise linear regression (obtained with a changepoint model) and possibly even as an exponential. I am *not* recommending these, however, because modeling choices ought to be informed by an understanding of what produced the data and motivated by theories in relevant disciplines. They might be a better start for your research.',), ('@whuber thanks! Fixed the code. Regarding theoretical motivation: where do these come from in the first place? My bench scientist collaborators will happily dichotomize the predictor variables and do t-tests on them. So it falls to me find a way to stop wasting data by finding a mathematical relationship that captures the transition from "y correlates positively with x" to "y has little response to x" to "y correlates negatively with x". Failing that, I\'ll have to recapitulate what, e.g., Michaelis and Menten did when they found a relationship between enzyme, substrate, and product.',), ("Are the points where those things 'kink' known in advance?",), ('(previous comment continued) so another way of putting my question is: what should I learn in order to be able to go from seeing a pattern in the data to a parametric model I can fit to future data and measure the effect of experimental variables on various parameters? What is the workflow from the exploratory phase (that apparently my subfield is largely in) to the hypothesis testing phase?',), ("@Glen_b no, the 'kink' points aren't known, and might not exist-- it might be some sort of continuous function. And if the kink-points do exist, there is no evidence that they would be the same under different treatments.",), ("@JakeWestfall my problem with polynomial regression: `plot(lm(y~poly(x),updown))`, i.e. not everything that goes up and then down is a polynomial. Sine regression also gives odd residuals, but at least they're symmetrically distributed. My bigger problem with sine regression is that for non-cyclical data, it will make completely wrong predictions. And, if the data had ranged over a wider area of low response beyond the peak, the fitted curve would be a series of sine waves instead of a unimodal function!",), ('+1 for the provocative title and a followup that actually makes sense',), ("I don't really understand the problem. I am most familiar with your crop yield/water example. There are (often quite complex) models available that are based on micro-meterology, soil physics, plant physiology, nutrient supply (fertilization), and so forth. You find them in the literature. If you fit an empirical model it can only approximate such a complex model over a limited range. It may be that a linear model is appropriate to answer your specific question, it may be that you need to be able to model saturation (use an appropriate non-linear function).",), ('What are you trying to do or show exactly? The estimated influence? The significance?',), ("Yes, I'm familiar with t- and F-tests. I'd like to show estimated influence, for which afaik t- and F-tests are not suitable.",), ("I don't understand yet what you want to achieve. So you are trying to find the R² of a restricted model with one less variable, correct? But I am not sure why you would use this to quantify the influence of the variable. The influence on the response is your parameter estimate, your partial R² would be the cp reduction of residual sum, yes? But that would only work as a metric if there is no multicollinearity in the data, and there surely is. That is probably the reason why the variance of your variables does not relate to the reduction in residual squares",), ("My question is: Should I use partial R² or the coefficients to show how much influence each factor has on the outcome? I was assuming both to point in the same direction. You are saying that's not true because there is multicollinearity in the data. Alright, so when I want to make a statement such as factor 'young' influences the result x times more/is x times more important than factor 'urban', do I look at partial R² or coefficients?",), ('I am going to word my opinions on this as an answer',), ("How would the regression output change if you were, say, to add $10^6$ to each `pop` value and add $-0.0116584\\\\times 10^6$ to each `fuel` value? Intuitively, that shifts the data far from `pop=1029` without altering the regression line and therefore should result in a much wider prediction interval. That means you can focus your research on those elements of the output that change.  (Even if you don't have the actual data you can make some up and run both regressions to see what happens.)",), ('Thanks very much! Only the standard error of the intercept (therefore t, p-value and CI) changes. This inspired me to figure out that $Var(\\\\hat{\\\\beta}_0)=\\\\sigma^2(1/n+\\\\bar{x}^2/SXX)$, then I can get $\\\\bar{x}$ to calculate the standard error of prediction.',), ("The standard error of a *predicted value* isn't what you said. What you have there is the standard error for the mean at a given $x$.",), ('Sorry I just followed the description of the option `stdp` in `Stata`. It can be thought of as the standard error of the predicted expected value, mean or the fitted value.',), ('What do you mean by "corr(i, j) = 0:5 for $i−j$ or for all $i=j$"?',), ('There are 8 elements in your beta vector, do you want the intercept to be 0, or are you calling the intercept one of your 8 predictors?',), ('Well, I mean that let the predictors be correlated at $p$=0.5. Thanks',), ('Mr. gung,\nAssume that $b$0=0.8 or whatever. Thank you so much',), ('So, do you mean that ${\\\\rm corr}(x_i, x_j) = 0.5$ for $i \\\\neq j$, and one should drop the "$i = j$" part at the end of the sentence? Could you please amend the question to clarify the set-up?',), ('I am sorry Mr.QuantIbex, ignore the $i≠j$ part.',), ('Predictors with mutual correlations of $1/2$ are readily obtained as `matrix(rnorm(8*n), ncol=8) %*% chol(matrix(1/2, 8, 8) + diag(rep(1/2, 8)))`, but what role does "$\\\\beta$" play?',), ('Thank you so much dear whuber, well they are just arbitrary values for the coefficients.',), ('Still not clear to me what the constraints on the predictors should be. It would help us help you if you could edit the question based on the clarifications you had to provide in the comments.',), ('Amongst the 8 predictors, 5 have as coefficient equal to zero ($\\\\beta_3 = \\\\beta_4 = \\\\beta_6 = \\\\beta_7 = \\\\beta_8 = 0$). So why do you want 8 predictors and not 3?',), ('Well, Mr. QuantIbex, after that I am using Lasso and I want to know whether Lasso is picking the non-zero coefficients or not? I hope it would be clear. Thank you so much',), ('Hint: what is the expectation of a constant?',), ('This question appears to be off-topic because it is about a statistical method and thus belongs on Cross Validated.',), ("If A, B and C are separate groups (not levels of a continuous variable) then you can't really draw lines between them, or place them equidistant on the x-axis.",), ('Could you add more details about your model? That would help us to answer the question. Here are some comments: Normally, you would just exponentiate the regression coefficient, so just $\\\\exp{(\\\\beta)}$. If the coefficient is negative, $\\\\exp{(\\\\beta)}<1$ and if the coefficient is positive, then $\\\\exp(\\\\beta)>1$. I think the interpretation is like this: the exponentiated coefficient is the multiplicative term to use to calculate the estimated dependent variable when the independent variable increases by 1 unit. In this case, the multiplicative term is $0.945$. See also [here](http://goo.gl/SJPWh).',), ('Thanks @Glen_b for the clarification. I will delete my comment and wait until the OP provides additional information about his goals. How would one calculate the mean?',), ("@COOLSerdash - I think it should stay; it's a relevant comment, it just needs some qualification.",), ("george, you can comment on your preceding question if you need further clarification or if you want to add details. You could then vote to reopen your post rather than reposting the same question. Please, tell us what's not clear with the possible duplicate, and we'll merge this question with your other one.",), ("@COOLSerdash Sorrt, somehow I missed the question about calculating the mean. If it's normal on the log scale, then conditioning on knowing the parameter values, you'd be computing the mean of a lognormal ($\\\\exp(\\\\mu + \\\\frac{1}{2} \\\\sigma^2)$). If you don't condition on at least the variance-parameter, the exponentiated estimate is instead log-t ... and then it doesn't have a mean.",), ("Thanks a lot @Glen_b. That's very interesting. Coming from epidemiology, I've never thought about Poisson regression in that way.",), ("@COOLSerdash Sorry, I must have missed something. My comments weren't intended to apply to Poisson regression, only to the case where you take logs and then fit a linear regression (thereby, I was thinking, assuming normality on the log-scale, at least of the coefficient-estimates). That action's not really appropriate for the Poisson case. Did I misunderstand?",), ('@Glen_b Ah now your comments make perfect sense to me! I thought by "log linear model" the OP meant Poisson regression rather than a linear regression with a log-transformed outcome (I think Poisson regression is sometimes called log-linear model). But re-reading the question I think you are perfectly right in assuming that the OP meant a linear regression with log-transformed outcomes (otherwise he wouldn\'t mention that the outcome was log-transformed, I guess).',), ('@COOLSerdash Yeah, I agree that normally statisticians would use log-linear model refer to a model whose linear predictor has a log-link (which is natural in the Poisson regression case), but as you note, the question says "where the dependent variable is logged", clearly suggesting modelling $\\\\log(y) = \\\\alpha + \\\\beta x+\\\\varepsilon$. Needless to say, I *don\'t* think it\'s a duplicate of a Poisson regression question, which would model $\\\\log(\\\\text{E}(y))$ as linear in $x$, not $\\\\text{E}(\\\\log(y))$.',), ('@Glen_b I totally agree and voted for reopening.',), ("They're both testing independence against dependence, but in different ways. DW explicitly tests lag 1 autocorrelation, the runs test is somewhat sensitive to that, but also responds to other forms of dependence. It's a bit like comparing a Jarque Bera test against a Shapiro-Francia test when you're interested in normality; they both are tests of normality against non-normality, but they respond differently to different aspects of non-normality. If you're *specifically* interested in what a more specific test looks for, that's a good thing, otherwise it might not be.",), ('Thanks, @Glen_b! Is test for non-correlatedness viewed as a kind of test of randomness?',), ("While it's not usually referred to that way, obviously it is the case - clearly if its rejected the data are dependent.",), ('if $X = USD / SDR$  then the ln approach should give the expected result. The only flaw I can see if you have took the inverse exchange rate...',), ('X would be a number of 60 different currencies, expressed X/SDR. Is that what you mean? The correlation should be pretty much the same, no matter if take absoulut values or relative, right?',), ("As a simple example where you have one explanatory and a dependent variable: A linear regression of the *ranks* of $x$ and $y$ would yield Spearman's correlation coefficient as regression coefficient.  And in this case, $x$ and $y$ are interchangeable in the regression.",), ("Just a few thoughts. Kendall's $\\\\tau$ and Spearman's $\\\\rho$ are both correlation coefficients based on ranks. The sought after relationship between $x$ and $y$ would then need to involve their ranks. However, computing the ranks introduces dependence between the observations, which in turn imposes dependence between the error terms,  eliminating linear regression. However, in a different setting, modeling the dependence structure between $x$ and $y$ with copulas would make a link with Kendall's $\\\\tau$ and/or Spearman's $\\\\rho$ possible, depending on the choice of copula.",), ('The following paper by Ruppert (2002) might give you some help. D. Ruppert. Selecting the number of knots for penalized splines. Journal of Computational and Graphical Statistics, 11:735–757, 2002.',), ("Although written in a different context, it may help you to read my answer here: [Difference between logit and probit models](http://stats.stackexchange.com/questions/20523//30909#30909), which contains a lot of information about what's happening in logistic regression that may help you understand these better.",), ('But if you do not include other determinants for quality of life, for which there are a lot, then your statistical procedure is not even asymptotically correct? Turning this around - without all determinants known you can not simulate (ie. forecast) new datapoints.',), ('thanks ima for the feedback...randomisation is expected to make groups reasonably comparable',), ('@bgbgh you can\'t estimate the confidence set by estimating the confidence intervals individually, because it does not yield any "global" information: just imagine you were measuring something that is actually a ball. With only measurements along axes, you might claim that you were measuring a cube, which is incorrect.',), ('The equation for a logistic regression is $y=\\\\ln \\\\left(\\\\frac{p}{1-p}\\\\right)=\\\\mathbf{XB}+\\\\epsilon$. What you have written is not a regression.',), ("That's not correct; the equation for a logistic regression is $\\\\ln(\\\\frac{p}{1-p})=\\\\bf X\\\\boldsymbol\\\\beta$. There is no error term.",), ("If this is for *self study*, you need to add the `[self-study]` tag, & tell us what you understand at present & where you're stuck.",), ('I edited the question so that it hopefully gets to the actual question the user is asking.',), ('Thanks for editing it. Equation is correct by the way. I was expecting different answers. One answer would be If one fits a linear reg instead then predicted response might be <0 or >1 which is clearly nonsensical as a response probability. I want to know your understanding to this. Is there any other difference?',), ('This model is often called the linear probability model and is much discussed e.g. in econometrics. It has many detractors, not least for the reason you cite, that it can return predictions outside the allowed range, and some defenders, not least because it sometimes fits about as well as other models more acceptable in principle.',), ('Concerning whether there is any other difference, does there need to be to make you lose interest in fitting a linear model to binary $Y$?  But there are several other differences such as the interpretation of $\\\\beta$ and the need to put restrictions on $\\\\beta$ (and to add complicated interactions that are difficult to interpret) to force the output be in $[0,1]$.',), ('What distinction are you making between "general linear model" & "multivariate linear regression model"?',), ("@Scortchi: general linear model doesn't assume anything on the errors, except their expectations are zero. I am not sure about how people generally assume on multivariate linear regression model?",), ("If you're not the one making a distinction, what reason to suppose there is one? I'd just call the latter a special case of the former ($q>1$). And note that often enough general linear models come with several more assumptions than a conditional expectation of zero.",), ('@semibruin : My question is not about selecting the number of knots but the knots themselves. I would like an estimation procedure which would give me the points in the data where the slope of the line is changing.',), ('http://r.789695.n4.nabble.com/Partial-R-square-in-multiple-linear-regression-td4632130.html',), ('Can you clarify what you mean here? Eg, what do you want to do w/ this information when you get it? Are you wondering how to conduct the R-squared change *test*? Do you want to know the variance in `var` marginally associated w/ each `VAR`, or partially associated? etc.',), ("When you do `as.numeric()` to a factor, it doesn't always convert it back to the original numeric values the factor labels are comprised of. For example, type `as.numeric( as.factor(runif(10)<.5) )`. You'll see a string of `1`s and `2`s, not `0`s and `1`s. That's what happened here. BTW, you may want to reconsider using OLS regression for a binary outcome. (BTW: I didn't post this as an answer because I think this question is off topic. This is purely a programming issue.)",), ("@Patrick: it's in the intercept, is that relevant here?",), ("@Macro: Thank you for the explanation! That is some strange behaviour, `0` and `1` are are more intuitive. I didn't know it is a programming issue so it was safer to post it here I think.",), ("@KarolPrzybylak, Fair enough. Glad I could help, anyway. Since your issue is resolved (and it turned out not to be statistical) this may be a situation where it makes sense to delete the question and  possibly repost it over on stackoverflow, if you think the question and answer will be helpful to the community over there. Of course, it's up to you. Cheers.",), ('@Macro: Sorry but I confused something. When I type `typeof(learn_set[,"learn_IV"])` I get `integer`, so it\'s not like I\'m doing `as.numeric()` to a factor. I will repost it later, thanks for the advice.',), ('I want to get the contribution of each variables VAR1, VAR2, VAR3, etc to explain var.',), ('@Karol, I\'m not sure what the function `typeof` is doing but it seems to always call factors "integers" and so does not preclude my solution. Based on the output you\'re seeing, I\'m sure what I pointed out was the culprit.',), ('Look up (1) Flexible linear regression, (2) Kalman Filters.',), ("You seem to be confounding errors and residuals together there. The assumptions apply to the errors, but the diagnostics are done on the available estimates of them (the residuals). You're interested in checking whether the entire set of errors are consistent with being sample from $N(0,\\\\sigma^2)$",), ('The issue with errors versus residuals aside, I think you also need to ask yourself why does the QQ plot tell us that the residuals follow a normal distribution. Think about what it means to be on the straight line in the plot.',), ('I think you should be using NB or SVM instead.',), ('One possibility is to choose all the parts of your model (such as say linear regression with student t errors) and write down the log-likelihood function and simply maximize that.',), ('What about some form of robust regression?',), ('@Glen_b That would work. Is there a reasonable fat tailed error distribution that results in a closed form solution? If not, is there any off the shelf software that can do this estimation or do I have to code it from scratch?',), ('Well, obviously anything that solves the general problem will need some level of input - chances are even very nice software doesn\'t know your density until you tell it, so there\'s always going to be some level of \'programming\'. But in R for example, there\'s functions to do ML estimation if you supply the functions you want optimized. On the other hand, if you specifically want "linear regression with t-errors" and are willing to use R, there\'s the function `tlm` in the package `hett` on CRAN. Or you could do something like M-estimation, or robust regression more generally',), ("Thanks. And yes, robust regression is exactly what I need, but the variety of techniques in this category is so great. In particular, it's hard for me to gauge whether I should go with M-estimation versus MLE with a specific distribution etc.",), ("The answers can be found at http://stats.stackexchange.com/questions/9131, http://stats.stackexchange.com/questions/44845, and http://stats.stackexchange.com/questions/16493 (even though they deal with prediction intervals). If that's not enough, more would likely turn up in a search for [regression prediction interval](http://stats.stackexchange.com/search?tab=votes&q=prediction%20interval%20regression). As far as the second part goes, it comes down to the difference between a standard error and a standard deviation: the standard errors vary with values of the IVs but the SDs do not.",), ("I'm pretty sure this has a duplicate somewhere. Ah, found it. Not quite an *exact* duplicate, but the approach [here](http://stats.stackexchange.com/questions/44246/nls-curve-fitting-of-nested-shared-parameters/44249#44249) should work",), ("Ok, If I do that approach I can combine all my data into a single fit. I still don't know how to determine whether the coefficients are different.",), ("There's a couple of ways that should work. Oh, actually, rather than try to type it all in a comment, see for example, pages 16-17 [here](http://stat.ethz.ch/wbl/nlreg). There are other tests, but hopefully that will assist you enough.",), ("If I'm understanding you correctly, your line shouldn't be a worse fit at the ends (at least not as much as this).",), ('Could you add a plot of your regression against the original data? (by the way, you can also directly use `model$residuals` or `resid(model)` to find the residuals)',), ('[This post](http://stats.stackexchange.com/questions/18606/does-it-make-sense-to-study-plots-of-residuals-with-respect-to-the-dependent-var) discusses the question, if it makes sense to study the residuals vs. the original variable (outcome).',), ("I am not sure what is on those pages that is relevant to what I'm trying to assess, but on page 13 and 14 there is an example using the puromycin dataset that is helpful. I can add one or two explicit difference variables to the model \n\n`V~(Vm+deltaVm1.2*indicator2+deltaVm1.3*indicator3+S)*S/(Km+deltaKm1.2*indicator2+deltaKm1.3*indicator3S)`\n\nand use dummy coding to determine the difference that going from prep 1 to prep 2 and prep 1 to prep 3 produces. But I'm still missing the comparison from prep 2 to prep 3.",), ('You can either (i) fit two models, one where prep2 and prep3 have the same dummies, and one where they have different dummies and compare them using an F test via the information I indicated, or (ii) reparameterize so that you have a single dummy representing the contrast (that difference in the two preparations) and test it via a t-test or F-test. (Well there are other ways but those should work okay). Failing that, if you post some data I could see if I could explain how to do it using your data as an example.',), ("This answers the question! Don't think the site will let me accept a comment as answer though.",), ("Since the previous answer I first pointed to didn't solve it, I will expand a little on the above and post it as an answer.",), ("On second thoughts, Yashka, I hadn't picked up before that you were actually doing a mixed model. I'm not sure that I understand enough about nonlinear mixed modelling to write a reasonably intelligent answer. So I'll leave it alone now that I get that. Hopefully someone with more experience with nlme's will respond.",), ('violate in what way?',), ('Heteroskedasticity is present, no linear relationship (partial residual plot), and non normal error terms.',), ('Why would you consider least squares then, rather than say a GLM?',), ('because my response variable is continuous.',), ('So... kind of like all the neatly available **continuous**, non-linear, non-normal, heteroskedastic GLM models then? Gamma, Inverse Gaussian, Tweedie, ...',), ('"Normalization" typically means changing values to fall within a given range such as $0$ to $1$ or $0$% to $100$%.  You describe *standardization,* which consists of a linear transformation making the mean equal to zero and the standard deviation equal to unity.',), ('It is very unclear why it is worth more than a few seconds of your time to deal with standardization.',), ("Aren't you integrating over $\\\\mathbb{R}^p$?",), ('@Stijn Yes, thank you for pointing out this typo.',), ("The trick is to rewrite $(y-X\\\\beta)^T\\\\Sigma^{-1}(y-X\\\\beta)$ as  $(y-X\\\\hat\\\\beta+X\\\\hat\\\\beta-X\\\\beta)^T\\\\Sigma^{-1}(y-X\\\\hat\\\\beta+X\\\\hat\\\\beta-X\\\\beta)$ and to then rearrange terms. I'll show you how if you still have not figured it out later.",), ('@BabakP Thanks. I will work on those lines.',), ("If you fit a model of a straight line with constant gaussian error to a bounded variable, obviously this is going to happen. The correct way to deal with it is not to fit a model you know to be wrong, but one that's at least plausible. If y's can only be positive, you don't fit a model that says they can be negative.",), ("I am not familiar with python coding but could you use the formula $$\\\\hat\\\\beta=(X'X)^{-1}X'y$$ in python?",), ('Are you asking for how one would go about writing an OLS function, or are you looking for one (which has been answered below)?',), ('Alternatively, you just expand out $(y-X\\\\beta)^T\\\\Sigma^{-1}(y-X\\\\beta)$ as $y^T\\\\Sigma^{-1}y -y^T\\\\Sigma^{-1}X\\\\beta-(X\\\\beta)^T\\\\Sigma^{-1}y+(X\\\\beta)^T\\\\Sigma^{-1}X\\\\beta)$ then simplify and complete the square in $\\\\beta$, which then gives you result of the form $(\\\\beta-\\\\hat\\\\beta)^T\\\\text{<something>}(\\\\beta-\\\\hat\\\\beta) + S$ where $S$ doesn\'t contain $\\\\beta$, and where the form of $\\\\hat\\\\beta$ and "<something>" are obvious by inspection of the expanded terms',), ("@BabakP I don't think explicitly inverting $X'X$ comes remotely close to either 'best' or 'efficient' (as asked in the question). The two requirements are somewhat contradictory ('best' presumably including numerical accuracy/stability, which costs some efficiency and vice-versa) so some tradeoffs are involved, but if you invert $X'X$, you're not even close to the 'best/efficient' *efficient frontier*. Using something along the lines of a QR decomposition of $X$ is the standard approach... for generally good reasons.",), ("Sure, but I didn't think I had to lay out all the steps.  If their taking the inverse I would hope they knew better than to try taking an inverse directly.",), ('You cannot quantify the power until you have proposed a specific and quantitative alternative model.  *All* discussions of power occur (necessarily) within such a context.  Unfortunately, "correlated with $x$ by $r$" is too indeterminate to be of use.  For instance, $x^{*}$ could be $y$ itself!',), ('If you want your account deleted just flag the question and leave a comment to the moderator to delete your account....',), ('The answer at http://stats.stackexchange.com/a/1448 includes an illustration showing this result is merely the Pythagorean Theorem.',), ('This reads like a standard textbook question. In what context does this question arise (i.e. what are you doing that it would lead you to ask this?)',), ('"*I am told both regression coefficients cannot exceed 1.*" -- don\'t believe everything you\'re told. Were they perhaps talking about standardized coefficients?',), ('The book I am studying says "But this is not possible because both the regression coefficients are greater than 1. Hence, we will now treat eq.1 as Y on X and eq.2 as X on Y. "',), ("This seems rather strange. Which book? Are you *sure* they're not talking about standardized coefficients?",), ("Introductory Statistical Methods by S.P. Gupta (not the most popular of textbooks). I don't know what you mean by standardized coeffs. If that implies the Y-intercept on the lines is 0, it isn't so.",), ("Either the book is wrong, or you've misunderstood it. Post the _complete_ text relating to your question.",), ("I've updated the question, with better context.",), ("I'm guessing the rule arises from constraints on the correlation coefficient, which is the square root of the product of the two regression coefficients.",), ('Thank you so much everyone this really helped me out (to Comp_Warrior for editing it and for those who answered). This was a question in the lecture notes I had and I tried for about an hour to get it but because the textbook I use focuses more on the estimates and not true value of a regression I was getting confused. Thanks again.',), ("Can you give more of the context from this book? How would a rule about correlation imply a constraint on one slope coefficient? Certainly, if one regression coefficient is above 1 the coefficient of the regression with predictor and response interchanged will be smaller than 1, but if you're talking about least squares regression (rather than population calculations involving conditional means), the assumptions of at least one of the two regressions will be violated. Perhaps you could edit to clarify your question further.",), ('What is the motivation for such a question?',), ("Are you summing over all reorderings?  Even computing the sum would be hard when n is non-trivial. But if it's over all reorderings I guess it could be possible to find some fast method for computing the sum.  Similar to the sum over all subsets method in conditional logistic regression, described e.g. here: http://hydra.usc.edu/pm518b/materials%202012/2012-09-27%20recursive%20algorithm.pdf",), ('I can\'t even understand the question because the notation is self-contradictory: expressions like "$a\\\\in x_i$ imply $x_i$ is a *set,* not a vector; and $\\\\beta f_i(x_i)$ is not defined because both $\\\\beta$ and $f_i(x_i)$ are suppose to be elements of $\\\\mathbb{R}^n$, which does not have a multiplication except for $n=1,2,4,8$. Do you think you could express your problem in *words* so that we don\'t have to figure out unconventional uses of mathematical expressions?',), ("whuber: sorry about the nonstandard notation. The multiplication of $\\\\beta$ and $f_i(x_i)$ is supposed to be an inner product. I've edited it to be more clear.",), ('Thanks, it\'s clearer.  I still wonder about one thing, though: in your example you use $3$-vectors $(a,b,c)$, whence $n=3$, but you list $6$ of the $f_i$, whence $n=6$. How is this contradiction to be resolved? I am also confused by what you are summing over: $i$ is the index, but now (with two different values of $n$) the term "$f_i(x_i)$" makes no sense.',), ('whuber: Sorry again, I made a huge mistake with the indexes. Hopefully the edit clarifies this issue. Concerning vector lengths, if $x_i$ is a length 3 vector, then $f_i(x_i)$ is also a length 3 vector, not length 6. My notation was meant to indicate that its output is one of the 6 possible permutations of $x_i$.',), ('How large can $m$ be? There is a simple efficient solution for small $m$ (less than $7$ or so).',), ("@whuber: I don't think m will be larger than 7.",), ('Do you mean "[coefficient of determination](http://en.wikipedia.org/wiki/Coefficient_of_determination)" for "correlation", & "[multiple regression](http://en.wikipedia.org/wiki/Multiple_regression)" for "multivariate regression"?',), ("Yes.  (My bad on $r^2$ but isn't multiple regression the same as multivariate regression?)",), ('No; multivariate regression means a multiple response (target or outcome or dependent variable). Having multiple predictors (independent variables) does not itself make a regression multivariate.',), ('Your question is puzzling. If the predictor in bivariate regression has been standardised, then its coefficient equals the correlation: this is in essence an inevitable consequence. If not, then not in general. The way to think of this is in terms of units of measurement or dimensional analysis. A correlation, and hence its square, has no units, but a regression coefficient has units (units of y)/(units of x). Standardising washes out both units and leaves you with dimensionless numbers.',), ("Ok so you are saying it's not possible, with standardized data, to have large $\\\\beta$ and small $r^2$, because they are both the same thing - a measure of effect size.  $\\\\sigma_\\\\beta$ meanwhile tells me the significance.  Thanks btw, learning a few things here :)",), ('No; as already pointed out correlation is $r$ and so $r^2$ is different in general (except for $r$ of 0 and 1). And no; the slope has nothing to do with significance. This should be easy to see in your favourite software. Standardise variables, do a regression, look at the correlation, look at the slope. Square the correlation.',), ("Just tried this - and it's not working for me - please see my edit",), ("I can't see what you are doing wrong. If you can post your data, we can check. Otherwise see my answer.",), ('This is not true "Note that to have a solution $X^TX$ must be invertible."  In order to have  UNIQUE solution that must be true but if it is not then you can have an infinity of solutions.',), ('Usually, when people use transformations to improve characteristics of the fit (like the normality of the residuals), the transformation is applied just to the LHS / $y$. Of course you *can* transform the RHS too, but that typically addresses different issues, such as curvature in the data that the model is missing. However, your non-linear model ought to be flexible enough to capture whatever curvature is there, I would think. Note also, that there is no reason the *same* transformation needs to be used on both sides.',), ("You can't really assess the suitability of the regression assumptions from this output; try [`plot(reg1)`](http://stats.stackexchange.com/a/65864/805) (that's a clickable link, by the way) as a first step. As for what a p-value means, try [this](http://stats.stackexchange.com/questions/14928/meaning-of-p-values-in-regression) (the first sentence [here](http://en.wikipedia.org/wiki/P_value) has a basic definition).",), ("(Note that the p-value at the bottom is the p-value of the null that all the coefficients - bar the intercept - are zero.) For a clear illustration of the danger of just using regression output like the above, try working your way line by line through the example code at the bottom of [`?anscombe`](http://stat.ethz.ch/R-manual/R-devel/library/datasets/html/anscombe.html) (i.e. just cut and paste one line at a time into R and see what it's doing).",), ('An alternative is to revisit the model.  Do you perhaps need another additive term, say $(a x_1^C + b x_2^d) x_3^e + f$ ? The transformation you have maps $(0,0,0)$ to 0; but you say that $y$ is at least 10...',), ("Sorry, that's a typo. It should range starting from 0.",), ("I guess my other question is, given my nlsResiduals plot looks the way it does, should I be using a transformation? I'm not interested in confidence intervals (which, I think normally dist'd residuals are required for), but just the coefficients for the equation to use as they are.",), ('I think you are supposed to propagate the uncertainty in the fitted values of a and b (i.e., a-hat and b-hat), not the parameters a and b (which have no uncertainty) and the random error term from the formal model.',), ('Could you please clarify what you mean by "total uncertainty"? Are you referring to some measure of variation in $y_i$ or in the estimated parameters $\\\\hat{a}$ and $\\\\hat{b}$? What measure of variation are you using?',), ('the middle term should be $e^{Bz}$',), ('Have you tried simply checking the correlation between the variables? Not meeting assumptions usually reduces power, but if you have statistically significant results, that might be a good start.',), ('You may want to read these threads: [pearsons-or-spearmans-correlation-with-non-normal-data](http://stats.stackexchange.com/questions/3730/), & [is-normality-testing-essentially-useless](http://stats.stackexchange.com/questions/2492/).',), ('Could you show us your data? With such few points it very hard to test for normality (to illustrate this generate 18 numbers from normal distribution, does the data look normally distributed? probably not).',), ('Welcome to the site, @pontikos. This is not an answer to the OP\'s question, it is a comment. Please only use the "Your Answer" field to provide answers. I recognize it\'s frustrating, but you will be able to comment anywhere when your reputation >50. Since you are new here, you may want to read our [about page](http://stats.stackexchange.com/about), which contains information for new users.',), ('Boris, it is not necessary for *either* variable to have a normal distribution and therefore a test of normality would be of little use. What matters most is that on a scatterplot the data appear to trend along a line and there are no extraordinary ("outlying") excursions from that line. A secondary issue is that the scatter of the data around the line should be approximately cigar-shaped or (American) football-shaped. All these can be checked simply by looking at the scatterplot.',), ('Thanks for the answers! \nI will take a look at the Doornik-Hansen test, the scatterplot and the normality plot. But will mostly consider the scatterplot.\n@pontikos I will come back with the data as soon as I have them!',), ('I know you say that you do not want to use matrices, but have you tried solving the matrix equations?  If you do, you should arrive at the conclusion you are looking for.',), ('@BabakP it is not that I do not want to, but my university stopped using matrices for a while now so our courses do not teach it.',), ('If that is the case, will you accept an answer with matrices?',), ('@BabakP I am sorry but that is completely useless to me, I do not understand it... (I have not learned how to use matrices to that extent in the field of econometrics). Thanks for trying though.',), ('What are those numbers at the end of your question supposed to mean?',), ('Can you provide some context to clarify your question? I\'m not sure what you are asking about. Eg, you ask about "other polynomials" before introducing the referent polynomial that you don\'t like.',), ('As the others, I am confused. What data have you got? What is the DV and what are the IVs?',), ('Thanks for the questions and reformatting. The values at the end are Y values (spaced in X). They are put into the regression to determine the coefficients for: ax + bx^2 + c. Actual X values are -40, -30, -20, -10, 0, 10, 20, 30, 40 respectively (i.e. in order Y values appear). The X values are telescope focuser positions. Peter - I am not sure about DV and IV. If this is not sufficient information, I will look at DV and IV and get back to you. Thanks to everyone!',), ('Additionally, this is used to determine the optimal focuser position for a telescope system (i.e. using an electronic focuser and camera). Y values are brightness from real images taken at each of the focuser positions (i.e. at each X value). The goal is to find the optimal focuser position which maximizes brightness (e.g. the X location at the peak).',), ('Could you please give an example of such an alternative model?',), ('One alternative is $y=\\\\beta_0+\\\\beta_1x+\\\\beta_2x^2+\\\\varepsilon.$  This is often used for checking linearity. It is almost never proposed as a "perfectly right" or even a realistic model--but by comparing the two models and finding that the data are more likely in the alternative model than the original, we have some evidence that the original is *not* very good (we conclude its fit is bad). Obviously there are many other possible alternatives, depending on *how* the fit could deviate from linearity. (Your question seems to assume the alternative $y=\\\\beta_0+\\\\varepsilon$.)',), ("It depends on the prerequisites for the class. I like the book [_Linear Models with R_  by Julian Faraway](http://www.amazon.com/Linear-Models-Chapman-Statistical-Science/dp/1584884258). This book is very applied and is self-contained and was my - and several of my peers' - introduction to linear models but it may a little tough for undergraduates with little background and comfort with probability/statistics.",), ('I would suggest looking at this post which seems to have gotten quite a bit of attention: http://stats.stackexchange.com/questions/138/resources-for-learning-r',), ("If you have uncertainty in your $x$'s, you generally shouldn't use ordinary linear regression because it's biased (though small uncertainties will result in small bias; maybe you don't care so much). Are the uncertainties in your y's always constant or do they vary? How are you fitting your line?",), ("I'm mainly asking about uncertainty in y's. But I'd be happy if the solution would consider uncertainty in x's as well.\n\nGenerally they aren't constant, but a solution that requires them to be constant would be fine as well, I tried fitting with linearfit, fit, findfit in mathematica and linest (and a custom function that I found that weights values by uncertainties) in excel.",), ('Can you explain as clearly as possible how the "uncertainty" values would be related to what would happen under say a repeat of the experiment? i.e. what, precisely, do these uncertainties represent?',), ('Err for example if y is weight, but the digital scale is only accurate to +-6. (Not the best example with these values, but for example when scale shows 255g it could be 255.0 or 255.9)',), ('Could you explain how that would relate to the first sentence of my question please?',), ("I'm not sure if I really understand you, example would be measuring density, where you would have volume on x axis and weight on y axis.\n\nFast example: Let's say real weights are 11.63, 23.267, 34.9. But our bad scale can only show with +-5 accuracy, so it shows 10, 20, 30. (Volumes are 1*V0, 2*V0, 3*V0). If we try to fit with a program, it will show density of 10/V0 and 0 error. But the real value is 11.7.",), ("What I am asking about is that I am trying to identify your model (in the probabilistic sense) for what the numbers we get (both the observations and the uncertainties) are, in the sense that if the whole experiment were repeated (but with the same $x$'s), what the distribution of the observations might then be.",), ('Let me see if I can rephrase your question (please tell me if I am misinterpreting it).  You have the following equation $$Y=aX+b+\\\\varepsilon$$ and you basically want to calculate the standard deviation of $\\\\varepsilon$?',), ("I think what you're looking for is in my answer here: [How to generate correlated random numbers (given means, variances and degree of correlation)](http://stats.stackexchange.com/questions/38856//38867#38867), which both explains the basic idea conceptually, & includes some custom R code & discussion of the standard function. If there's anything you're asking about that isn't covered, edit your Q to clarify what you still need to understand.",), ('Can you clarify what you mean by *"My data does not show linearity"*? Depending on what you mean, it may (or may not) be a violation of the assumptions of your model.',), ('SPSS makes this *much* harder to do than R or Stata, but a regression spline is one of the best general purpose solutions.  As a side question what made you pay $ for statistical software?',), ("@FrankHarrell As much as I dislike the business strategy of SPSS, I have to frankly say that it's relatively learner-friendly: interface is simple and functions are well categorized. Plus some lecturers/teachers specify the software to go with their class, it does not have to be Lauren's own decision to pick up SPSS.",), ('As I said, if done exactly the same the measurement would be the same. The real weights 11.63g, 23,265g and 34,9g would still be measured as 10g, 20g, 30g.',), ('Upvoted your comment, but experimentalists are even crazier than I thought if this is syntactic sugar for them! Which version is more intuitive.... ANOVA hypothesis test on $\\\\beta$: is the ratio of explained variance to the unexplained variance sufficiently high? T-test on the $\\\\beta$ term of a regression model: is the effect of $\\\\beta$ sufficiently different from zero? And, with the latter formulation you also get the direction of change. And, if you had to transform the data, you can back-transform the parameter estimate into a physically meaningful quantity. Unlike SS.',), ("I would hazard to say that it is learner-friendly only because it is restrictive and doesn't implement best statistical practice.  If it doesn't make regression splines easy then watch out.",), ('So these uncertainties are instead some kind of truncation/bias? Is there any other consistency to their behavior? (say, if I measured an object of say 11,62g would I also be certain to get 10g?)',), ('what are your `vals`?',), ("If you condense the information of seven points to a two-parameter function, it would be mere chance to end up with exactly the same values. I don't know if this already answers your question though.",), ('@Glen_b is that relevant? Those are daily usage rates.',), ("Yes, sometimes what is being modeled can make a big difference to suitable models (with count data turned into proportions, for example). Even without any more detail than 'usage rates' we can guess that the values must be non-negative, for example, which may suggest considering GLMs, perhaps.",), ('That would be a bad fit. Why would you ever do that?',), ('why a bad fit ?',), ('I think what @Memming means to say is that linear regression already supplies *some* kind of fit in the form of $\\\\beta_0 + x_1\\\\beta_1 + \\\\cdots + x_i\\\\beta_i.$ Applying the logistic function to these values is *really* going to screw up that fit, likely (but not invariably) making it worse. You can find out more by searching our site for (high-voted) answers about logistic regression and its interpretation: there are lots of them that should give you the insight you seek.',), ('can you give me a link? please',), ('A recent one is at http://stats.stackexchange.com/questions/69820 (I remember this because I provided an answer :-).  For others, you could begin with a [search on "logistic" and "regression"](http://stats.stackexchange.com/questions/tagged/logistic+regression?sort=votes&pageSize=15), but you\'ll have to wade through many of them because (unfortunately, IMHO) they often have a strong `R` orientation which makes them readable only by `R` aficianados.',), ("I think there's a minus sign missing in the exponent, because the inverse logit function (a.k.a. the logistic function) is $1/(1+e^{-x})$.",), ('Linear regression coefficients may serve as a good initializer for logistic regression, but as @whuber said is right.',), ("It's hard to help with this little info. Maybe explain what your variables are, what you goals are, and post some data if possible.",), ("There's no cut-and dried answer.  If you're serious about learning to handle a variety of situations like this, you'll want to get familiar with the literature on transformations, exploratory data analysis, and ways to use linear regression flexibly (e.g., making use of partial plots; squared or interaction terms; etc.).  Eventually you may want to look into more advanced topics such as regression splines.",), ("Great thanks! That's what I needed to know.",), ("I'm guessing there isn't anything left to this question that hasn't been answered, so we will close this Q (if not, you could always edit it to clarify what you still need to understand & we could re-open it).",), ("To get some insight, it sounds like you are supposing the data $x_i=f(t_i)$ have a particular functional form; namely, one in which the intercepts vary linearly with $t$. Because the intercept is $f(t)-f'(t)t$, we can differentiate it to obtain the slope $\\\\beta$. The derivative is $f'(t)-f''(t)t-f'(t)$ = $-f''(t)t$, implying you believe $f''(t)\\\\approx -\\\\beta/t$ for a range of $t$. Integrating gives $f(t)\\\\approx C_0-C_1t-\\\\beta(t-1)\\\\log(t)$. Is this consistent with how you are thinking about your data?",), ('Just to clarify, are you talking about single items with 4 and 7 response options respective or scales that are the result of taking the sum or mean of a set of items where each item happens to have 4 or 7 response options?',), ('Yes sir I am talking about the same scenario.',), ('which one: individual items or composite scales?',), ('composite scales',), ('No I am not assuming that the sequence of intercepts has any functional form. Local linear regression is non-parametric. Also $y$ is the dependent variable and $x$ is the independent variable, so we are working with $E(y| x = x_i)$ as a function of $x_i$.',), ('Please edit your question to explain your notation and terminology. What are $x_i$ and $y$ and how are they connected? What exactly do you mean by "intercept and slope of the values of $y$ centered at that point"?  What is the "sequence of intercepts"?  A small worked example or illustration might help people understand what you are doing.',), ("I'd start out by writing out what $L^TL$ is in terms of $X$ and $y$, then noting the dimensions of the various elements, and trying to figure out what the determinant is.  If you don't know how to do this latter, there are well-known (ok, well-known to a very, very small fraction of the population) formulae for the determinant of block matrices with various structures that you can find on the web.",), ("@jb I believe this formula has a simple, beautiful geometric description.  The LHS is the squared length of a vector orthogonal to the space spanned by $X$.  Multiplying by the denominator on the RHS therefore gives the squared volume of the hyper-paralleliped spanned by $X$ and $y$. But that's precisely what the numerator of the RHS is, QED. (This generalizes the familiar theorem of Euclidean geometry: the area of a parallelogram is its height times its base. Turned around, it says the squared height equals the squared area divided by the squared base.)",), ('@whuber Interesting interpretation! Thanks!',), ("@whuber - that's pretty cool, I have to agree!",), ('At this link, http://en.wikipedia.org/wiki/Determinant#Block_matrices  you can find why this relation holds, and also what references you should look up in order to be able to prove it.',), ('For future reference, please be aware it is both impolite and against site policy to crosspost simultaneously at different SE sister sites. Choose the best one and, if it later turns out that it will get more or better attention at another site, use the migration facility. Cheers.',), ('Closing this thread as there is an accepted answer on another SE site.',), ('To @whuber: Please google for "Local linear regression". You will find a Wikipedia entry and a number of technical articles which explain the terminology. With reference to the first two articles, the sequence of intercepts is represented by the function $g(s)$, and the slope by \\\\beta. It is clearly stated in the second article that \\\\beta estimates $d g(x)/dx$.',), ('I understand local linear regressions, thank you. What I do not understand--and I suspect few others understand either, given there are no responses to this question so far--is *your particular terminology*. A local linear regression is used as a smoother (or "estimator" of a curve); thus, it does not (in any natural sense) have any "intercepts" at all, nor does it have any "slopes": it fits values to a set of specified independent values. My first comment represented my initial guess concerning what you might possibly mean; evidently I did not read your mind correctly. I still cannot.',), ('Probably I am quite late to ask this but what kind of mathematical expertise are you expecting your undergrads to have? Do your students have prior exposure to Calculus? Probability? Economics? Will this be toward the end of their studies? If you expect to have a lot of Lin. Algebra, Matlab (or Octave) might be more helpful due to cleaner syntax. (Generally speaking, I upvoted Macro\'s suggestion on J.Faraway\'s book but it is a bit of a "shotgun answer".)',), ("There's loess, splines, robust regression...probably other methods too.",), ('You can ensure positive predictions by using a generalized linear model with logarithmic link function. By the way, although your $R^2$ value is quite encouraging, a better check of whether the model follows the main shape of the data is a plot of residual vs predicted. Plots of observed vs predicted may also help illuminate your problem.',), ('@NickCox gave one suggestion. I would plot the data in more ways than just residual vs. predicted. However, you can certainly rescale money variables. One common method is to take log(cost) as the dependent variable. (I think this winds up equivalent to the log link function, but might be easier to comprehend). Log(cost) can, of course, be negative. And logs of money variables are often sensible because, e.g. a difference between 0.01 and 0.02 per click is important, but difference between 1.01 and 10.2 per click is not.',), ('@Peter Flom I think meant 1.02 not 10.2.',), ('Yes, you are being specific, but your specific question is covered by all the answers to the duplicate: you are testing the hypothesis that the intercept equals zero. That is well defined and specific.',), ('A little sample data would help people illustrate potential solutions.',), ('It would help to clarify by defining explicitly and in mathematical terms what $Y$ is. "Payback rate" is a very general term. Is it measured as a ratio of monetary values? Does it count instances of payments-non payments? Is it a weighted average of the behavior of old and new customers? And if yes, what are the weights used? Etc. How one models the RHS has obviously a great deal to do with what exactly does this RHS attempt to explain (the LHS of the regression specification). Otherwise, it would be blind mechanical search for a good fit.',), ('Look into equivalence testing.',), ("You can't *show* that it's 0, since it can be arbitrarily close to zero while being unequal to it. e.g. if $b_1 = 0.0000001$ then $b_1\\\\neq 0$ - and you'd ideally reject that point null, yet with reasonable values for and moderate sample size (and for the disposition of the $x$'s I guess), you can't. Peter's suggestion to consider equivalence testing is a good one (but it's showing something a bit different from what you're asking).",), ('Thanks @PeterFlom, equivalence testing is what I was looking for. If you write it as an answer I will gladly accept it.',), ('Good point.  Y now defined more clearly above.',), ("My reply at http://stats.stackexchange.com/a/13317 addresses the negative part of this question: namely, why $R^2$ does not of itself tell you anything about linearity, especially when the independent variables in the data sets are different.  Although you did not ask it, this question begs for a positive response, too (and perhaps you should mention this explicitly): given that your proposed methods don't do the job, what *does* work for assessing linearity?",), ("Thank you for your answer. I am trying to show how linear the data increases by time, and their difference. If my methods don't work, what else method can I use to address this?",), ('Please edit your question to include the clarification you just made.  It would also help to explain more fully what you mean by "more" or "less" linear and precisely what "difference" you refer to.',), ("I am asking this question because this scenario arises in Genome Wide Association studies, very much in the mode in today's genetics. We have chips for getting genotype info at different spots of DNA over the whole genome, but we don't need all of them, because in every small region there are those highly linked (correlated) spots, thus for every such region we only pick one spot, to save time and money.",), ('The theory is that if one of the spots is related to a disease, then even if we only have info on another spot nearby, we should be able to detect that this region is related to the disease through that other spot which is included in our testing chip. This is intuitive, but not at all rigorous. I tried to find out an explanation in the literature with no avail. I am thinking this could be the topic of an interesting paper. But maybe someone has published something somewhere not so easy to find.',), ('I think there\'s an overlooked distinction between the most up voted answer in the purported duplicate, and the answer sought here.  The "dup" provides an understanding of the p-value, whereas this question seeks an answer that is an example of precise wording of the interpretation of the p-value.  A concise answer, such as that in the comments here might be valuable to some practitioners.',), ('I would add the regression line to them.',), ("I tried to resist but could not.  Have you read Arthur Goldberger's wonderful chapter on micronumerosity?  It's quoted in full in this blog post:  http://davegiles.blogspot.com/2011/09/micronumerosity.html",), ('"*The negative slope indicates that the values are decreasing together negatively*".  If they \'decrease together\' (i.e. one decreases when the other one decreases) they\'d have a *positive slope*. You mean that one *decreases* as the other *increases*, which is the opposite of any sense of \'together\'. Adding \'negatively\' to the end of that doesn\'t serve to make it less confusing.',), ("Your initial comment about resisting commenting seems to suggest that you think my research into ridge and other regularization methods is misguided? I have read about micronumerosity in the course of my research. Unfortunately I can't get any more data for a given day, but I can add data in the bayesian sense by building priors based on previous days observations. Could this be used to address micronumerosity?",), ('The trace is invariant under cyclic permutations.',), ('[$tr(AB)=tr(BA)$](http://en.wikipedia.org/wiki/Trace_%28linear_algebra%29#Trace_of_a_product)',), ('@Glen_b Thank you :)!',), ("No, the point of Goldberger's chapter is that multicollinearity is not a problem to be solved via statistical technique.  Just like micronumerosity is not a problem to be solved via statistical technique.  The standard errors are big because your data don't reveal the thing you are interested in, not because you analyzed them incorrectly.  Of course, you can always make the standard errors smaller by bringing in outside information, but then it's the outside information not the data which are identifying the parameter(s) of interest.",), ('Understood but its not just that the standard errors are large the coefficients themselves tend to unfeasibly large and offsetting. Although these numbers provide the BLUE fit they are pretty much non-nonsensical in terms of real life values there are supposed to represent + I would not trust and prediction made using these extreme values. I do take your point though that the problem would probably disappear if I could just get more data.',), ('I understand now.  When you have multicollinearity, usually you can make pretty precise comparisons and good predictions as long as you make them "with the grain" of the multicollinearity.  For example, suppose that X3 is always pretty close to equal to X4 in the data.  Predictions at points where X3 and X4 are pretty close to equal will usually have pretty low variance and be reasonable.  Similarly, if you want to measure the effect of X3 and X4 rising together by one point, you will get precise estimates.  It\'s only if you try to ask a question the data can\'t answer that there are problems.',), ('To use an example I always use in class, suppose you have a sample of shoe factories with variables for total production cost, number of right shoes, number of left shoes.  You can get excellent, plausible estimates of the extra cost from producing an extra pair of shoes and excellent forecasts of how much a shoe factory producing 10000 pairs of shoes will cost to run.  But, your estimates of how much it would cost to produce 1000 left shoes and 6000 right shoes will be terrible, because nothing like that ever happens in the data.',), ("student; ocram beat me to the punch, with an even more general result (though it's a consequence of the one I mentioned, since you can replace either $A$ or $B$ with arbitrary products and get the result that it's true for cyclic permutations)",), ("Why just the log?  Shouldn't this question apply to any data transformation technique that can be used to minimize the residuals associated with mx+b?",), ('One rule of thumb: use enough points that it looks like a smooth curve when you plot it.',), ('This certainly looks like a homework question. Please add the "self-study" tag. See [homework questions](http://meta.stackexchange.com/questions/10811/how-do-i-ask-and-answer-homework-questions/10812#10812)',), ("@PeterFlom I've added the tag. It's not homework but part of my exam-preparation.",), ('please define $r$, $S_x$ and $S_y$.',), ('Please tell us what $Z,$ $\\\\varepsilon,$ $\\\\hat{\\\\varepsilon},$ and $\\\\sigma$ represent.  In particular, what formulas do you know that express $\\\\hat{\\\\varepsilon}$  in terms of $\\\\varepsilon$ and $Z$?  (In so doing, I suspect you will find the answer to this question yourself.)',), ("@whuber $\\\\hat{y}=Z \\\\hat{\\\\beta} + \\\\hat{\\\\epsilon}$. The epsilons are the error terms and $\\\\sigma^2$ is variance. I still however don't know why this holds...",), ('That\'s because you expressed $\\\\hat{y}$ rather than $\\\\hat{\\\\varepsilon}$ in terms of $Z$.  You might find that searching our site for ["idempotent"](http://stats.stackexchange.com/search?q=idempotent) gives particularly helpful pointers.',), ('If that is a homework, then add a "self-study" tag.',), ('Is $\\\\sigma^2$ assumed to be known?',), ('@Stijn yes ....',), ('I can guess what is going on, but you need to give more information.  The dependent variable is (daily?) surface temp and the independent variable is (daily?) lower troposphere temp for a "rectangle" including the surface station?  Is that true?',), ('Do you mean `scatter y x || lfit y x` ?',), ("This question appears to be off-topic because it is only about how to do something in Stata, not about Statistics. If there's an underlying statistical matter you want help on, please edit the post to make it clear.",), ("@Scortchi and voters on closure: I think there is a statistical question deep inside this as well as a Stata question, so I'd suggest keeping it open. The focus should be: exactly what goes on what axis, and is this a single or a multiple plot?",), ("@NickCox et al. Let's ask Newb. Newb, do you know what you want to graph and just not know how to do it in Stata? Or do you want to know about good graphics for this sort of situation, and then you can figure out the Stata code on your own? Or both?",), ('Could be a duplicate of [this](http://stats.stackexchange.com/questions/73320/how-to-visualize-multi-linear-regression) then.',), ('an ANOVA with continuous predictors would usually  be called ANCOVA (analysis of co-variance). The GLM (general linear model) framework allows for a principled comparison (the difference between modelling and hypothesis testing nonewithstanding)',), ('@Bill\nDependent variable - _Tmax daily_ (Surface Observatory Data).\nFor independent variables, we have daily data for four time periods(5:30, 11:30, 17:30, 23:30). So for _Tmax daily_ we are using reanalysed data from nearest grid point(only) at time 11:30 & 17:30 as independent variables - _Tmax, RH, solar radiation (Long & Short Wave), wind (N-S & E-W)_. So total of 11(=1+2+4+4) independent variables. \nDo let me know if that is only you meant to confirm.',), ("A regression with one response and a one binary predictor is in essence equivalent to Student's t test comparing two means. This is often regarded as too puzzling to be mentioned in introductory texts and too obvious to be underlined in more advanced texts. (In practice, not everything in your favourite software to do either procedure may match exactly.)",), ('OK, I hope to be able to get back to you today.  If not, then Monday.',), ("This hasn't been clarified....",), ("My question was about whether it's possible to compose all $\\\\beta_1 X_1 \\\\ldots \\\\beta_P X_P$ into a single $X$-variable, and then to conduct simple linear regression.",), ("You would need to get the $\\\\beta_j$ first. If that's by estimation, it is multiple regression and not to be followed by a simple regression. If that's otherwise, you could do that.",), ('I think the poster is saying that there are multiple $y$s. numpy.linalg.lstsq can handle this task.',), ('To predict maximum_price (one variable) you need to select variable(s) from the same dataset to use as predictor(s). Manufacturer and model need not be used in a beginner\'s level model; they just define observations. At a more advanced level, you would be expected to consider whether (e.g.) cars from the same manufacturer could be alike in ways not fully captured by the other variables. The tag "categorical-data" seems irrelevant to your main question. I\'ve not edited it out, but I advise that.',), ('Here is an analogy that might help. Suppose you had data on 93 people, which include family name, individual name, height and weight. How would you use those names to build a regression model predicting weight from height? As a first stab, you would probably not use them. However, there might be some information tied up in those family names as families tend to share genes, similar environment, life habits etc. which might be relevant to weight given height. There are more advanced models that make use of that information.',), ('Hi Nick, Many thanks for your advise. Sorry about the tag "categorical data" I was not sure what to add there. You said the variable manufacturer and model will be used in more advanced level, right? caan you please let me know whether there is any different methodology is used to model at advanced level.',), ("At a more advanced level you might try a multilevel model (`PROC MIXED` or `GLIMMIX` in `SAS`). This is one way of dealing with data that is not independent (which is what @NickCox 's answers imply).",), ('@PeterFlom, thanks for your insight. Being new to SAS, I am  not aware of these functions. Anyway, I am gonna try working on this model like Nick advised and go ahead without variables Manufacturer and Model. Let me see where I end up. Thanks to both of you for your help.',), ('@Peter Flom said it. I have never used SAS, but evidently Peter knows it well.',), ('Oh, `MIXED` and `GLIMMIX` are not for beginners. :-).',), ('@Bill, Hope you are fine. Could you share your views on problem?',), ('Why would the second order partial derivatives be between -1 and 1?',), ('I suppose you meant $\\\\alpha=0$ and $\\\\beta=1$ in your third question?',), ('Corrected! Thanks @mpiktas',), ('Hello Glen, In Prism, they use Cov to refer to the normalized covariance matrix, where each value is between -1 and 1. I do not really understand why it is (might be) important',), ('Is $b$ under "**Notation**" supposed to be the same as $u$ under "**Question**" (etc)? Are you thinking of this as a mixed effects model?',), ('@gung Thanks for pointing out this notation inconsistency. I think it is now fixed. Yes, it is a mixed effects model, as you cleverly guessed.',), ('@gung Although this is not a "self-study" question (it is not homework), it may look as such since I tried to be concise, I will leave this tag since I just need some hints to get a solution.',), ('Our policy regarding what to call `[self-study]` is that it does not have to be from an actual class. Instead it refers to the type of question that it is.',), ('@gung Thanks for the clarification.',), ('With just one response variable, your method is multiple rather than multivariate. Indeed, using several predictors is hardly a big deal now, and many people would drop the multiple, not that "multiple" is in any sense wrong. I\'ve edited accordingly.',), ("Physically, your regression doesn't make sense. Direct radiation is an external input. The other variables are state variables saying something about the atmosphere at ground level. The physics here doesn't reduce to linear relations any way.",), ("Direct radiation at the top of the atmosphere, yes, but it gets more complicated as to how much of it gets to the surface. I agree it's a long shot, but I'm trying to use this estimate data I don't have and I'm trying to make something that might be better than nothing.",), ('A [reproducible](http://stackoverflow.com/questions/5963269/how-to-make-a-great-r-reproducible-example) example always help.',), ("Maybe I do not understand what you want, but assuming you want the logLikelihood of a OLS model you just fitted (eg. `lm_test <- lm(weight ~ age)`) won't that just be `logLik(lm_test)` ?",), ("Thanks but unfortuantely this calculation has to be done in Matlab and I don't have the function mle() which is in the newer versions of matlab.",), ('That small detail that was a bit important... I thought you were talking about `R`. OK, a bit more rigorously then : $-\\\\frac{N}{2}\\\\log(2 \\\\pi) - N\\\\log(\\\\sigma) - (1/(2\\\\sigma^2))  \\\\sum( lm_{residuals}^2)$ where $N$ is your number of readings, and $\\\\sigma$ is the standard deviation of your error. Do you have `lscov` functionality, so I can present an example using that or you want the standard *matrix left division* notation for MATLAB?',), ('It seems I have lscov() :-) (Although I am prolly more used to using left divide).',), ('See the answer below, I take account for both cases.',), ('I do not agree with @IMA. Partial R squared is directly linked to partial correlation, which is a nice way to study confounder-adjusted relations between iv and dv.',), ('It makes no difference whether you teat them as categorical or numeric. (To make them categorical in R, you would `as.factor` them, and then the dummy for the second level of the factor would replicate the original 0-1 variable.)',), ('i see. thank you glen.',), ('Questions / misunderstandings about the marginal distribution of $Y$ & the distribution of $X$ are common. In addition to the good answers below, this thread: [what-if-residuals-are-normally-distributed-but-y-is-not](http://stats.stackexchange.com/questions/12262/) might be helpful.',), ("They'll probably just be parameters (constants) that need to be estimated from the data, and then the estimates of the constants are used to project the curve past the red line.",), ('Just some comments and then a question: (1) this is not "linear regression." If you\'re interested in researching such techniques, look instead at "nonlinear regression" or "curve fitting." (2) The data shown clearly are badly described by a formula of this type, as evidenced by (1) the values reach $0$ for large but finite $t$ and (2) the values reach $1$ at a strictly positive $t$. What does this "series" represent, by the way?  *This matters* for the analysis. The apparent consistent decrease in the values suggests the plot shows derived, rather than original, data, which is important.',), ('This question appears to be off-topic because it is entirely about which software syntax is appropriate. See the help for more advice if unclear. ',), ('Btw, none of these methods answer the question about normality of the errors.',), ('@NickCox the first two questions in the body of the post -- "*How can I diagnose the normality assumption about time? Should I need to perform a linear regression to measure the dependency of time on age and karno?*" seem completely on topic to me. They\'re not directly questions about syntax but about statistical issues.',), ('@Glen_b Why then ask "Are the following commands all to answer the question"?',), ("I didn't say there weren't any syntax questions. There are statistical questions - ones which can be addressed - and so the question looks to be on topic for CV, to my eyes. But even the directly R questions seem to be borderline (since they're about implementation\\\\* of a statistical calculation). -- * not useful, perhaps (though addressing why is again a statistical issue) -- I think even that arguably falls under the last topic in the list of what's on topic in the help.",), ('It seems that the title means diagnosis of assumptions of the linear model (normal distributed residuals), while the body means asks about diagnosis about the response itself. Linear model assumption has nothing about the response distribution itself, but the normal residuals. But maybe response distribution is your particular study in your project?',), ('I agree with @Glen_b .',), ('user2983722 As above, your question has produced quite different interpretations. Please clarify: Do you want general advice on assessing normality in a linear regression context? specific advice geared to R syntax? specific advice geared to the dataset you refer to? Why you suppose a linear regression with time as response makes sense, any way? In short, precisely what is "the question" you are addressing?',), ("@NickCox Perhaps, i couldn't able to express  what i actually want. Please, see my edit.",), ('As @Michael Mayer hinted, any normality assumption in regression is about the errors, not the original data, and as such is best examined by looking at the residuals from a (serious) model.',), ('You can obtain formulas without using matrix arithmetic by using repeated "linear matching" as described at http://stats.stackexchange.com/a/46508. Better, though, would be to learn how to work with matrices: there are plenty of textbooks and online resources. Your education should not be limited to what your university offers! (I find it unbelievable, though, that your university does not teach this material, it is so basic. You might want to do more research into what is available where you are.)',), ("What problem(s) do you run into when you simply regress `lILC6` against the original five variables?  Since the third PC still accounts for 20% of all variance, with 100-78 = 22% left to explain, none of the remaining two PCs (not shown) can have more than 20%, whence each has at least 2% (and likely more): this implies the eigenvectors don't vary a whole lot--the ratio of the highest to the lowest is at at most $\\\\sqrt{30\\\\%/2\\\\%} \\\\lt 4$, indicating there should be no multicollinearity problem at all!",), ('Necessarily b=1 or x=0 in the equation as written.',), ('@conjectures sorry there was a typo here. I have corrected it directly in the question',), ('Why do you want to do this? Categorizing variables is not usually a good idea.',), ('Dear Peter, I want to calculate the odds ratio and conduct logistic regression with the group based on new cut-off.',), ('Why would you do that when you have a continuous measure already? Hardly ever a good idea.',), ("It sounds like a better method might be spline regression; but that article did stuff I don't recommend",), ("the link is to a biomedical journal article. They often use cut-points to provide clinically useful thresholds, with the understanding that you're trading a bit of statistical quality for a bit of clinical utility.",), ('Related question: http://stats.stackexchange.com/questions/44838/how-are-the-standard-errors-of-coefficients-calculated-in-a-regression/44841#44841',), ('If you set e.g. $x_2$ to 0 then $b_1$ is only the contribution of $x_1$ when $x_2 = 0$. Is that what you want?',), ('This is not the right approach. Use instead the "matching" method described at http://stats.stackexchange.com/a/46508.  Equivalently, perform multiple regression.  Note that in any event the "contributions" of the factors are not additive unless all factors are orthogonal.',), ('Yes, just use simple algebra to isolate that extra intercept, the answer should not be equal to $y$, but $y + \\\\beta_0$. In a way, you\'re adding two regression planes on top of each other without noticing that the "height" of the regression planes was double counted. In other words, if you subtract the intercept from the sum of $y|x_1 + y|x_2$, you should be able to recover $\\\\hat{y}$. Notice that it will not be equal to $y$, cause you haven\'t added the error term back to either side.',), ('Is this homework? If so, please add the `self-study` tag. Also what is a "linear trend cycle component"? Plus please tell us what data you have. Clearly you have more than "65 values"; you must have data on season, and probably other things as well.',), ('Is your question rather from "time-series" realm than "regression" realm?',), ("Ordinary least squares regression won't work here becuase it is a time series dataset, you can try any time series regression methods in SPSS.",), ('Can you post the data online ?',), ('clearly in plotting the data, I see a trend-cycle and seems to be quadratic in nature, you should be using time series regression for this type of problem. If you want to use regression, then you can regress lending = Q1 + Q2 + Q3 + time + timesq, where timesq = time ^2. This would still not give a good fit, you also might want to add lag terms. Also you can try ARIMA in SPSS, that would automatically do this for you.',), ('@charles Thanks, did you saw any previous study adopted this kind of fitting to get optimal cutoff point?',), ('I think the article says it pretty clearly. One may write a syntax to name a cut point, run the two regression, export the two $R^2$, and repeat the process with another cut point, so on so forth. When all repetitions are done, plot the two $R^2$ against the cut point, and look for the one that optimize the models (aka give highest $R^2$ for both models). Unless you have spotted a very clear inflection in $x$, there can be multiple optimums, which will cause some problems.',), ("@JuneKang  I haven't seen something like this. I'm off campus so can't access article. But as someone mentions in your other question it sound a lot like what the -segmented- package does. For that package you have to provide the number of cut-points. If you don't have a preset number this can sort of be dealt with by running a number of cut-points(1-5) and comparing AIC and choosing best model.",), ("To reliably estimate a coefficient takes a deal more than a couple of points per coefficient. You're talking about estimating 5050 parameters (plus an intercept and $\\\\sigma$). Do you have enough data for all that?",), ('How are you assessing distributional shape? Some kind of diagnostic display?',), ('Thanks for your answer - will try to digest and act on it when next in the office. Yes, I used scatterplots where Y = Studentised residuals and X = predicated values / predictors in the model. I also had a look at the mean, SD, etc., of the residuals.',), ('For your 1st question: The $\\\\hat{\\\\beta}_0=3.2$ is given, so you know this parameter. Does that help? If this is a homework, add a self-study tag.',), ('The intercept $3.2$ is assumed to be known ?',), ('@Stat: What does that mean? I thought the simple linear regression had two parameters, the intercept and the first term?',), ('It means that they may be some situations where you know one of the parameters. For example: suppose you want to find the relation between the weight $Y$ of a person and the time $X$ due to a special diet. Then for sure you know that at time zero the weight is for example 65 kg. So when $X=0$, your $Y=65$. This means that in linear model $Y=\\\\beta_0+\\\\beta_1X$, your $\\\\beta_0=65$. So you know this parameter before fitting. Also you may expect your estimated $\\\\beta_1$ to be negative!. This may not be a perfect example, but will give you an idea.',), ('@Stat: Oh, so if one of the parameters are known, it is subtracted from n? So in this case, since the intercept is known, we subtract just 1?',), ('You might be interested in http://en.wikipedia.org/wiki/Ecological_fallacy, which applies here.',), ('Consider this strategy: average your averages, but keeping two distinct x values. With any luck, your y values will differ. Now what is your $R^2$? Correct or not, the key point is that you changed your regression problem to a different regression problem. The more you average, the more variability you eliminate. Feature and bug, all in one.',), ('The differences in $b$ and $x_0$ do not appear large: check their standard errors. The difference in $a$ is due to a basic mistake with logs: $\\\\log(y) = \\\\log(a) + b\\\\log(x-x_0)$ (which I guess is compensated for in the last line), whence the "a" in the log model should be close to the log of the "a" in the original model. When this change is made (by replacing "a" in the first model with "exp(a)") the changes in all three coefficients are within the ranges indicated by their standard errors. In short, all appears fine: is there really a problem to be resolved here?',), ("An example in which a scientist's claims were wholly discredited due to the use of average values in a regression is documented in this [2007 EFSA Review](http://www.efsa.europa.eu/en/efsajournal/doc/19r.pdf). For recent (amazing) developments see http://retractionwatch.com/2013/11/28/controversial-seralini-gmo-rats-paper-to-be-retracted/: the same scientist misused statistical analyses so badly that six French scientific academies joined to denounce his work.",), ('As long as you don\'t define what you mean by "want to just \'control\' variables" you won\'t get an answer to your question. From the figure caption in the (unsprecified) publication you are referring to it should be possible to deduce how they "controlled" the covariates.',), ("Thanks @whuber! Cheers for seeing the log(a) error. I do optimize it as a nontransformed parameter, as it is prone to creating NaN's. And after thinking some more about this, I think you are right and there is no problem. I was surprised it gave such different exponent values, but when looking closer at the $x_0$ differences and how the two solutions optimize it, it does make sense. Glad you made me see it this way!",), ('I have a couple of practical suggestions then for achieving stable and reliable solutions. (1) Use $\\\\exp(a)$ in model 1 and $a$ in model 2 to obtain comparable results without NaN problems. (2) Run model 2 first and feed its estimates as starting values to model 1. (3) Break model 2 into two parts: only $x_0$ is nonlinear; $a$ and $b$ are efficiently determined by least squares (*e.g.*, use `lm`). You can then use a *one* variable application of `nls` to estimate $x_0$. If you want the full output of `nlsLM`, feed it those estimates as starting values.',), ('Take a look at the **party** package and see if this suits your needs. It can handle a number of types of models in the nodes of trees IIRC.',), ('Thats a great tip on the starting values. The parameters are very strongly correlated, and I guess the solution is prone to local minima as well. The reason I apply a nonlinear solution is to estimate the 3 parameters simultaneously to quantify the correlation and hence get a more realistic uncertainty estimate. Now I just need to come up with a good way to do the same with a 3 degree polynomial with zero intercept. Any good tips on that @whuber ?',), ('Offhand I would consider orthonormalizing the basis of polynomial functions $x^i,$ $i=1,2,3$ (as evaluated at the data) and using ordinary least squares.',), ('In what way is the data "not good"?',), ('Actually they are not clustered very well. And if I perform ANOVA approximately no significant gene I would get. I think it should be because of fewer number of replicates.',), ('Are you referring to a deterministic relationship?',), ("One thing that might help you visualize what's happening in the plots is to jitter in both y and x (note that if it is in completed years, as is typically the case, Age is also discrete). Which is to say, add a small amount of 0 mean uniform (say) noise in both directions, scaled so that the fuzzed data is still distinct if the x or y values differ (that is, the width is less than half the gap between values)",), ("Also note that if y is limited to values between 1 and 6, a linear model seems unlikely to work over a wide range - you'd expect the relationship to flatten at the ends, as it gets closer to the bounds in y. One thing you could do is compare a smooth of the relationship with a linear fit. (What software are you working in?)",), ("Thank you very much for the hints! I'm working in sas.",), ("Hmm, can't directly help with SAS.",), ('One regression does not remember what you did previously. So, just exclude what you want to exclude.',), ("Your constraint implies that you are regressing $y$ on a single variable $x_1 + x_2$ and forcing its coefficient to be $1$. That doesn't solve the problem of errors in predictors. Errors in the dependent variable are what you expect with regression.",), ('I mean I want to exclude the variable without having to do the training again.',), ('What "training" is involved here? You don\'t give any details of what you did except "regression". If you are using some flavour of machine learning, you should explain what that is.',), ("In my case I'm using Lasso, but I wanted an agnostic answer in relation to the training procedure.",), ('Ok, thanks. But I am not sure how should I force the coefficient of (x1+x2) to be 1.. Can I do that with a least square fitting?',), ('I am not intimately familiar with Lasso, but I imagine that repeating Lasso with one predictor omitted is quite sufficient for Lasso to "forget" it. Again, if you want something different, you need to spell out what that is. Different advice is to edit your question to make it clearer: many people are reluctant to read a trail of comments to see whether the question was changed.',), ("I've edited the original question for more clarity.",), ("You can constrain things with least squares by reparameterizing (let y* = y-x2 and x* = x1-x2. Fit y* = ax* + c and then let b = 1-a. However, that won't deal with your errors-in-variables. In some cases at least, though,  reparameterizing might work there as well.",), ('If you omit one $x$ from $X$, expect all coefficients to change.',), ('Would it be possible to supply a reference for that? I searched for omitted variables and mostly I got derivations on how the bias of the regression would be affected, not on how the other coefficients would change.',), ('It should be stated or at the very least implied by every regression text. The converse is that if you add a new predictor, expect coefficients to change unless the new predictor is uncorrelated with all the others. Try it with data.... (You seem to be trying to learn statistics by searching the internet. You need to work through reliable textbooks, not browse for what you think you want.)',), ('Please show your code, and, if possible, some idea of your data.',), ("I've never used Weka, and I wouldn't ask my worst enemy to do regression using Excel, but it appears that you have reverse coded some of your dummy variables. You've put Woche4=0 in Weka, but (assuming it's coded 0, 1) the variable that's used in Excel will be Woche=1.",), ("Yes it's reversed, but that shouldn't change the intercept, should it...? I'll try it anyway.",), ("There are two main reasons you'd expect to reject the deviance test (i) the model has a lack of fit; or (ii) the model fits okay but is overdispersed relative to the binomial.  Might the second main possibility be the case?",), ('Thanks, Glen_b!.. However, I am not sure I follow what you say. Could you please develop a bit more, or maybe give me some pointers?.. Thank you very much!..',), ('@Macro could you please help me understand the concept of "standardized" variables.',), ('Yes. The coefficient of smoking will be equal to the difference in mean results for the two groups. The p value of the null hypothesis of no smoking effect will be similar to the one of the corresponding two sample t-test (using equal variances). In SPSS, you can use smoking as a numeric covariable or as factor. The numbers will be the same. Just try it out.',), ('Thank you peter, very kind of you',), ('Should follow from the delta method, and also probably be part of the output of whatever statistical package you are using. Can we see some code?',), ("I use python with  scipy and lmfit. lmfit's confidence finding algorithms are currently flawed and I don't know how to fix it (that's another story)",), ('I have made some edits to your question to incorporate a description of the model being fitted. You should check it still says what you want.',), ('See also related question and visual explanation http://stats.stackexchange.com/q/70899/3277',), ('Are you using `anova` from the stats package?',), ('What is your model?',), ('yes. Not the `car::Anova`.',), ("How do you mean? Unfortunately I can't post actual data if thats what you mean.",), ('R is pretty terrible with anova, given that it defaults to sequential sums of squares. Why are you fitting your model with `lm`?',), ('What is your dependent variable? What independent variables? How big is the sample? Are the variables continuous or categorical? that sort of thing. The actual context would be nice, too, even without data.',), ("what's the best way to fit a multivariate regression model where both the multiple responses and predictors are continuous variables?",), ('Is your dependent variable naturally bounded?  Sort of looks like it.  And yeah, you need to provide more info.',), ("I guess I usually use `glm`, but I don't deal with much continuous multivariate data.",), ("Anyway, Pillai's trace is a multivariate statistic (like Wilk's lambda or Lawley-Hotelling trace). The first df is the number of degrees of freedom of the model term (which clearly are all continuous) and the next two dfs are the numerator and denominator degrees of freedom for calculating the F statistic. The p-value is the test that the probability of the observed data is greater than F.",), ('More details added',), ('Are you asking in general, or are you asking specifically how to do this in R? Note that the latter would br off-topic for CV (see our [help page](http://stats.stackexchange.com/help)).',), ('It looks like your data is bound between 0 and 1.  Is the outcome actually a proportion?',), ('In general.And may be better then in R.',), ('I think this Q is on-topic, then. Is there any specific model diagnostic you are interested in? Are you just interested in a lack of fit test, or what all of the diagnostics that should be standard are?',), ('Diagnostic for Univariate model is well set up, but I did not see a routine diagnostics method for multivariate models where the response has more than two variables like here.',), ('You should not be doing ordinary linear regression with a "bounded (0-1) response variable".  Neither the assumption about linearity of the mean nor the constant variance assumption are likely to be true, except in some special cases, and even then usually only approximately. Is your bounded variable compositional data, or is it a proportion based on counts, or something else?',), ("What is your goal in estimating the model? There are plenty of direct and indirect diagnostics but they aren't all appropriate in every situation.",), ('You should probably take a course in regression, there is much more to it than just looking at the output, at least when the output is so limited. You have assumptions to check, graphs to make, etc.',), ("As Peter correctly suggested a short course in regression will probably benefit you greatly. For self-study, very approachable and hands-on expositions of linear regression with R coding examples are presented in J.Faraway's books: `Linear Models with R`. Additionally, the following thread : http://stats.stackexchange.com/questions/170/free-statistical-textbooks offers a variety of free e-books that can take you a very long way.",), ('Your question amounts to "teach me to understand regression", which would take a book to answer. "*...are the two variables highly correlated?*" - that depends on what counts as high, and there\'s no single standard; a chemist might call that $r$ very low, while a psychologist might say it\'s unbelievably high. It depends on what we\'re dealing with (but then why would a high $r$ matter in any case?). "*The r-value is something that I think points to the correlation*" -- no, it *is* the correlation. "*what would be ways to find out which ones do not satisfy this constraint*?" -- what constraint?',), ('@Glen_b Since the correlation coefficient is not 1, there are values which are not showing direct relationship. Is there a way to figure out such values.',), ("If there was a perfect linear relationship, you wouldn't need regression at all. Because there's going to be variability about any underlying linear relationship, you don't expect it to be 1.0 even when the population relationship is linear.",), ("I think this is mostly (if not as directly) covered by Macro's answer in [this post](http://stats.stackexchange.com/questions/31690/how-to-test-the-statistical-significance-for-categorical-variable-in-linear-regr?rq=1). It even includes discussion of the use of `anova`. If for some reason that doesn't cover it, I can undelete my answer here.",), ('question amended with more details',), ("Now I have no clue what you're even asking. What information do you want to show? If you're not interested in summarizing contribution to the sum of squares, what *are* you interested in showing?",), ('The most economical way to present this regression in a table is to show an empty table, because the regression indicates there are no significant linear relationships between the response and *any* of the variables you have included!',), ('question again edited',), ('No, you cannot average the t values or p values across months. I would also worry about the "two not defined because of singularities" and the odd pattern of coefficients for months.',), ('@PeterFlom: the singularities are cause by the setting of `x1`, `x2` and `x3`: for example all males are adults and the first six months, and all females are juveniles and the second six months.',), ('Where is the month of April?',), ('You should post your code here.',), ('The output will look better if you add this to your analysis:\n## from the AER book by Zeileis and Kleiber\noptions(prompt="R> ", digits=4, show.signif.stars=FALSE)',), ('Welcome to the site.  "in the good" = ?',), ('Have you considered natural splines rather than higher order polynomials?',), ("the issue is that in practice you don't observe $\\\\epsilon$ (only $\\\\hat{\\\\epsilon}$) so you can never compute $\\\\mbox{cor}(X,\\\\epsilon)$",), ('As said above, $\\\\epsilon$ is unobservable so you need to rely on your own knowledge and use the common sense to decide about exogeneity.',), ('The correlation between regressors and residuals (not errors) in a linear regression model estimated by least squares _is_ always zero. You cannot test exogeneity (conditional uncorrelatedness) without instrumental variables.',), ('Thank you for your comments, as putting them all together answers my question. Is anyone willing to organize these together into an answer which I can accept?',), ('You forgot to include a vector of 1 representing the intercept. The rest looks correct.',), ("I changed only the  'X' as you suggested, but then I got different coefficients again.",), ('The expectation of the errors is zero.',), ("How do you define X' ? Existence of X' would imply that X' is square, which, unless I am mistaken is not the case.",), ('Sorry, rolando. English is not my first language.',), ('The very closely related question at http://stats.stackexchange.com/questions/25068/interpreting-plot-of-residuals-vs-fitted-values-from-poisson-regression/30276#30276 may be instructive in this context.',), ('No apology necessary, Rene, I was just trying to get the meaning straight.',), ('This is an *enormous* subject. I can see that narrowing your question might be difficult, but any adequate answer would--and has--taken several large books.  You might begin your research by linking through the [tag:model-selection] tag I added; it will show you some threads in which these topics have been discussed.',), ('Yeah, I was worried the question was too broad and fluffy.  But I\'d be happy with someone just saying "I\'d try the elastic lasso vector  extension support (elves) method first.  Good luck!"',), ("Maybe you shouldn't be! As @whuber is implying, your quest for good predictions is much more complex than the matter of choosing the right model. You might want to spend a couple hours studying regression, if you haven't already. Your second question is pretty basic, but the answer may not be. Your information is all relevant to other, variably complex (or simple) issues that you may want to understand better. I can throw some jargon at you to help you narrow down the topics worth studying if you like, but this is just as likely to raise other questions as answer yours. Does that sound good?",), ('Also, how do you know there are errors in the predictors? Do you have any quantitative information about how much error? This might include redundancy among multiple indicators of the same latent construct (i.e., if you think some of your predictors are measuring roughly the same thing)...',), ("Come on...  Everybody knows that blindly following a recipe is better than true understanding.  Besides, I don't have the luxury of too much time.  Not being a statistician, I get bogged down by the jargon and the notation.  I was hoping for some narrowing of the subject, so I don't spend a day or two to discover I'm going the wrong direction.",), ("Some of the measurements we are making are inherently correlated (e.g. the temperature in one spot is almost the same the temperature in a nearby spot, bigger things weight more, etc).  We have done repeated measurements of a single object and how the predictors vary.  But that's only for that one object, the next one will be different.  And I wouldn't know what to do with the variation anyway.",), ('Are the two models run on the same population? Is Y the same? Is X? (it would help to use subscripts on Y and X to distinguish them).',), ("I rarely make this recommendation, because people come here out of self-sufficiency, but since you have a complex problem and don't have the time to learn how to analyze it, it seems like the best course of action is to collaborate with someone who has the knowledge and experience to help you.",), ('What did you read where? What was the context?',), ('I dont think it can be enforced with real input data. You can of course do PCA in your data to transform it to orthogonal series.',), ('I am not sure what you mean by "enforcing". One can check whether the variables are orthogonal. But if they are not orthogonal then they aren\'t. Multiple regression is very often used when the input vectors are not orthogonal.',), ('@adam this is my belief also, however I am very fresh to statistics to really know that',), ('@PeterFlom by enforcing I understand a way to design the experiment that would cause the inputs to be orthogonal\nI am not sure however what authors means by enforcing. That was the question. If it exists such kind of design of an experiment in order to get orthogonal inputs. Solely my imagination produced only the scenario for nominal inputs, but for continuous I am not able to design such thing.',), ("This has a flavour of: I get 42 as the result of a calculation with data I won't show you. Did I do it right? Better answers: Your tag indicates use of SPSS. Does SPSS reproduce regression results from elsewhere reliably with known data? You can carry out your own checks. Have you searched for literature on bugs, limitations or reliability of SPSS? As of 2014, it's a fair guess that regression routines in all major statistical software have been banged on many, many times, but that's no absolute guarantee of correctness.",), ('Thanks a lot for response;  I have descriptive analysis of variables and I edited my question. I do not have access to data set; I found these results in a paper and I want to be sure of the validity of results.',), ('**There are several things fishy about this summary:** [Last] and [Score] range from $0$ through $100$, so how did they take the logarithms of the zeros? Moreover, because it is clear that [Last] has some *negative* skewness (notice how close its mean is to its *maximum* value), using its logarithm would usually not improve anything--it would only give the extreme low values more leverage. Furthermore, it is strange that "Model 4" would include interactions of [Win R] and [Win F] with other variables without actually including [Win R] and [Win F] themselves.',), ("There's no way to be *certain* of the results without the data (and even then, validity would be only for the data you were given, whose provenance may well be doubtful). You can however, often do some reasonableness checks. (You can rely on SPSS for basic regression calculations, since it's easy to check it works on your own data.)",), ("When bootstrap aggregating trees, the overall regression function becomes more and more complex with every tree one adds.\n\nOn the other hand, when bootstrap aggregating linear functions of the form `a_0 + a_1 * x_1 + ... + a_d * x_d` , the resulting averaged linear function (after bootstrap aggregating) still has the same linear functional form as the one you start with (i.e. the 'base learner').",), ('"the line vertically closest to all the points" ?\nOne usually takes the sum of squares -- see the nice picture on Wikipedia\n[Coefficient_of_determination](http://en.wikipedia.org/wiki/Coefficient_of_determination).\nThe sum of vertical distances is the L1 norm, \nwhich is less sensitive to outliers but much less common.',), ('*Ahem*. (i) The expression of the problem is nonsensical. How can 3=18? Surely the intent is something like $f(3) = 18$; (ii) if you can see enough to write $18=3\\\\times 6$, $32=4\\\\times 8$, etc., surely you can then see enough to split the second term in each of those ($6=3\\\\times 2$, $8=4\\\\times 2$, and so on) to then write: $18=3\\\\times 3\\\\times 2$, $32=4\\\\times 4\\\\times 2$, etc, and *instantly* spot the quadratic, $f(x) = 2x^2$. (You did the hard part, the next step is even simpler!)',), ('Running models on lagged data is the essence of predictive modeling. If your data are lagged 4 months, you need to take that into account. Just make sure your output corresponds to the predicted, non-lagged value when you are calibrating your model.',), ("Additionally, did the problem specify a minimum information content criterion on the answer? If I remember my math correctly, there are an uncountably infinite number of functions that fit these points, all giving different answers for $f(10)$. I'm not typically pedantic, but time-waster emails deserve it.",), ("I know this doesn't answer your question (sorry), but have you tried a univariate time series? How far into the future do you want to predict?",), ('@TrevorAlexander if you think this question is a waste of time, why bother to respond to it? Clearly some people find it interesting.',), ('@jwg because [someone is wrong on the internet](http://imgs.xkcd.com/comics/duty_calls.png). ;)',), ('you are purposely not including a constant in the regression? Usually a Beta_0 is included.',), ('Are you familiar with [tag:var] (vector autoregression)? That sounds a lot like what you are talking about.',), ('This looks like a homework question. If it is, please add the self-study tag.',), ("@PeterFlom It's not homework but a question that I'm self-studying. I added the tag though.",), ("Sounds like you're not fitting a mixed-effects model which you should do here to account for variation between the operators and (nested within that?) the robots. As for the issue with negative damages, you can get around that by doing some kind of transformation",), ('R lm works as expected, the problem is with your data, i.e. linear relationship is not apropriate in this case.',), ("Could you draw what line you think you should get and why you think your line has smaller MSE? I note your y's lie between 0 and 1, so it sounds like linear regression would be quite unsuitable for these data. What are the values?",), ("@Glen_b The red line in pkofod's answer below seems a better fit to my eyes. Would that line not decrease the MSE? It's just my intuition.",), ('@Glen_b The y values are probabilities of being of a certain class. The x value is a feature (based on an estimate, x=log(error)).',), ('Well how did you obtain the y-values then? No matter what, OLS is not suited for these kinds of models. Look at your blue line. What will the predicted probability be for x=10? Is that a probability?',), ('@pkofod The y values are probabilities of being of a certain class, obtained from averaging classifications done manually by people. The x value is a feature (based on an estimate, x=log(error)).',), ("@tucson I saw that the first time you wrote it, but to understand your problem it's beneficial to know where the data comes from.",), ('One place where ridge regression is used is when there is colinearity. In this case, the OLS parameter estimates are not biased, but their variances are off.',), ("If the y values are probabilities, you don't want OLS regression at all.",), ("I think you've answered your own question @Baz. If we could do what you suggest, then why would we run the regression at all?",), ('@PeterFlom What would you recommend?',), ('What no-one seems to be underlining is that when the response variable is a proportion bounded by 0 and 1, any kind of straight line fit is problematic as it predicts values outside that interval for some values of the predictor. From the graph, the relationship looks pretty weak, regardless, but if you are set on modelling it, some kind of logit or probit model that respects the bounds looks a better call to me. Often substantive or scientific reasoning helps here. What do you expect to happen for extreme values of $x$?',), ('(sorry could post this before) What looks to you  like "a better fit" below is (approximately) minimizing the sums of squares of orthogonal distances, not the vertical distances\' your intuition is mistaken. You can check the approximate MSE easily enough! If the y-values are probabilities, you\'d be better served by some model that doesn\'t go outside the range 0 to 1.',), ('@Glen_b Thanks. Ah, you are right, I thought I had to minimize the orthogonal distances, not the vertical ones.',), ('@Peter are the OLS estimates unbiased for all levels of multicollinearity up to and including perfect multicollinearity',), ('If that is the case then it seems we can use the OLS estimate to estimate the bias?',), ("@Baz I am not sure about perfect multicolinearity, but that is (in my experience) always an error that is easy to fix (e.g. you include height in inches and height in cm as IVs) and I don't believe ridge regression can cope with it either.",), ('Thanks again Peter, OK except in case of perfect MC I can use OLS to estimate bias?',), ("So what you have is a setting where you know the OLS-estimator is unbiased, but you then employ some other estimator which you don't know if it is unbiased and you want to assess that, have I understood you correctly?",), ("I know that the estimator I am using is biased but I want to be to estimate it's MSE. I can estimate the variance quite easily by generating different realisations of the errors and recalculating the beta parameters but to calculate the MSE I must find someway to estimate the bias without knowing what the true value is.",), ("Thanks for your advice, although I must admit I don't know how to apply either of the suggestions.\n\nI tried plugging in `lme` instead of my `biglm`, but obviously I need to do a lot more reading on this to understand what exactly to provide as parameters to it.",), ('This answer may help http://stats.stackexchange.com/questions/81806/omitted-variable-bias-verification-in-gretl/81813#81813',), ('Is this the only way you can get the data?  It would be better to have a dataset with one observation for each battle, identifying the two operators, the two robots, and the outcome.  If the data have to come as summary information, can you get different summary info, or is this it?',), ("This is it, the data is received from an external system owned by another company and this is unfortunately the extent of data available.\n\nI have a few more summary variables available which I didn't mention here (you can consider them DamageB, DamageC, etc...) but they are closely correlated with DamageA & DamageB so I don't think they are that useful and I didn't mention them to avoid confusion.",), ("Linear regression might have one x-variable (simple linear regression) or more than one. Multiple linear regression can *only* refer to the case where there's more than one.",), ("@Glen_b, I didn't exactly get it. you say that Linear regression might have one x-variable  or more than one. Multiple linear regression can only  more than one x-variable  ?",), ('Correct. Linear regression can encompass both [simple-](http://en.wikipedia.org/wiki/Simple_linear_regression) and [multiple-](http://en.wikipedia.org/wiki/Multiple_linear_regression#Simple_and_multiple_regression) linear regression (though perhaps more frequently refers to simple linear regression). But those latter two terms are mutually exclusive categories.',), ('It could be that this regression suffers from the presence of a few outliers. Could be a case for robust regression. http://en.wikipedia.org/wiki/Robust_regression',), ('This is more of a comment than an answer. Would you mind expanding it to make it more of an answer? Alternatively, we could convert it into a comment.',), ('You can convert.',), ('@Yves Thank you, yours is a worthy suggestion that nicely supplements many of the answers.  I hope others will also upvote your comment so that it appears prominently in this thread.  Or, if you feel so moved, consider amplifying it into a full answer.',), ("You're mistaken. $Q$ *is* $n\\\\times n$. However, $QR$ is of the same dimension as $X$, $n\\\\times p$. The $p \\\\times p$ dimension given for $R$ in the text is what is called 'little R'; the full dimension of $R$ is $n\\\\times p$ but the lower part is zero.",), ('It looks like there are two versions of the QR factorization. I was taught The "skinny" version where A is mxn,Q is mxn and R is nxn. See http://www4.ncsu.edu/eos/users/w/white/www/white/ma580/chap3.3.PDF',), ("Since you only care about how far a score is from the mean score, just make up a mean score as its value won't affect the answer.",), ('Hint: write down the prediction equation using deviations from means (which is what the question asks). Then remember how the constant term in a simple regression is calculated.',), ('Ok I got it, thanks!',), ("See Penguin_Knight's answer, [here](http://stats.stackexchange.com/questions/73765/does-stand-alone-dummy-variables-in-linear-regression-models-make-sense).",), ('please add the `self-study` tag and check its [tag wiki info](http://stats.stackexchange.com/tags/self-study/info)',), ('You can find this in every decent textbook on regression.',), ('Additionally, you can look at the code behind `effect` to se how they are calculated.',), ('I once realized that I tended to dislike the phrase "confidence intervals" in the reports devoted to prediction. I thought I had two types of predictions: predicting the mean and predicting the individual observations. Since then I found several references to the same way of thinking. For example, [here is a link](http://jackman.stanford.edu/classes/350B/07/predictionforWeb.pdf) that answers your question in the style I like.',), ("I know this doesn't answer your question, but robust standard errors (Huber-White or otherwise, check out the `sandwich` package for those) won't do? Sure, the estimation will be less efficient but at least you'll be sure the variance-covariance matrix will be consistently estimated.",), ('I think you would benefit from mixed effects models. A model where you can specify the variance structure would foresee heteroskedasticity. See Zuur et al. 2009 page 75+ for more details. Another good resource on MEM is Pinheiro and Bates.ž',), ('I second @RomanLuštrik as to the use of mixed effects models. Within that framework you can explicitly model variance, and therefore heteroscedasticity. You can do it in R as well.',), ('The function is `gls` from `nlme` package.',), ('Thank you so much for all your help. I will try all of these suggestions and let you know how it goes.\nThanks again!',), ('Why is symbol not present as a main effect?',), ('Your question is unclear, because you shift from asking if they are "supposed to be symmetrical" in the 1st sentence, to implying that they are not in sentence 2 & asking (presumably) why they aren\'t in sentence 3. Can you make this more consistent / clear?',), ('OK, let me ask it this way - why are the confidence and prediction bands symmetrical around the regression line when the regression is non-linear but take on an hour-glass shape when it is linear?',), ("Just some comments, in case they're of any help: it looks like your responses must be non-negative and converge to $0$ (or close to it) at $0$, whereas these bands evidently are erected using a model of independent additive error.  That makes them unrealistic, especially at the left.  Moreover, the patterns of blue dots suggest the error has strong serial correlation, which also needs to be accounted for in constructing these bands.  Although you might not want to cope with this additional complexity in your data to do the fitting, it indicates the bands you have drawn aren't worth much.",), ('You are right.  The band does cross into the negative territory.  However, I am not interested in the values of the bands themselves, but rather in the EC50 values corresponding to the band limits.  Is there an alternative to constructing the bands this way?',), ("Short answer: no. The error doesn't appear in that image, and is not observed. Your best estimate of the error is the residuals (but sometimes it's a demonstrably poor estimate).",), ('By "not observed", do you mean the error is not not observed in the plot or it is not observed in linear regression?',), ("Errors are not observed at all, including in both those senses. If the errors were observed you could calculate the population parameters in simple regression from two observations and their associated errors. The error term is the model for how the data values don't lie on the population line",), ('Yes, but as I intimated they can get complicated. Generalized least squares and time series methods can cope with the serial correlation. Nonlinear transformations of the dependent variable are one tool to handle non-additive error. A more sophisticated tool is a generalized linear model. The choices depend partly on the nature of the dependent variable.  BTW, although I\'m unsure what you mean by "EC50 values" (it sounds like you are modeling dose-response relationships), anything computed from the illustrated bands will be suspect.',), ('http://orm.sagepub.com/content/15/3/339.abstract To all who come across this in the future, I recommend reading Dalal & Zickar (2012) -- The "solution" of using mean centering is not as straightforward as one would think.',), ("(1) I have no idea (2) you don't need to remove correlated predictors in linear regression (the coefficients for the correlated variables will be poorly defined but not always an issue) (3) I assumed the correlated variables would be retained in a random forest with relative importance spread between correlated variables, while in a linear model variable selection will often get rid of some of the correlated variables.",), ('http://stats.stackexchange.com/questions/44838/how-are-the-standard-errors-of-coefficients-calculated-in-a-regression/44841#44841',), ("@ocram, thanks, but I'm not quite capable of handling matrix stuff, I'll try.",), ("@ocram, I've already understand how it comes. But still a question: in my post, the standard error has $(n-2)$, where according to your answer, it doesn't, why?",), ('Welcome to our site!  Please use the formatting tools available when you are editing so that your code is readable.  Your "code" for $b$ obviously is not correct `R` code so it\'s impossible to tell what you\'re doing wrong.  I believe there is `R` code posted on this site doing exactly what you\'re trying, so searches on relevant keywords like [tag:r] and [tag:regression] as well as on likely parts of the code (such as `solve`) might turn up some useful stuff for you.',), ('Thanks so much for your guidance! I rewrote the code that I am trying to evaluate and I have searched for "solve" and "ginv" although they are not working as they should, or most probably I am using them wrong!',), ("You don't invert a matrix with `^-1` in R, and in fact shouldn't explicitly invert $X^TX$ at all. (Also you don't do matrix multiplication with `*`). You should solve $(X^TX)\\\\hat\\\\beta = (X^TY)$. See `?solve`, which does both solution of linear systems and inversion. That's still not the best function to use if you're trying to be accurate (QR decomposition is probably the most common way these days), but it will do for getting the ideas down.",), ('For general information about the effects of suppressing (ie, not fitting) the intercept in a regression model, it may be useful to read this excellent CV thread: [Removal of statistically significant intercept term boosts $R^2$ in linear model](http://stats.stackexchange.com/questions/26176/).',), ("Welcome to the site. With just the information you have given, it isn't possible to give good advice. I wrote [how to ask a statistics question](http://www.statisticalanalysisconsulting.com/how-to-ask-a-statistics-question/) on my blog. It may help you improve the question.",), ('I second the advice of @PeterFlom. You could also take a look at my answer [here](http://stats.stackexchange.com/questions/77196/anova-with-huge-dataset-use-only-the-mean-for-each-condition/80288#80288) as a starting point to analyzing reaction time data.',), ('worth noting (1) that on the right pane you have (low)(moderate) dummy variable and not (low)(high) dummy variables.  (2) My understanding is that JMP uses fairly odd coding strategy for dummy variables. Looks like it has done "reverse helmert" coding?',), ("My bad, I mixed the variable names. Now it's fixed.",), ('Possibly helpful notes from the JMP docs: [Coding Nominal Effects](http://www.jmp.com/support/help/Estimates.shtml#1047389) and [Ordinal Factors](http://www.jmp.com/support/help/The_Factor_Models.shtml#96069)',), ('Trivial example: X has a Bernoulli distribution (ie, taking the values 0 or 1); Y = X + N(0, 0.1). Neither X nor Y is normally distributed on its own, but regressing Y on X still works.',), ('I guess you are thinking about the distribution of the residuals, not the distribution of the variables.',), ('I have an example worked out here: [What if residuals are normally distributed but Y is not?](https://stats.stackexchange.com/questions/12262//33320#33320)',), ('Please explain your notation: what do the values of the $E_{ij}$ *mean*?  What is an "event" (the word has a conventional meaning in probability that seems to differ from your usage)?  What does it mean to "produce an observation"?  What does it mean to "match the observation"?  Where you speak of a "total contribution," what is contributing to what?  What are the $a_{ij}$?',), ("Shouldn't the last row of your w matrix consist of a pair of 1's rather than w_4?",), ("Correct - the last row should be 1's - fixed. The Eij's are just the known data for Event i variable j (so it's just a real number). I'm using Event in the context of the problem - not in the conventional sense - it is a predictor matrix. The aij are the traditional regression coefficients.",), ('Re: Q!, just because a particular variable has the largest raw coefficient does not necessarily mean it is the most significant variable.',), ("@gung so DINC isn't the one I should have selected? I thought it's the one which has the most difference from zero! Can you tell me which one is the one with most difference with 0?",), ("Mona, the question doesn't say 'most different'. The word you left out of your comment implies something other than what you did.",), ("It may or may not be. I don't know. The fact that the *coefficient* is furthest from 0 does not necessarily mean that it is the most *significantly* different from 0. Those are two different things. Also, please learn our policy regarding HW (`[self-study]`) questions. We do not do your HW for you or give you answers. We provide hints only. Please read the [wiki](http://stats.stackexchange.com/tags/self-study/info) for `[self-study]` to learn more.",), ("@Glen_b I was kind of sure about selection of DINC now I am confused...doesn't `x variable whose model coefficient is most significantly different from 0.` mean the one which its abs of coef is the greatest? Can you please explain a little more?",), ('@gung should I use t-test for finding which model coefficient is most significantly different from 0?',), ("Isn't there some model output available from R that would give you the significances of each variable?",), ('I added the summary which has t-values at the end of the question. Can you please take a look?',), ('@gung According to t-value CBE is the one I should select',), ('"most different" means what you did. The inclusion of the qualifier \'significantly\' implies the person asking the question is looking for which one is the most standard errors from zero, or which one has the lowest p-value.',), ('@Glen_b Apart from that do you know why I am receiving "Error in plot.new() : figure margins too large" error? I didn\'t receive it yesterday!!',), ('Did you change the size of the plot window?',), ('I am afraid I only have time for a quick comment, but if you search for "compositional data analysis" you should quicky find some useful info. In short, if I have 3 variables that sum to 1, as soon as I know the value of 2 of them, I must then also know the value of the third...thus trying to estimate 3 independent "effects" is nonsensical. Also, please see this related question http://stats.stackexchange.com/questions/68944/analysing-data-measured-as-proportional-composition',), ('You need to describe your motivation for feature selection, and review the literature (and huge number of posts to this site) on its pitfalls.  If you really want to proceed you might consider using the bootstrap to quantify the volatility of this process.  For example it is easy in R to get bootstrap confidence intervals for the importance ranking of each predictor based on partial $R^2$ or on marginal correlations.',), ('Are you sure you did not simply regress one of the *independent* vectors against the rest?',), ('(1) "Lagged" in what sense? This does not appear to be any definite *series* of data. (2) A good choice of model depends on the nature of the data: one would use two quite different models depending on whether $X$ were considered to be measured with substantial error or not, for instance. As another example, one would also choose different models depending on how the truncation of $Y$ at $0$ and $4$ occurs: are these natural limits or artificial cutoffs? Please edit your question to provide more information about the data so that it can be answered appropriately.',), ("(1) what have you thought of? and why don't you like those approaches (2) lagged y? is this a time series? (3) the issue - to me - seems not to be the prediction, the giving honest estimates of variance around mean.",), ('Y appears to be bounded above and below. What is Y?',), ('The question is edited. Thanks beforehand!',), ('@whuber I hope my edit contains the extra information you requested? Would you like to give my question a second thought? Thanks a lot.',), ("I don't know where you found this quote, but it sure is poorly phrased.",), ("Besides the things mentioned in the answers, your prediction intervals also won't have the right coverage.",), ('Kindly post comprehensive answers. THANYOU',), ('You refer to regression in the title & use that tag, however in the body of the question, you seem to be discussing a classifier. In machine learning, people often think of regression & classification as being somewhat distinct. Can you clarify what topic you are thinking about? Are you thinking about both at a greater level of abstraction?',), ('Oh dam, your right, I did write the word classifier. That was me being completely careless and using the term classifier incorrectly. I agree classifier and regression are very different. I apologize I will correct that.',), ('No need to apologize. We just want to be clear. Thanks for editing.',), ('There is no way to tell unless either 1) There is theory to support a model or 2) You run the models.',), ("Just a note to say I've used the `mtcars` dataset (built into R) so that any R users can themselves run the models and respond about which would be the best model. I've already ran the models but wasn't sure which one works best.",), ('This is the website where I found some more information. It seems to only deal with 2-dimensional data though. http://easycalculation.com/statistics/learn-regression.php',), ("This website is for questions of mathematical research. I don't see a research angle in your question.",), ('Welcome to CV. Can you clarify your question? Are you trying to predict Z as a function of X & Y, or are you trying to predict an outcome in another dataset from X, Y & Z here? If so, is there any way to establish a correspondence b/t a given triplet in this dataset (Xi, Yi, Zi) & a particular outcome in the other dataset (Oi)? You mention a "bias term", that terminology is more common in machine learning than statistics; are you trying to train something like a neural network, or do you want regular (OLS) regression in statistics?',), ('The Mathematica function `FindFit[...]` could achieve what you desire.',), ('I believe that the problem is asking to fit a line to 3-dimensional points. There are 100 points with 3 data values each, as well as accompanying result values. As an example:\n\nX value (0.442, 0.798, 0.708) has matching Y value (-6.228) and I need to fit a line to 100 points like the X value, while also using the Y data.\n\nI was directed here as a better location to ask my question. I was not informed that it was a research website. I apologize for any transgressions.',), ('Normalising the data should not change $r^2$',), ('this is the same as stepwise variable selection. see multiple posts on this topic elsewhere on this site',), ("why not doing it on the logit model directly? You'll get a better assessment that way..",), ('Thanks for your answers. The purpose of using Linear Regression was just to help with the computational complexity. ..I will read up on  the backwards/forwards selection posts.',), ("Yes, your formula from matrix notation is correct. Looking at the formula in question, $1-\\\\frac1{n}\\\\,=\\\\,\\\\frac{n-1}{n}$ so it rather looks as if you might used a sample standard deviation somewhere instead of a population standard deviation? Without seeing the derivation it's hard to say any more.",), ("This looks like routine bookwork. Is this for some subject? If you do need (for some non-obvious reason) to actually run a regression on a subset (see Andre's comment for why in general it's not necessary), the following approach is pretty typical. If `mydata` is a data frame containing `y,x1,x2` and `sex`, then instead of `lm(y~x1+x2,data=mydata)` you replace the `data=` argument with one that appropriately subsets the rows of `mydata` by the relevant value of `sex`.",), ('I don\'t really need to remove the male data, I just need to run the regression on only females.  I have a "female" column in which females are denoted 1, and males are denoted 0.  My current code looks like this:  lm(y~x,data=mydata)  I just don\'t know what to add to that to make it only run for rows that have female  =1',), ('I thought I\'d just go ahead and answer, because this is quite easy. However, the others are right; simple code requests belong on Stack Overflow, and in this case, you could probably Google it very quickly (that\'s why I\'m not flagging to migrate – it\'s a little *too* simple – though I don\'t mind if it gets migrated). In fact, [the first Google hit for "code for excluding conditions in r"](http://www.statmethods.net/management/subset.html) contains my answer, and several alternatives (hence I\'ve edited it in)!',), ('This question appears to be off-topic because it is about how to use r.',), ("Well, I wouldn't do it with Excel, but you could at least get parameter estimates with its optimization. You need to use the Solver add-in. If you want least squares fitting of nonlinear functions, it's very easy in R.",), ('This question appears to be off-topic because it is about how to use software.',), ('Your question is then actually how do I *subset a data frame in R*? which can be very helpful if you google it. Using `mydata[as.logical(mydata$female),]` as your data set in `lm` is one of half a dozen obvious ways (`mydata[mydata$female==1,]` is another; as is using `subset`)',), ('"Linearity" (or lack thereof) refers to the relationship between the predictors and the response, about which you have offered no direct relevant information. Could you please amend your post to provide that?',), ("In addition to @whuber's point, the marginal distribution of the response is not really of interest, but rather the conditional distribution / the distribution of the residuals. On another note, are all Y values integers / counts?",), ('Some useful searches you can investigate include [multinomial logistic regression](http://stats.stackexchange.com/search?q=multinomial+logistic+regression) and [ordinal regression](http://stats.stackexchange.com/search?q=ordinal+regression).',), ('"*whether a person\'s weight is correlated with their weight*" --- I\'m pretty sure that correlation is 1',), ('indeed, thanks.',), ('The prediction interval should be of the form you stated except for the degrees of freedom. A little more though should be given to it.',), ('This has been answered many times.  The short answer is no.  But instead of asking if a certain parameter makes sense to examine, state what you would like to estimate and go and estimate that.  I find this is most easily envisioned through differences in predicted values.',), ('So you mean plug in different values for the predictors then study the differences in fitted values?',), ('Can you write out the regression equation you want to estimate using maths instead of R code?  I think people are getting confused about what your statistical model is.  To start, you could use $ y_{tsi} $ as your response/dependant variable, where $ t $ is the time factor, $ s $ is the symbol factor, and $ i $ is any "repeated" observations with the same value for symbol and time.',), ('Yes, but be more intentional about it.  An effect of interest may be changing hp by $x$ units holding wt to $y$.',), ('seems related to this question as well http://stats.stackexchange.com/questions/89353/difference-between-iid-data-and-non-iid-data-for-a-simple-regression-problem',), ('you might want to expand your questions to include (1) number of predictors examined (2) method of examining predictors (feature selection) (3) number of observations (4) motivation behind building model',), ("Note that if $\\\\sigma^2$ doesn't vary across $i$, it won't affect the maximization, so the weighted least squares results would be the same as the log-likelihood results.",), ("What is the purpose of your document and who are the audiences? I'd start from getting similar articles and look for some examples on how they are done in your own field. I am more familiar with biomedical literature and most of the time, we just use a table. Illustrations are more often seen when the authors try to explain an interaction.",), ("@Penguin_Knight, this is in computer science domain, however I think this is a generic rather than restricted to a particular domain. Please correct me if I'm wrong.",), ('Try `s<-lm(tcyc ~ tinst+tmem+tcom-1, data=fit)`',), ("Hmm... though question. I'd say the only generic part, for me, is don't show more than you should, and make sure the components to be emphasized really get emphasized. Even in just my field, I have seen all three options. 1) tabulating the results is the most common, followed by 3), but mostly the form of plotting predicted outcome, and then 2). But for 2), I'd use what @gregory_britten suggested: use adjusted X instead of each individual X.",), ('@Georg, as far as I know, `+0` and `-1` are fully equivalent in R. They both suppress they intercept.',), ("yes, patrick is right. I tried using nnls package but still it was giving some coefficient as 0, again it doesn't hold good.",), ("I expect it's a formatting typo, and the intention is $(β_0, β_1)'$ (equivalently $(β_0\\\\;\\\\; β_1)^T$)- which denotes the *[transpose](http://mathworld.wolfram.com/Transpose.html)* of the row vector $(β_0, β_1)$ -- that is a column vector, consisting of those two elements.",), ('Thanks, this makes a lot of sense now. I\'m still trying to comprehend how to go about this solution, but with the "," in place, the question is much clearer now.',), ("You have to explicitly remove the intercept from the model, if you don't want to have it. This can be achieved by appending $-1$ to the formula, i.e. `E.curr ~ Y.17 + HE.last3 -1`",), ("Well, that solves the problem with the intercept but the coefficients are still wrong. The missing intercept also was just one reason why I think I'm using the wrong model. Apart from that, my results are just plain wrong.",), ('Please make the reference more precise than "a book".',), ("I didn't use it since it's a German book. I'll nevertheless put it in.",), ('In English-language discussions becoming zero is sometimes  described as vanishing. That however is the least of the puzzles here.',), ("Although I don't read German, I second @NickCox's suggestion. The reference (including page #) is needed at a minimum. In addition, an excerpt might be nice. On a different note, are you sure that the denominator isn't square-rooted (ie, $\\\\sqrt{1-{\\\\rm corr}(X,Y)^2}$)?",), ('@gung The denominator is not square rooted. I will try solving it with a rooted denominator though.',), ('You can omit one of the variables and still obtain an unbiased estimate of the other if they are independent.',), ("I don't see anything that establishes the y's have mean 0",), ("@Glen_b thanks for having a look at my question. I've provided the whole solution to the problem. There is actually two methods to arrive at the answer. Is it possible to arrive at the answer ($-\\\\frac{1}{2}$) without assuming the all the y's have mean 0?",), ('Method 1 is plainly wrong concerning the expectations of the $y_i$.  In fact you can work out algebraically that $\\\\mathbb{E}(\\\\hat\\\\alpha\\\\hat\\\\beta) = \\\\alpha\\\\beta-\\\\frac{1}{2}$.',), ('@whuber could you please show me how you worked out $E(\\\\hat{\\\\alpha}\\\\hat{\\\\beta})$? I am stuck at the part with the curly braces. Do I expand them or can they be further simplified first?',), ('Substitute the definitions and use the linearity of expectation. For instance, the first term within the curly braces is $\\\\mathbb{E}(y_1y_2)=\\\\mathbb{E}\\\\left((\\\\alpha+\\\\epsilon_1)(\\\\alpha+\\\\beta+\\\\epsilon_2)\\\\right)$, which you expand algebraically into terms like $\\\\mathbb{E}(\\\\alpha^2)=\\\\alpha^2$, $\\\\mathbb{E}(\\\\alpha\\\\epsilon_2)=\\\\alpha\\\\mathbb{E}(\\\\epsilon_2)=0,$ and $\\\\mathbb{E}(\\\\epsilon_1\\\\epsilon_2)$ (plus others of similar forms). The latter is $\\\\mathbb{E}(\\\\epsilon_1)\\\\mathbb{E}(\\\\epsilon_2)=0$ due to independence of the $\\\\epsilon_i$. Evidently, your text is confusing the $y_i$ with the $\\\\epsilon_i$.',), ('It is safe to make the presumption that breeds between species have nothing to do with each other? That is there is no relationship between rat breed 1 and rabbit breed 1?',), ('How many breeds of rabbit are there?',), ("I assume it's safe to consider there's no relationship. However, I can't come up with any examples, what could be such a relationship.",), ('Two breeds of rabbit, two breeds of rat.',), ("If you have only 2 breeds of rabbits, you can't have 2 dummy codes for the 2 breeds, the design matrix will be singular. 1 dummy code is enough for a categorical variable w/ 2 levels.",), ('@gung, if you look at the models closely there are 3 variables for 4 groups (2 species by 2 breeds), the 2 rabbit variables are a different coding of an interaction term.  The design matrix will not be singular (as long as all 4 cells have observations).',), ("Thanks for the pointer, @GregSnow, I think you're right. It's an unusual way to use the dummy coding. I guess it is what's sometimes called the 'flat' (one-way) approach to factorial ANOVA. I didn't recognize it at first.",), ('@gung, I actually made the same mistake (the similarity of `rab` and `rat2` visually made the mistake easier), just managed to see the difference before posting.',), ('The terminology is all standard from any regression text, with the exception of `Residual standard error` which is the conditional variance.  Never did figure out why.',), ('This question is an obvious duplicate, which you seem to recognize. If previous threads did not provide the information you needed, you should state what you learned from them & what specifically you still need to know.',), ("(1)have you looked at the residuals, done a lag1 test (Durbin-Watson), white-noise test (portmanteu) or looked at auto-correlation plots? (2) if this is OLS then an assumption that the residuals are not autocorrelated, in this case where there is high suspicious one should either prove they aren't or correct (3) there are a very large number of methods to correct/account for autocorrelation including praise-winston or cochrane-orcutt (if this is OLS)",), ('In the presence of autocorrelation, the parameter estimate is unbiased, SE is biased (downward), as are t (upward) and p (downward) in favor of significance...I think?',), ("1) Residuals look reassuringly random.  I can't make an autocorrelation plot, I don't think because the results are unevenly spaced and autocorrelation assumes the presence of a time series object, which has spaced data. Furthermore I don't think lag1 is appropriate either for the same reason. 2) it is an OLS.  I i'm just not sure how to show residuals are not autocorrelated if the data structure violates the assumptions of autocorrelation tests. 3) I can't figure out if these would work for unevenly spaced data.",), ('I will add, however, that if I assume a lag of 1 between each data point regardless of actual ing and run a Durbin-Watson test or a Box-Ljug test, I get non significant p-values suggesting no autocorrelation. However, I am skeptical that I can trust this result, given the uneven spacing.  Any thoughts?',), ("This is not a SO question -- what you're asking is how linear regression w/ or w/o weights works.  But the simple answer: assigning weights is equivalent to adding more data points where you think the values are more reliable.",), ('This question appears to be off-topic because it is about data analysis methods -- migrate to CrossValidated?',), ("@CarlWitthoft that's not strictly true as the type of weighting used by R does't affect the df of the t-distribution.",), ('@hadley  fair enough.  I prob\'ly should have written "sorta kinda like" instead of "equivalent to" .',), ('Will ask this in stats.stack. Thanks all for the interest shown.',), ("What are the variables?  Also, usually Y is the dependent variable and X the independent variable (there's no rule that it has to be so, but it's pretty  common)",), ('Yes you are right. Edit is done. Most of the variables are physical  parameters such as: temperature, pressure, speed...',), ('To statistical people temperature, pressure, speed are variables, not parameters.',), ('Is this, perchance, time series data?',), ('@NickCox Yes in this case it is. I just meant that those variables are physical. Sorry.',), ("@PeterFlom Yes indeed Y is a time serie. But I don't know anything particular about this field. Would it change the strategy to choose?",), ('If there is a dependence structure in time, it is usually a good idea to think about modelling it explicitly. Time series people are defined by their thinking there is no other way to do it.',), ('The R-squared measure indicates "fit", i.e. how well the regressors "explain" the available data set, namely, the past. With time series data, in many cases we are interested in the future, in acquiring a model that would forecast adequately future values of the dependent variable. Fit and forecasting usually don\'t improve together, and one should attempt to strike some sort of balance. What is your target here?',), ('Yes, given that it is time series, your strategy should change.',), ('@AlecosPapadopoulos   Okay, I edit my post to explain more the context of my problem.',), ('You have a deeper problem: the implicit assumptions in your multiple models contradict one another.  Why, then, are you conducting all these different regressions?  What are you really trying to accomplish?',), ('Why do the implicit assumptions contradict one another?',), ('Let your data be $(x_i,y_i,z_i)$, $i=1, 2,\\\\ldots,n$. In the first regression, "y=mx+b", the implicit assumption is that the values $\\\\epsilon_i=y_i-(mx_i+b)$ are identically distributed but independent random variables (iid) with expectations of zero. The second regression assumes $\\\\delta_i=x_i-(m_0y_i+b_0)$ are iid. Plugging that into the first gives $\\\\delta_i=x_i-(m_0(mx_i+b+\\\\epsilon_i)+b_0)$. That simplifies to $\\\\delta_i=(1-m_0m)x_i-m_0b-b_0-m_0\\\\epsilon.$ Unless $m_0m=1$ the expectation of $\\\\delta_i$ varies with $x$ and even so its expectation is $-m_0b-b_0$; either is contradictory.',), ('Take a look at http://stats.stackexchange.com/q/22718 for more information about this asymmetry and http://stats.stackexchange.com/q/16381 concerning the assumptions you are making in your regressions.',), ('see http://stats.stackexchange.com/questions/88461/derive-variance-of-regression-coefficient-in-simple-linear-regression',), ("It's reasonably straightforward if you start from the fact that the line goes through $(\\\\bar x,\\\\bar y)$ and write the slope estimator as a kind of average.",), ('General answers have also been posted in the duplicate thread at http://stats.stackexchange.com/questions/91750.',), ('@AsymLabs - The log might be special in regression, as it is the only function that converts a product into a summation.',), ('`regress the scores on my dependent variable` Not clear enough so far. Are you regressing the "dependent variable" on the components? Or regressing a chosen component on that variable?',), ('@ttnphns after I run PCA, I will get **principal components** and the corresponding **scores**. Now, I want to regress these principal component scores on my dependent variable.',), ('If you are not shrinking or penalising the regression in "PCA space" this will achieve nothing.  PCA is an "affine" transformation (I think) of your X matrix, and it is reversible/invertible.  Its like modelling $ Y=A^T (X-c)\\\\gamma + e $ (where $A$ and $ c $ are functions of $ X $) instead of $ Y=X\\\\beta +e $.',), ("You'll usually just feed the (small number of) components to the regression that account for, say, 90% of the variation in the original data, as given by the cumulative sum of squares of the eigenvalues. I assume this is what the OP had in mind.",), ('@probabilityislogic would you please explain a little more',), ('The **parameters to be estimated** are (multi-)linear. If you were *estimating* the values of the exponents, the estimation problem would not be linear; but squaring a predictor fixes that exponent at precisely 2.',), ("My understanding is that @user777's comment, as well as the answers below, apply not only to polynomial regression, but also any regression that uses a [bijection](https://en.wikipedia.org/wiki/Bijection,_injection_and_surjection) of the predictor variables. e.g. any reversible function, like $log(x)$, $e^x$, etc. (plus some other functions, obviously, since 2nth powers aren't bijective).",), ('Thanks everyone; all of the answers and comments were helpful.',), ('@user2806363 if you are satisfied with my answer could you please mark it as accepted so we can close the question?',), ("I don't recognize your formula from [Wikipedia's version](https://en.wikipedia.org/wiki/Akaike_information_criterion#Definition), but I wouldn't – I'm lousy at manual calculation. Don't worry about negative AICs though; see [Negative values for AICc (corrected Akaike Information Criterion)](http://stats.stackexchange.com/q/486/32036).",), ('The same formula as used are using in this article page 13\n[link](http://dept.stat.lsa.umich.edu/~kshedden/Courses/Stat401/Notes/401-multreg.pdf)',), ('I found [this page](http://www.duke.edu/~rnau/testing.htm) quite helpful, for a review of the assumptions and the ways of testing them.',), ("Why would you want that? What would it tell you that isn't in the summary of linmod?",), ("I think this should be left open; it's pretty clear what the person is *asking*; it's not clear *why* he or she wants to know.",), ('It would allow me to t-test whether r might be 0.',), ("Apologies, I was typing on my phone so couldn't Latex things correctly.",), ('So would `cor.test()`, and the $t$-test of the slope coefficient (same test regardless of whether the variables are standardized).',), ('Please explain your dataset a little bt, do you have n measurements of the independent variables and n variations of temperature in time and you would like t use for predicting the k-th all the k-1-th preceding ones?',), ('This does not provide an answer to the question. To critique or request clarification from an author, leave a comment below their post - you can always comment on your own posts, and once you have sufficient [reputation](http://stats.stackexchange.com/help/whats-reputation) you will be able to [comment on any post](http://stats.stackexchange.com/help/privileges/comment).',), ("i knew that, but since at the time I didn't have enough reputation I couldn't do it, but since I thought I could be of help I decided to answer, sorry if I acted wrongly, the intent was good.",), ("No serious harm; just against site policy, hence the collective cleanup action. Hopefully we can just convert this to a comment for you, or if not, hopefully you'll find other ways to help :)",), ("you can't gain anything from imposing arbitrary conditions on error terms. you gain something only if your new conditions better reflect the reality.",), ("@Aksakal, I see what you mean, and the normal distribution represents reality, that's why we use it? Suppose we were to impose a bell curve shape distribution to $\\\\epsilon_{it}$ that had the appropriate bounded domain. Would this be a good solution to the problem? Suppose $Y_{it}$ was a person's age, and we choose a nice bell curve distribution for $\\\\epsilon_{it}$ that had domain $[0,150]$, this would be better than just using the normal distribution for $\\\\epsilon_{it}?$",), ("if normal distribution assumption seems reasonable, then you use it. you can also test the assumption using Jarque-Bera or similar tests. in physical sciences normal assumption often works very well. there are bounded distributions, such as beta distribution. frankly, i don't understand what is the problem to solve here. bounded distributions have their own issues. the point is to come up with the distribution which reflects the reality, in your case it could be lognormal, for instance",), ("@Aksakal, your making very good points here. Okay, suppose you were modeling student's performance on test score. Maybe you wanted to see how parental wealth had a impact on student performance, your model was $T_{it} = \\\\beta_0 + \\\\beta_1 W_{it} + \\\\epsilon_{it}$, and the test scores were in range $[0,100]$, what distribution would you use for the error term in this situation. My problem is that using the normal distribution is not realistic, I would suppose a bounded distribution from $[0,100]$ would be better.",), ('in this case normal assumption may or may not work very well, depending on what is your goal. secondly, you seem to have a panel data set, not the cross-sectional data, which suggest me that MIXED effects regression, instead of simple linear regression. i would start with normal assumption and test it. you always start with simplest stuff and try to make it work before going for fancy things.',), ('Thx. Answer 1 to http://stats.stackexchange.com/questions/88461/derive-variance-of-regression-coefficient-in-simple-linear-regression helped me perfectly.',), ("I'm sorry, but it is in no way clear what beta 1 hat is! Can you elaborate on the model in question?",), ("Yes, you should do multiple regression. But since your dependent is a proportion, you should probably use [beta-regression](http://cran.r-project.org/web/packages/betareg/vignettes/betareg.pdf). I don't think you should try correlating the dependent against all explanatory variables independently.",), ('What do you mean that this is for a paper? Are you writing a paper on heteroscedasticity?',), ('Guess I should have worded things better. The paper is on the topic of "How should you fit ANOVA and linear regression models, if the equal variance assumption is violated?"\n\n(that topic, word for word, was assigned to me by my professor)',), ("Our policy is not to provide direct answers to questions that pertain to people's class assignments, but to provide hints to help people get unstuck. To understand the process more fully, see the [wiki](http://stats.stackexchange.com/tags/self-study/info) for the `[self-study]` tag. You should probably be able to learn a lot by searching the site & reading relevant threads, though.",), ("What class is this, I wonder? Who assigns a 20 page paper on how to fit an ANOVA w/ heteroscedasticity? I've never heard of such a thing. Are you a statistics major at Harvey Mudd?",), ("it's STATS 3093: Probability and Mathematical Statistics II. (A third year course at a so-so Canadian university with an awful stats department). I'm a math major, (statistics minor). It's the 4th stats (13-14 week) course I've taken. The course had nothing to do with the topic, it was just sort of a project to do something that's (mostly) new to us to try to push us. Heteroscedasticity was in no way covered in any course I've taken.",), ('X1=68 Y1= 55 X2=54 Y2= 38 etc...',), ("My first question, would be, what tool would you prefer to use to do a linear regression? Would love to help with a solution that can show you how to do it vs just doing it for you (which I don't think will help in the long run).",), ('Dear Nathaniel, actually i have no idea what is the best tool to use :((',), ('Please have a look at the solution provided (needed a bit of a distraction for a moment), generate in excel. While I would rather help teach folks how to use the tools and interpret, I hope that this helps. If it does, please feel free to accept it.',), ("I had an identical problem, solved it with R's `segmented` package: http://stackoverflow.com/a/18715116/857416",), ('I cannot understand your last sentence. It may help you to read: [What is the difference between linear regression on y with x and x with y?](http://stats.stackexchange.com/q/22718/7290)',), ('The typical issue with identifiability in data analysis is when you have sets of equations to solve (eg, in SEM) that share the same variables & there are more variables than equations. Thus, there are multiple possible sets of parameter values that will reproduce the data exactly, & you cannot tell which set might be a reasonable one to pick. I cannot tell if that is what you are asking about. You seem to have really just 1 equation. If you have multiple paired data for X1 (a predictor) & X2 (a response), you can solve for a1 or a2 using regression methods.',), ('The answer depends on what assumptions you make about the independent variable in each model. When $X_1$ is assumed normal in the first model or $X_2$ is assumed normal in the second and $\\\\varepsilon$ is assumed independent of the independent variable, then both are equivalent to the standard bivariate normal model (with zero means).',), ('the context can be compared to SEM. I have many regressions between variables in $X$. Variables on the right of the regression and the residuals of the regressions are independent. The objectif is to find the structure of the generative sem knowing only $X$ the set of covariates. So the question is : "are there several (linear) structures leading to the same distribution for $X$ ?" This questions stands for X all Gaussian and then (if necessary) $X_1$ following Gaussian Mixture models.',), ("Where does the notion that there's a need to compute $(X^TX)^{-1}$ come from? That's not a good idea at all.",), ('Document classification, bioinformatics (microarray data, genomics, PPI networks, ...), web data mining (Twitter, Facebook, Google, ...), ...',), ("What's alpha? Your regression doesn't contain one.",), ("What are the column headers? What is your N? What are you trying to do? Also, stepwise is a bad method, but that's not your question",), ('The headers are the usual: B, standard error, Beta, t, Sig., and correlations (zero, partial, part).\n\nStepwise maybe bad method, but it SHOULD drop variables instead of leaving empty the significance.\n\nN is 40, and I am trying to get an interpretable result. :)',), ('Cross-posted: http://math.stackexchange.com/questions/766939/what-is-rm-cove-i-hat-y-i-in-simple-linear-regression',), ('Dear Eupraxis and Maarten, thank you for your response: **@Eupraxis** - Here is the picture, the way I understood your explanation about using function prediction intervals to calculate 95% CI bounds of estimated X value. Is it what you proposed (sorry for hand drawings)? ![enter image description here](http://i.stack.imgur.com/cZafb.jpg) What if my function cannot be re-expressed in linear way? I am not even sure how to do it for logistic curve anyway. Please also see my comment for Maarten. **@Maarten** - Thank you for the article! I tried to read it, but frankly it was quite hard to compreh',), ('Could you please [merge your two accounts](http://stats.stackexchange.com/help/merging-accounts)? A moderator will convert this non-answer as an edit to your question, unless you want to do it yourself.',), ('This does not provide an answer to the question. To critique or request clarification from an author, leave a comment below their post - you can always comment on your own posts, and once you have sufficient [reputation](http://stats.stackexchange.com/help/whats-reputation) you will be able to [comment on any post](http://stats.stackexchange.com/help/privileges/comment).',), ('@Andre Holzner - what you say it true, but, but, but... doing this random forrest is actually a form of regularisation, in a similar class to ridging.  I\'ll tell you a secret, a regression tree is actually a linear model - similar class to splines.  putting my Bayesian hat on, the random forrest regulariser would likely correspond roughly to the "spike and slab" priors used in Bayesian context.',), ('In R I\'d use `read.table(stdin())` to read the data into a data frame, copy-pasting the data direct from this question (followed by return on a blank line to end data entry), then `lm` to fit the model (assigning to a model object), and then `predict` to get information at $x=100$ and - assuming that by "standard error" you mean residual standard error - `summary` on the fitted model object to get that. But the sequence of actions will be different for every tool you use, and there are many good tools available.',), ('What exactly do you mean by "error for linear regression"? Do you mean an estimate of the variance of the errors, standard errors for the coefficients, standard error of prediction, or something else?',), ('hi whuber, the question is incomplete. I was searching for the RMS error of regression. I found it here http://www.stat.berkeley.edu/~stark/SticiGui/Text/regressionErrors.htm#rms_error_of_regression . Basically it is the vertical residuals.',), ('Since you have $\\\\hat y_i$ on the left, your right hand side should have $\\\\hat \\\\beta$\'s. While you\'re editing, please remove the "e" from the end of the word \'formula\'; you have only one formula so you should not use the plural.',), ('Might help if you share the model output.',), ("What do you mean by 'reliable'?",), ('Have you checked the possibility of high-degree multicollinearity among your regressors?',), ('"ENTER" methods are not based on sound statistical principles.  Avoid that at all costs unless you use the bootstrap to find out how poorly the method performs and you are still satisfied with the result.',), ("I've edited your post to include LATEX formatting. Please double check that it still says what you want.",), ("You could always read this: [Coefficient of determination](http://en.wikipedia.org/wiki/Coefficient_of_determination), and then refine your question in the light of what you don't understand.",), ("The point that @jbowman is trying to make is that $\\\\hat{Y}$ is a function of $\\\\hat{\\\\beta}$ NOT $\\\\beta$, so even though you are confident that $\\\\beta=0$, that doesn't mean $\\\\hat{\\\\beta}$ is 0 (it will probably be close, but not exact.)",), ('Why are there 12 points? What is the underlying question of interest?',), ("You don't need to remove correlated predictors. In fact, they're almost always correlated.",), ("Technically, in OLS you can also test H$_{0}\\\\text{: }r=0$ (where $r=\\\\sqrt{R^{2}}$, or, if your prefer to be pedantic $r = \\\\text{sign}(\\\\beta)\\\\sqrt{R^{2}}$), and you can also go wild with a test of H$_{0}\\\\text{: }\\\\alpha=0$. Just sayin'...",), ("thank you all. however, a new question appeared. I thought correlation only measures the strengh of the relationship. but it is the same as the standardized beta if I only have one predictor. this means it also measures how y changes if x changes. why can't it be greater then one then, let's say for example x increases by 0.5 standard deviations and y then by 1,5 standard deviations.",), ('Velcome to the site!',), ("This result is the [Cauchy-Schwarz Inequality](http://en.wikipedia.org/wiki/Cauchy%E2%80%93Schwarz_inequality#Statement_of_the_inequality) in disguise: the standardized $x$'s and standardized $y$'s can be viewed as unit vectors (in a space whose dimension equals the data count, but that doesn't matter because according to Euclid two vectors--which implicitly include their common origin--determine a plane or just a line). The standardized $\\\\beta$ is the dot product of these vectors, which is the cosine of the angle between them. Cosines of angles always lie between $-1$ and $1$.",), ('You say you "know" the subgroups exist and then you say you want to "find out" if they do. Do you mean you want to find which subjects go into which cluster? Or something else?',), ('Sorry i was not clear there, i want to assume i dont know subgroups exist and i want to find out whether they do or not.',), ('Can you clarify your question / the thinking behind it? Note that regularization can be seen as the application of a prior centered on 0.',), ('@gung: Yes. My question is that how to achieve small error if you apply linear regression for a given data set. Because the problem of linear regression is that overfitting. So we want to avoid that term by other term. How to avoid it?',), ('@gung: Please see my edit question',), ('If this is a self-study question please add the appropriate tag.',), ("The problematic nature of the last statement becomes clear when you consider that first, distributions do not have standard errors: they have standard deviations; and second, if that's what you mean, any $t$ distribution with 2 or fewer degrees of freedom does not even have a standard deviation.",), ("If the constant variable x has a constant measurement error, and the errors are used only to weight the variables in a relative way, isn't this situation equivalent to not having errors in x?",), ('@pedro That is not the case, because the errors in $x$ are not merely weights in a formula.  With errors-in-variables regression the fits will differ and the covariance estimates of the parameters will differ from ordinary regression.',), ('Thank you for the clarification. Could you expand a little bit on why that is the case?',), ("why does your x-axis have the label `Y^2`? That would imply you're either squaring $y$ or plotting $\\\\hat{y}_2$ -- and in either case that's not sounding like what your text says it is. Is your y-variable really called `Y^2`??",), ("Oh my model was lm( yVar^2 ~ xVar+xVar+xVar ), i transformed it by squaring because otherwise the residuals weren't normally distributed. The Y^2 label is a bit confusing though as it seems like the Y should be the transformed Y already.",), ('You should only have two dummy variables for the three shops.',), ('What makes you think this data fits a nonlinear function and what function do you think that is? To me it looks pretty much like a horizontal line with a couple of outliers are 0.',), ('Which sign do the resulting parameters have for each variable? Ara all of them numeric variables or are there factors?',), ('What is the size of your sample? What is the goal of your model? (inference or prediction in future samples)',), ('You have described a lot of useful things--except for the most important, which is the purpose of your additional experiment.  Presumably it will be testing some hypothesis.  What is it?',), ('You have two good answers below. If you want more information, it may help you to read my answer here: [Linear regression prediction interval](http://stats.stackexchange.com/a/33642/7290), which pertains to prediction intervals, but the idea is very similar.',), ("@whuber, I tried to answer your question with an edit to my own. Basically, my new observations would be meant to test a hypothesis that another variable affects y. I phased this as a new variable *q*, but it could also be one of my *x*s at a values outside the range it's been seen historically. I know that in order to design any experiment that I need to know or assume something about the random variation that will be present. What I'm wondering is how my regression results help me fill in the necessary assumptions. Thanks.",), ("There's a detailed intuitive explanation given in this post: [Shape of confidence interval for predicted values in linear regression](http://stats.stackexchange.com/a/85565/805)",), ('N=44, not perfectly balanced accross factor levels. The goal is not predictions at this stage, just finding the best way to model sample data variance.',), ('TA for the helpful answers and the excellent links.',), ("Is this a real experiment or an exercise? If it's the latter, please consider adding the [tag:self-study] tag.",), ('It is not clear what you want. You finish with *"The mean of this production rate is compared with the mean of the initial production rate"*. Do you want to know how to compare those means? Or how to get them?',), ("Can you add some more details to flesh this out? I'm not sure I follow your question.",), ('Are you referring to the intercept of your model being nonzero?',), ("If I had to guess, I think OP means that $\\\\hat{b}=-0.3$, when he has substantive knowledge that it should be zero, but that's meeting OP more than half-way. One way to enforce $b=0$ is to simply not estimate an intercept term in the model, which is the same as fixing it at precisely zero.",), ('The question is unclear. Please try to edit to make what you want more explicit.',), ('There are so many possibilities...could you be more specific as to what you are trying to achieve?',), ('possible duplicate of [When is it ok to remove the intercept in lm()?](http://stats.stackexchange.com/questions/7948/when-is-it-ok-to-remove-the-intercept-in-lm)',), ('You will also want to read: [Removal of statistically significant intercept intercept term boost $R^2$ in linear model](http://stats.stackexchange.com/q/26176/7290).',), ('Are you trying to model a structural change in the data? If so, then the package [strucchange](http://cran.r-project.org/web/packages/strucchange/index.html) might be appropriate.',), ('There are several votes to close this thread as a duplicate of one of those referenced in the preceding comments.  Those votes are valid based on (1) and even (2), but (3) looks new.  I would therefore like to suggest that respondents focus on the third question.',), ('Describe polynomial order and how higher orders may be required for a specific application?',), ("More use to ask candidates technical questions that naturally arise from discussion of their past experience & the job you'll be wanting them to do than to set them a quiz. You risk making the interview boring & putting them off working for you.",), ('I\'m stuck just trying to understand the data.  What do you suppose those *negative* "concentrations" mean?',), ('My apologies for the cross-over with other questions, however I found the questions I looked for, including the one linked, did not address specifics, such as where you expect an always positive value, I am happy for the focus to be on the third point though, as that is the most important.',), ('The data of $Y$ were logarithmized.',), ('One thing people have found with such *errors-in-variables* regressions is that the ratio of variances (of the y errors to the x errors) determines whether you should care much about errors in the x variables. If I correctly understand the ellipses in your picture to represent the joint error distributions, then those ratios are very large--perhaps 15:1 or more--indicating you could just as well apply ordinary least squares to your data.',), ("Why don't you use logistic regression? You can define the ratio as your dependent variable is many statistical packages.",), ("It's not quite clear to me what you're asking for.",), ('My question is: how do i interpret the estimate b. Does this mean that a 1 percentage point increase in X, increases the growth rate by 0.07 percentage points?',), ('Much clearer, thanks.',), ('What\'s "CC"? Do you mean \'correlation coefficient\'?',), ('It\'s a little unclear what models you\'re actually fitting. "Grouping" the response or the predictor(s)? What\'s a "basic regression"? This sounds like the sort of thing that can happen when the response varies in a non-linear fashion with a predictor. Perhaps you need to include polynomial or spline terms; perhaps an indicator variable for when the predictor is zero. What about the usual regression diagnostics?',), ('How are you determining the value of the explanatory variable for each group? Are you averaging them?',), ('@scortchi The usual diagnostics, in particular the effect size (using $R^2$) come out much better when the data is grouped as opposed to ungrouped.  I did mean grouping both the response and predictor variables, I will adjust that in the question.',), ("@ssdecontrol That's correct.",), ('Plot of residuals vs fitted values? Could you write down the two models? And what do the variables represent?',), ("Or with just two variables a simple scatter plot would show what's going on.",), ("I'm still not clear on what your new data looks like. This is my best guess: (0, .35),  (0, .35), (0, .35), (0, .35), (1, .65), (1, .65), (1, .65), (1, .65). In which case it sounds like you're trying to do a t-test to compare whether the mean of group 0-3 is different from group 4-7.",), ("So you're averaging the *response*! The coefficient of determination's bound to change.",), ('Are these percentages continuous (like the percentage of cream in milk, for example), or discrete (like binomial proportions, a count in some category out of a total count)?',), ('@Scortchi it might be worth writing an explanation of that important issue as an answer.',), ("Uhm... i don't get the difference. Aren't they both continuous?\nAnyway I think the second describe better my data, since we are speaking about people out of at total.",), ("Did you notice that $0.66^2 = 0.44$?  As far as your last remark goes, there's nothing strange about that because (presumably) both regressions involved an unnamed dependent variable I will call $Y$: the $0.68$ and $0.10$ values describe *only* how $A$ and $B$ are related to $Y$, not to each other. You can find [plenty of explanations of such phenomena](http://stats.stackexchange.com/search?tab=votes&q=regression%20significant) on our site.  [This thread on correlation](http://stats.stackexchange.com/questions/5747) also looks relevant.",), ("@whuber I did, and thank you for the links.  There is no need then for a strong correlation between two variables to suggest that they produce the same 'strength' of effect when regressed on the same dependent variable.",), ('The distribution of counts divided by counts is definitely discrete. Indeed, the numerator is usually modelled as a binomial, the denominator is conditioned on (treated as constant), so the ratio is usually treated as a scaled binomial. However, even if the denominator was alse a random variable, the ratio would still be discrete since its sample space is countable',), ('I wonder if the regression is using sampling weights (weights that denote the inverse of the probability that the observation is included because of the sampling design) and the t-test is using importance weights. That is what I get with Stata using your data.',), ("a correlation of 0.44 isn't particularly strong. It's perfectly possible to have three variables with $\\\\hat\\\\rho(y,a)=\\\\sqrt{0.68})$,$\\\\hat\\\\rho(y,b)=\\\\sqrt{0.10})$, and $\\\\hat\\\\rho(a,b)=\\\\sqrt{0.44})$ for example.",), ('I believe this question is answered at http://stats.stackexchange.com/questions/9131/obtaining-a-formula-for-prediction-limits-in-a-linear-model/9144#9144, which provides the correct formula for regression prediction intervals.  Implement that and let us know how it compares to the `R` output.',), ('Because there are myriad ways to express PDFs, a good solution should aim for a representation that is suitable for your purpose: what, then, is the objective of this exercise?',), ('Ok, the main thing is that I need to be able to integrate the pdf * a gaussian (I added the expression above) analytically, uniquely for each pdf.',), ('Where do these PDFs come from and in what mathematical form are they currently represented?',), ("It's a probability of magnification, they're currently just histograms (these are gaussian kde plots). Physically I think there is motivation to use a log-normal distribution, but that's hard to convolve with the gaussian.",), ('So what you are really saying is that you have batches of univariate data: sets of *numbers,* not distributions! That makes all the difference because it opens up many more options for the solution. Please consider editing your question to include an explanation of these data and of why you will need to perform the convolutions.',), ('Your problem is the best fit will be obtained when `a` and `b` are huge and `c` = -0.5',), ('Your function (given its starting values) is undefined for non-positive $x$ and is equivalent to $y=1$ for positive $x$.',), ('@Glen_b yup... at least in my graphing software for values of x up toward the order of $10^{1}$ on up to $10^{22}$. Or... Hmmm wait, that should be "for negative" and "for non-negative". My bad.',), ("Yep... ok just did it numerically, it's not exactly one, but it does not get far away from one. For his starting values, at $x=10^{50}$, $y=1.033$.",), ("Yes, we are agreed that the starting values aren't much good. But that's not the main problem, since other starting values can be chosen easily, and that doesn't happen with better starting values. See what happens in your graph when a=50000, b=1000, and c=-0.5 --- yet he still has multiple problems.",), ('Either your terminology is wrong or your model is written wrongly. Model formulas go `y ~ x` for response `y` and predictor `x`.',), ('You have what\'s called "repeated measures." There\'s a good long explanation in the answer here: http://stats.stackexchange.com/questions/35590/linear-regression-with-repeated-measures-in-r',), ("@Glen_b, Sorry that's just a typo.",), ('Can you please edit your question to clarify your actual meaning?',), ('@Glen_b, The crux of my question is: how to do a regression model when you have data from mutiple runs of the same experiment. For example if I was looking at the temperature along a really thin metal wire (so its essentially 1-d), and then I mark off lenghts on this wire called x=0, x=1, x=2, ... x=total length. Then I take 10 temperature measuresments for each value of x. How do I create a regression model, which gives Temperarute as a function of length, i.e. T(x), given that I have 10 T values for each value of x?',), ("@ssdecontrol, I've looked into repeated measurements, and I don't believe that's what I want. Sorry about the confusion, it is my fault for being to vague too with question. I've edited the question with the example I've provided above.",), ('You will get better advice if you state why you want to constrain the coefficients. Because you can take any linear transformation to the variables and change the coefficients (but still have the same fit) the constraint is not always meaningful unless transforming the variables would also change the constraint.',), ('What do you mean by range of errors? How those range of errors are computed?',), ("The mean of the errors should be zero in general, otherwise your model is biased. It would help a lot if you would say how precisely you calculate average mean (I've just realized that this is a bit of misnomer, since average and mean are for practical purposes synonyms) of errors, and how your errors are defined precisely.",), ('the range of errors was computed as follows : abs(min(actual-prediction)) + max (actual-prediction))\nthe everage of errors was computed as follows: avg(actual - predicted)',), ('What do you mean that the "models have different Y axes"? Are they different variables?',), ('Yes, one set of comparison was for models having different response variable but same predictor. But I think it is not possible to do such a comparison.',), ("1. Your model omits an error term. If there's no error (observed data exactly equals your no-error RHS), significance can't even come into it. 2. Do you account for the serial correlation in your model?",), ("Fixed. Autocorrelation is not required, I'm really more interested on how to practically blend the two models/predictors in one model.",), ("If there's no time-correlation issue, how does fitting at 20 minute intervals instead of 10 minute intervals improve things?",), ('The two $X$ variables are completely different. $X_2$ works better when applied to a longer term forecast because it uses a different set of data points unrelated to $X_1$ or $Y$.',), ("I don't see how that helps. If there's no time-correlation issue anywhere (everything is independent of everything at a different time to it), going to half the frequency simply halves the amount of data.",), ("Why are you not concerned that **X2** is only significant at 20 minute intervals? Have you looked at other intervals and *not* found significance? NOTE: I'm not saying that you *should* be concerned--it's not a red flag... Pink? Maybe...",), ('@Glen_b I rephrased my question, I hope this makes it more clear.',), ('@SteveS I should have wrote $R^2$ rather than significant. Please check my question again, I edited and added an example.',), ('non-linear regression is a huge field! You should say something about your preferred application area, which will make it easier to give advice.  A similar question (with answers) http://stats.stackexchange.com/questions/25608/non-linear-regression-references        http://stats.stackexchange.com/questions/8570/references-on-numerical-optimization-for-statisticians/10166#10166',), ("First, I'm not quite clear with about this point: Is the data *given* in 10 & 20 minute intervals or are you choosing these two intervals? Also, why are you running two separate regressions instead of a single regression with your two independent variables?",), ('Last, I recently read [this](http://www.aeaweb.org/articles.php?doi=10.1257/jep.28.2.3) article (and really liked it!) on **Big Data**--your question made me think of this line: `"An important insight from machine learning is that averaging over many small models tends to give better out-of-sample prediction than choosing a single model."` Granted, "many" could be *hundreds* (i.e. not 2). Still, I thought the line might be relevant...',), ("@SteveS Yes, that's the kind of answer I was looking for. I read about averaging models too, was wondering if there are other techniques. (I am choosing the Y sampling interval to better fit the models).",), ("Well, I was just sort of throwing that out there... However, my advice would be to keep it simple (i.e. a single model incorporating both **X**'s). Remember: That higher R^2 could be purely due to chance--a result more of *data dredging* then anything else. Plus, recall that it's the out-of-sample **RMSE** that you should care about, not the **R^2**.",), ("@SteveS I'm interested in the claim made there that the insight about model averaging comes from machine learning. Recently here (I think in a comment thread) I pointed out to someone that in statistics the practice (model averaging over potentially less complex models instead of selecting a single model) goes back many decades.",), ('Most of the time I really hate it when people go on and on about their datasets ("*Just get to the damn question already!*"). However, this may be one time where explaining a little bit about the type of data you\'re working with (e.g. financial? biological? *Tweets*??) could really help sort out the best approach to take as far as choosing the right model & timescale goes...',), ("@Glen_b: Well, you'll have to take that up with the author (Hal Varian)... Actually, I think he's not trying to credit the *machine learning* crowd with *inventing* the technique, but rather to suggest that they're doing a great job of actually implementing this idea *in practice*. Also note: It's an Economics article; hence, I think he's saying that with economists in mind (and not trying to make a general statement)... In other words, you make a good point.",), ("@Glen_b: My mistake for taking that quotation out of context... Anyway, unless you really can't stand Economics (you certainly would not be alone), you should definitely check it out--Varian is the chief economist over at Google (so the dude knows about Big Data). Plus, the particular journal is geared towards non-specialists so you won't find a more readable academic journal in Economics.",), ('1. The two variables you gave in your data (Wavelength, aCDOM) do not match the two variables in your model (St0104, Wavelength). $\\\\,$ 2. the $Y$ variable in the data that you have supplied is discrete. Indeed, it appears to be integers between 31 and 35, all scaled by a constant. What is it?',), ('hi sorry I should have mentioned the name of the y variable is St0104 the values are actually the absorbance of CDOM at 440nm over the wavelength range of a sample called St0104.I only gave a small set of the full data as it was quite long. X values are wavelength Y are the a440 (absorbance at 440nm).',), ('How is absorbance measured? Why are the values all multiples of a constant?',), ('(1) The residuals will be awful because the $(x,y)$ data trace out a step function (having only five levels). (2) The narrow range of y-values (varying less than $\\\\pm 6\\\\%$ from their middle) indicates there is little difference between fitting $(x,y)$ or fitting $(x,\\\\log(y))$. (3) One would guess the $y$ values are either counts or rounded decimals that have been converted (via some kind of calibration information) to absorbance values. A maximum likelihood estimate suitable for *the raw data* ought to converge and do a good job of estimating $b$.',), ('The Y values were calculated by choosing a wavelength of interest (in this case 440nm) and using the formula a440 = 2.303(A440) where 2.303 is a constant and A440 is the (corrected) absorbance at that wavelength. I used a spec fluorometer to measure absorbance of the sample. I am trying to calculate the spectral slope (SCDOM) which is estimated from the slope of an exponential curve of a440 as a function of wavelength. Again I am new to R and stats so I find R a bit confusing. I tried this regression in Matlab and it works fine if I use  exp function with 2 terms but I cant figure it out in R.',), ('maybe `glm` could be an option?',), ('glm has weights, but does it have anythiong similar to STATA\'s "absorb" that allows me to include fixed effects but repress them in the output?',), ('hmmmm not completely sure about that, maybe this can help? http://www.r-bloggers.com/linear-models-with-multiple-fixed-effects/',), ('See [Where do the assumptions for linear regression come from?](http://stats.stackexchange.com/q/55113/32036)',), ("See [here](http://stats.stackexchange.com/questions/16381/what-is-a-complete-list-of-the-usual-assumptions-for-linear-regression) and [here](http://stats.stackexchange.com/questions/17585/assumptions-needed-for-multiple-linear-regression) and a number of other posts (search is useful). However, note that if you're not doing any inference (testing, calculating standard errors/interval estimation), a number of the assumptions aren't required, though you may lose efficiency if some of the not-required ones don't hold.",), ("A negative predicted price is not necessarily an error.  We have no basis to expect perfect predictions. Although such a model should give one pause--clearly it does not fully or correctly capture the relationship between prices and the explanatory variables--it could still have great predictive value.  (Nevertheless, I have to agree with @Glen_b's answer suggesting that you consider using a more realistic model.)",), ('If you are having difficulties with interpreting the output after excluding the intercept, then you probably do not have the statistical background that supports excluding the intercept in the first place.  I suggest this question belongs on CrossValidated.com',), ("Why would you want to exclude the intercept? That's a dodgy move in most circumstances...",), ('Among other things, i want to simulate the Dickey-Fuller distribution by montecarlo methods in order to compute critical values.',), ('A good place to start would be by researching "Anscombe\'s quartet," which is shown in the answer at http://stats.stackexchange.com/a/16131.',), ('This is the subject matter of [circular statistics](http://stats.stackexchange.com/questions/105641/best-transformation-for-sinuous-data-sets). How to include these values depends on whether the aspect is explanatory or a response variable; presumably it is explanatory. In this case consider re-expressing aspect into a value more directly related to sun exposure, for which you say it is a proxy.  For instance, the pair (slope, aspect) can be converted into a rough numerical assessment of mean total daily insolation per unit area.',), ("Thanks Nick, your description of the dataset is correct, basically each subject has a certain mean reaction time (Y, expressed in ms) for each condition (X=1 through 9). Although I've never done regression in Statistica before, I think that a table like the one you described would probably allow me to get a mean regression slope for the dataset, but the problem is that (I thought) having repeated measures across rows is not statistically correct. Is this where I am wrong? This is the reason why I had each of the 9 means arranged as columns as opposed to making a dummy variable out of it.",), ('**This is an irreproducible result:** when I run your code with your data and starting values, `nls` completes successfully with a reasonable set of estimates.',), ('What do you mean by "regression weight" there?',), ('The "estimate" output of the summary(lm()) function - is this correct?',), ("I'm not sure what you mean by correct. Are you asking if it's okay call the estimate of the coefficient a regression weight? Some people call them that, but the phrase 'regression weight' has more than one possible interpretation, which is why I asked for clarification of what you intended.",), ("What's the dependent variable?",), ('I understand, thank you. Then: Yes, I mean the estimates of the coefficient. I will change that in the original question.',), ('Both the dependent and the independent variable are chronological age measured in years',), ('You are partly right: The smaller the pairwise differences, the closer the points will scatter around the line through zero with slope 1. But your two analyses are not similar, for instance the t test only checks for *mean* differences, not for individual differences.',), ("They don't test the same thing at all. The regression could indicate how much more sensitive the paired t-test would be compared to an independent one but the coefficient can be substantial with absolutely no paired t-test effect (and vice versa).",), ('@Michael Mayer. Can I say that the two methods are testing against the same null hypothesis, so that interpreting the p-value (whether mean of difference is 0 and whether the coefficient of B is 1) should be consistent between the two methods? When interpreting the estimates of the two methods, they are different, because as you said, t test only checks for mean differences, not for individual differences.',), ('No, the null hypotheses are not equivalent (imagine a "X" shaped scatter plot: the mean difference will be around zero but the regression slope is 0, not 1). What I said is: If the individual differences are small, then the regression slope will be around 1. That\'s the only relation between the two approaches. They investigate different aspects of the relation between X and Y.',), ('@John, thanks for the comments. Can you give me more insights about your comments? I am still bit confused.',), ("@Michael Mayer. Thanks. So the paired t-test is testing the intercept (against 0) of the linear model when fixing the slope at 1. The other method is testing the linear model slope (against 1) when fixing the intercept at 0. They are testing different aspects of the linear model. Therefore, to better illustrate the linear relationship, it's better to use the full model, y=kx+u, right?",), ('I don\'t get you. The only way you can run a paired sample t test with a regression in R is "lm(X - Y ~ 1)" and then check if the only parameter is zero or not.',), ('@Michael Mayer. Yes, in your formula X indicates the measurements, Y indicates the treatment factor (A or B). In my formula, X indicates the measurement from treatment A, and Y indicates the measurement from treatment B. Therefore, the linear relationship between the two treatment can be formed as $y_{i}=kx_{i}+\\\\mu+\\\\epsilon_{i}$. In terms of paired t-test, we fix $k=1$, then $y_{i}-x_{i}=\\\\mu+\\\\epsilon_{i}$.',), ("In my formula, X and Y are as in your formula. It's easy to run and check if it is similar to the t-test.",), ('This question appears to be off-topic because it is about how to use MATLAB.',), ('On this site we can provide theory and related advice, but we are not a coding resource.  If the former is what you are interested in, then please edit this question to supply additional details of the situation.',), ('Bear in mind that inferences on transform data cannot be validly applied to non-transformed data because $f(\\\\sigma^{2}_{X}) \\\\ne \\\\sigma^{2}_{f(X)}$. So if you find statistical evidence that $f(X) \\\\ne f(Y)$ you **did not** just find statistical evidence that $X \\\\ne Y$.',), ('Edited as per the comments. Thanks.',), ("Know when to stop what? Why not just use all $N$ variables? If you're worried about over-fitting you can use regularization with cross-validation to understand and maximize your model's predictive power. Please explain why you only want to use a few variables.",), ('Do more variables always help? I thought there was a curse of dimensionality and adding more degrees of freedom is not always good. This question I have is can you know what the optimal result is beforehand, and then decide to stop say when you are close enough to optimal? Is there a global optimum?',), ("More predictors means fewer degrees of freedom, especially if you delete cases with missing data listwise (which hopefully you don't have to). I don't think there is a global optimum though, except in theory: $R^2\\\\le1$, AFAIK, and adjusted $R^2$ should be lower...",), ('I think it depends on if the predictors are orthogonal. If they are not then more predictors can make the result better just by chance.',), ('Check the answer to [this](http://stats.stackexchange.com/questions/58107/conditional-expectation-of-r-squared) question.',), ("If the $X_i$ are really six orders of magnitude apart, it sounds like they are different things altogether or at least have been expressed in different units of measurement. Wouldn't that make the slopes automatically uncomparable? (2) Why are you not just doing a multiple regression of $Y$ versus $X_1$ and $X_2$? Is it (perhaps) because you have no data for which $X_1$ and $X_2$ are simultaneously available? (3) Exactly what F-test are you referring to?",), ("You should always look at the data before you run a regression, yes.  Doesn't have to be a histogram, though.  In the two-variable case you describe, I would plot a scatterplot (maybe with a histogram on the margins).  And look at it for a while.  Because maybe there's a crazy outlier observation.  Because maybe $y$ only takes on three values.  Because maybe the $y$ are all positive, or all between 0 and 1.  Because maybe the relationship between $y$ and $x$ looks less like a line and more like a hump or a circle or a hyperbola.  Or ...  Who knows?  If you don't look, you certainly won't know.",), ('@QuantIbex does that dependence necessarily imply $E[\\\\varepsilon_i\\\\varepsilon_j]\\\\neq 0$?',), ("Can you clarify what you are asking about? I don't think this question is answerable in its current form. I am aware or robust  'sandwich' errors, eg, but those are for you betas, not for predicted y.",), ('Check out the `car` package.',), ("Are you looking for a way to calculate a standard error of the estimate that is robust (against what)? If so, I'm not aware of and haven't been able to find a method, but either way, this question really needs clarification. It does no good to post an answer in the negative to address only one possible interpretation...",), ('If you mean "heteroskedasticity-robust," then by definition it can\'t be a single number.',), ('Which robust standard error do you mean? Please clarify.',), ('To give an answer, you should expand your question and explain how exactly (formula, software) you got this result.',), ('The estimates are found as described at http://stats.stackexchange.com/questions/107597/is-there-a-way-to-use-the-covariance-matrix-to-find-coefficients-for-multiple-re.  With these in hand, any test you want is obtained in a standard fashion.',), ('What is the nature of your items and response variable? Likert scale? If this is psychometric test are there correlated items (covariates)?',), ('Variances of ratios can be tricky!  In particular, if $\\\\bar{y}$ can be zero there is obvious problems.  If the probability that $\\\\bar{y}$ is close to zero is so small that can be neglected, you could try bootstrapping (or simulation).  In that case you could also try the "delta method", that is, a linear taylor approximation and then calculate the variance in that linear approximation. But if $\\\\bar{y}$ can be close to zero, that can be very wrong.  For a better answer, we need more details.  Give us much more information about your data and model!',), ('@kjetilbhalvorsen Thank you! I added a few details to the post. Since I have no experience with bootstrapping I would like to use the delta method. It looks like I would need the covariance between $\\\\bar{y}$ and $\\\\hat{\\\\beta_2}$ and $\\\\hat{\\\\beta_3}$, respectively. Do you have any suggestion how to compute this?',), ('You can read about the method here:  http://en.wikipedia.org/wiki/Delta_method',), ('Great, thank you!',), ('The variables are measured on likert scales, but there is not psyochometric scale underlying the variables. The covariates are correlated to some extent without explicit planing or knowledge about a scale.',), ('Thanks -- supose sample size n varies over cells of the variance-covariance matrix (for example due to item nonresponse and more particular due to the problem described in this question http://stats.stackexchange.com/questions/110559/estimate-linear-regression-using-items-randomly-selected-from-an-item-pool?noredirect=1#comment212379_110559). Should the relevant tests be adapted somehow?',), ("That's a difficult question to answer--but is best addressed at the other thread you started.",), ('By the way, my current intuition is to multiply impute missing outcomes a large number of times. The fraction of missing information might be high, but given data are MCAR, estimates of paramters and their standard errors should be unbiased given large number of imputations.',), ('I figure that in this situation multiple imputation might be another approach to deal with the problem (see my comment on the other thread).',), ('Do you need to find the value of the deterministic parameter as well or do you have it avalilable?',), ('Did you switch the definitions of $\\\\alpha$ and $\\\\beta$ partway through your post?',), ("Yes I need to find them. I'm trying to do something based on this stat.duke.edu/~fei/samsi/Readings/06TomaReich.pdf but just on a simple linear regression model with time series output and input variable. If it helps, you can assume that the range of the deterministic parameter is given. Please help me. I'll be grateful. Thanks. @davidhigh",), ('In your result you have a "**+**" preceding the y^(i)--is that a typo?',), ("Thank you for showing what you've tried so far & where you are stuck. Our policy is to provide hints to help get you unstuck & to the point where you can do this on your own. For more info, you may want to read the [wiki for the self-study tag](http://stats.stackexchange.com/tags/self-study/info).",), ('Please check the first line of your quoted question. Is there something missing after $\\\\beta^T$?',), ("What's your estimate of $\\\\beta^Tx$? Can you use basic properties of variances or expectations to compute its variance?",), ('This is a theoretical question. There are no concrete values. In my  post, I talked about computing $Var(\\\\Sigma_{j=1}^k\\\\beta_jx_{ij}|x_i)$ directly, but it is difficult because the $\\\\beta$ can be correlated which creates one big mess.',), ('I edited my post so that it specifies that $x_i$ are nx1 vectors',), ('http://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.linregress.html',), ("Have tried this already but doesn't support multivariate... correct?",), ('Ah, I see. How about http://www.datarobot.com/blog/multiple-regression-using-statsmodels/',), ('Seems on the right path, however, I am not sure how to get the overall standard error... I can see on the std error of the individual variables. Any ideas?',), ('Take the square root of `result.mse_resid`.',), ("It's really not difficult to calculate on your own once you have the coefficients. I'd just do that.  I don't usually calculate the standard error of the regression anyway. The standard errors for individual variables are trickier (especially if you move away from standard assumptions of OLS).",), ('... assuming you\'re following [this answer](http://stackoverflow.com/a/14971531/3309177) (from the link you\'ve provided) and you\'ve saved it to an object named "result".',), ('The quotation is a summary of the detailed explanation that precedes it.  You could take the preceding paragraph as @Cardinal\'s *definition* of "strong mean offset."',), ('I like definitions by "X is sth, that...". Taking the paragraph does not change anything - I still don\'t understand, that\'s why I decided to ask a direct question.',), ("How are you doing on this? In your attempts I see both $x^Tx$ and $X^TX$. But $x$ is a vector and $X$ is a matrix, so you've got a problem there. Also I think you want $Var(x^T\\\\beta)$, not $Var(x_i^T\\\\beta)$",), ('This QQ-plot strongly suggests you have a heavy-tailed distribution. The "most vanilla" rescaling would be to first try to make your data zero-meaned and having std.deviation of 1; this is not a final solution just a first step. Try it and see if your model behaves better.',), ('Well, I have used scale() to make predictor variable with mean equal 0 and sd equal 0 - qqplot still looks the same, any other ideas for rescalling?',), ('OK, so something more informed might be helpful. What is `variable`? Is is reasonable to take the `sqrt` of it? To `log` it? To do something like `I(variable/1e6)` maybe? Try to think easily interpretable transformations before doing something *heavy-handed* like a Box-Cox.',), ('A warning if you estimate the df as well -- then MLE may not work so well. See the discussion [here](http://davegiles.blogspot.com/2013/12/more-on-student-t-regression-models.html) for example.',), ("1) Don't confuse the ANOVA procedure with the ANOVA table; you can have ANOVA tables in more places than that. 2) ANOVA is simply a special case of regression in any case.",), ('The error term is always included when fitting a regression. You just need to specify the non-random component of the model (b0 + b1*x1)',), ('Yes, but I want to model the error term in the regression. Therefore I want to include it. For example y = b0 + b1*x1 + 30*epsilon',), ("If $\\\\epsilon$ is fixed, e.g. to mean 0 variance 1, then the $b_2$ estimate is what `lm` gives you as 'standard error' in the summary output, ie the conditional variance.  If it's not fixed then $b_2$ is not separately identified in $b_2\\\\epsilon$ without more information.",), ('If you were going to use logistic regression what would the outputs be?',), ('And in general, what are you hoping to achieve?  Do you want to predict the recovery rate exactly or just whether or not it was above (or below) a threshold?',), ('Box-Cox gave me lambda close to 0 so this mean log transformation, still nothing close to normality in qqplots of residuals',), ('Have you cross-validated with and without it? What about holding a separate "calibration" set of data that learns the linear model from the $\\\\hat y_{held}$?',), ('Thanks, well on this particular dataset the linear post-treatment gives a higher cross-validated prediction error, but I was wondering if there was perhaps a more general answer.',), ('Could you precise something about your data ? What is the dimension of the data ? What about the goodness of fit of the OLS regression ?',), ("There's not enough information here to discern what's happening. A reproducible minimal example (i.e. the smallest example that shows the problem you describe) might throw light on what's happening.",), ('Your question seems rather vague. There may or may not be issues, depending on the circumstances.',), ('Gravitational force is inversely proportional to squared distance, but proportional to masses.  If your experiment was about gravitational force and your variables were mass and distance then you would find strong causal relationships between both, but their ratio would be more informative.',), ('In any event, be particularly careful if there are any small values of x2.',), ('Could you tell us a little about your motivation for using $x_1/x_2$ in this model? For instance, is it suggested by theory, or did it emerge from a principled exploratory data analysis, or did it perhaps turn up after trying lots of different models?',), ("Without some further diagnostics (eg. $\\\\beta$'s conf. intervals, plots of observed vs fitted, residual vs fitted, $R^2$, etc.) it is a bit hard to say. Please provide some additional diagnostics first. In general, have you thought of a GLM with Gamma or Inverse Gaussian?",), ('I have no idea what those models are. (I have only taken stats 101). Are you saying that I cannot rely on intuition to make this decision? It feels reasonable to me that the shape of a log function would fit my variable. Therefore if I take the log of it, it will linearize my variable and be a better representation of reality. I am ok with discarding intuition in favor of more systematic diagnostics, but it is an awkward pill to swallow.',), ('Your intuition could be correct, I do not know. Nevertheless, given that you admit not having substantial experience on this field I urge you to follow standard procedures and not rely on your intuition immediately. Even in a `Stats101` course the importance of plotting your data should have been stressed anyway. The models I proposed are specifically designed to model dependent variables that are (any) positive numbers.',), ('Try then robust regression methods. In particular the functionality offered by the packages `lqmm` and `robustlmm` might can handy...',), ('You have "repeated measures" in your title, but don\'t use packages nlme or lme4. Seek advice from a statistician.',), ("I don't know much about neural nets and random forests, but I'm surprised to see a fit like that. Hopefully someone can answer with an explanation of why something like this would happen.",), ('These answers touche on the issue of the constant term, http://stats.stackexchange.com/questions/111544/removing-the-intercept-term-in-a-dynamic-regression-justified/111571#111571, and http://stats.stackexchange.com/questions/80790/deliberately-fitting-a-model-without-intercept/80798#80798although as @Glen_b notes, time series have their own aspects on the matter (such as differencing, that validly eliminates the constant).',), ('Internally or externally studentized?',), ("This is a homework related question and it's not specified but I would assume internally.",), ("the package `nlreg` (for heteroskedastic nonlinear regression) has a function (`nlreg.diag`) that can return a variety of such things including studentized residuals; I don't know much about it but it might be able to do ordinary nonlinear regression for you.",), ('Just to let you know, we welcome questions like this but we treat them differently. Instead of giving you answers, we provide hints to help you figure it out for yourself. For more about our policy here, read [this](http://stats.stackexchange.com/tags/self-study/info).',), ('Thanks, yes, I am just looking for hints to make sure I am on the right path to answering the question',), ('This set of questions is probably too broad to be answerable here. You might break them down into a series of questions that you ask as each is answered. In addition, I think some of this material is already well covered on the site.',), ('Thank you @gung. I edited the question to narrow the focus',), ('Could you maybe answer in intuitive way the following question.Figure 2.1 can be viewed as representation of 3d plot where the color of points simply represents a third dimension. There is a cloud of point where majority z=0; and there is another cloud where majority of z=1. By building linear model one simply builds the plain separating two clouds of points. Question, how does this plain projects on 2d plot? Or is it not, and decision boundary line in 2d plot has nothing to do with plane in 3d plot?',), ("WHen the denominator df in an F go to infinity, it's exactly equivalent to a chi-square",), ('Note that the weights are not a complication at all, because they can be absorbed in the values of $b$ and $A$, leading to an ordinary least squares problem with a single linear constraint. That means you problem is solved as described at http://stats.stackexchange.com/questions/24193 .',), ('Very nearly the same question is asked--and answered in four different ways--at http://stats.stackexchange.com/questions/61733. It differs only in explicitly addressing the two dimensional case.',), ('@user777 I am sorry about that: I was thinking that if these threads were close enough, we could merge your answer with the ones there.  I liked your explicit demonstration of `rstan`, which has no parallel in the answers within the other threads.',), ('@user777 I liked your solution as it was addressing this specific problem. If possible can you add it back? If anything it should be informative to someone else.',), ('@whuber Thanks, I was looking at other threads as well. The four different ways address a problem of a slope within the borders, not the sum of coefficients. The first link was helpful but I am still struggling to wrap my head around solve.QP or mgcv. I was hoping my question is generic enough to be useful for others and also sufficiently different from existing solutions in other threads.',), ('There is no essential mathematical difference between bounding the slope and bounding the sum of coefficients: both are bounds on linear combinations of the coefficients. The solutions offered to those questions apply with very little change to your slightly more general formulation, thereby immediately giving you access to a variety of approaches to choose from. However, I have not voted to close your question, because although it does appear to be answered elsewhere, evidently it does take a little mathematical manipulation to see that those answers can apply.',), ("@whuber I think I've finally cracked it, but since this is the first time I've ever attempted to solve a problem like this, I'd appreciate your expert feedback. Thanks!",), ('Is it possible that some "$+$" signs in the text are being mis-rendered (or misread) as "$-$" signs?',), ('I had thought this, but it happened twice in the text (2 different chapters) so I thought it to be unlikely.  Of course, a derivation of the formula would help! :)',), ('The negatives are a result of the positive correlation between an observation and its fitted value, which reduces the variance of the difference.',), ('@Glen Thanks for explaining why it turns out that the formula makes sense, along with your matrix derivation below.',), ('Why did you split your data into 3 subsets & fit 3 different models? What are you trying to do? Can you provide any information about your situation, your data & your goals? What do you mean by "the anova code"? Are you referring to the `anova()` function in R?',), ('Hope this helps: http://stats.stackexchange.com/questions/74334/question-about-prediction-bands-for-non-linear-regression-computation?lq=1',), ('Hope this helps: \\\\n\\\\nhttp://stats.stackexchange.com/questions/74334/question-about-prediction-bands-for-non-linear-regression-computation?lq=1',), ('See [here](http://i.stack.imgur.com/Xhilv.png). Also see [this](http://stats.stackexchange.com/questions/51061/does-the-normal-probability-plot-systematically-underestimate-the-mean) closely related question.',)]
----------------------------
